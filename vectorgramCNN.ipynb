{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vectorgramCNN.ipynb",
      "provenance": [],
      "mount_file_id": "12tNi7wAcg654KA2lZ860ReG8Kzd5e_jm",
      "authorship_tag": "ABX9TyOUi12Garoj7sAPxHu7wdDp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ianomunga/vectorgram/blob/main/vectorgramCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpO8EgJzeXd8",
        "outputId": "e65290af-f183-486c-bfe7-7583f31e7080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cglhOHXGXdKs"
      },
      "outputs": [],
      "source": [
        "# system related\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# path manipulation\n",
        "from pathlib import Path\n",
        "\n",
        "# regex\n",
        "import re\n",
        "\n",
        "# plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from matplotlib.patches import Circle\n",
        "\n",
        "# data manipulation / preparation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# image manipulation\n",
        "from PIL import Image\n",
        "#from IPython.display import Image [commenting out to resolve some inconsistency with PIL.Image]\n",
        "\n",
        "# metrics\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from matplotlib.patches import Circle\n",
        "\n",
        "# keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPool2D, Dropout, Input\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MammoScan:\n",
        "    def __init__(self, scan, sc_info):\n",
        "        self.__scan = scan\n",
        "        self.__sc_info = sc_info\n",
        "    \n",
        "    @property # scan image\n",
        "    def scan(self):\n",
        "        return self.__scan\n",
        "    \n",
        "    @property # scan info\n",
        "    def scan_info(self):\n",
        "        return self.__sc_info\n",
        "    \n",
        "    @property # from the scan file name\n",
        "    def scan_name(self):\n",
        "        return self.__sc_info.name\n",
        "    \n",
        "    @property # x coordinate of the abnormality\n",
        "    def x(self):\n",
        "        return self.__sc_info.x\n",
        "    \n",
        "    @property # y coordinate of the abnormality\n",
        "    def y(self):\n",
        "        return self.__sc_info.y\n",
        "    \n",
        "    @property # radius of the abnormality\n",
        "    def radius(self):\n",
        "        return self.__sc_info.radius\n",
        "\n",
        "    @property # class of the abnormality\n",
        "    def ab_class(self):\n",
        "        return self.__sc_info.ab_class\n",
        "    \n",
        "    @property # background tissue of the abnormality\n",
        "    def bg(self):\n",
        "        return self.__sc_info.bg\n",
        "    \n",
        "    @property # severity of the abnormality\n",
        "    def severity(self):\n",
        "        return self.__sc_info.severity\n",
        "\n",
        "    @property \n",
        "    # generates cropped downsized rotated images of the abnormality\n",
        "    def transformations(self):\n",
        "        return self.__transform()\n",
        "    \n",
        "    @property\n",
        "    # returns the matricial representation fo the scan image\n",
        "    def pixel_matrix(self):\n",
        "        return np.array(self.scan)\n",
        "    \n",
        "    # instance method\n",
        "    def plot(self):\n",
        "\n",
        "        # Create a figure. Equal aspect so circles look circular\n",
        "        fig, ax = plt.subplots(1)\n",
        "\n",
        "        fig.set_size_inches(8, 6)\n",
        "        ax.set_aspect('equal')\n",
        "\n",
        "        # Show the image\n",
        "        ax.imshow(self.scan, cmap=plt.cm.gray_r)\n",
        "        ax.set_ylim(bottom=0, top=1024)\n",
        "        ax.set_title(self.scan_name)\n",
        "        \n",
        "\n",
        "        # create a circle to patch on the image\n",
        "        x, y, r = self.__get_crop_coords()\n",
        "        print(f'{x}, {y}, {r}')\n",
        "        circ = Circle((x,y), r, fill=False)\n",
        "        ax.add_patch(circ)\n",
        "    \n",
        "    # private method\n",
        "    def __set_x(self, xValue):\n",
        "        self.__sc_info.x = xValue\n",
        "    \n",
        "    # private method\n",
        "    def __set_y(self, yValue):\n",
        "        self.__sc_info.y = yValue\n",
        "    \n",
        "    # private method\n",
        "    def __set_radius(self, rValue):\n",
        "        self.__sc_info.radius = rValue\n",
        "        \n",
        "    # private method\n",
        "    def __get_crop_coords(self):\n",
        "        '''Returns a tuple with x, y and r'''\n",
        "        # check scan class to decide on how to crop\n",
        "        if pd.isnull(self.radius):\n",
        "            radius = 48.0\n",
        "            self.__set_radius(radius)\n",
        "        if pd.isnull(self.x):\n",
        "            x = float(np.random.randint(500, 513))\n",
        "            self.__set_x(x)\n",
        "        if pd.isnull(self.y):\n",
        "            y = float(np.random.randint(500, 513))\n",
        "            self.__set_y(y)\n",
        "            \n",
        "        return (self.x, 1024.0-self.y, self.radius)\n",
        "    \n",
        "    # private method\n",
        "    def __transform(self):\n",
        "        '''Creates a dict \n",
        "                  with rotated and mirrored versions of self.scan'''\n",
        "        # create dictionary\n",
        "        transformations = dict()\n",
        "        # get crop values\n",
        "        x, y, r = self.__get_crop_coords()\n",
        "        # crop and resize scan\n",
        "        cropped_scan = self.scan.crop((x-r, y-r, x+r, y+r))\n",
        "        resized_scan = cropped_scan.resize((48,48))\n",
        "        # create rotated images\n",
        "        for angle in (0, 90, 180, 270):\n",
        "            rotated = resized_scan.rotate(angle) # rotated by angle\n",
        "            mirr_tp = rotated.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "            mirr_lr = rotated.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "            \n",
        "            transformations[angle] = dict(zip(['rotated', 'mirr_lr', 'mirr_tp'], \n",
        "                                              [rotated, mirr_lr, mirr_tp]))\n",
        "\n",
        "        return transformations"
      ],
      "metadata": {
        "id": "aKMg8LrjYz3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helpers.py\n",
        "\n",
        "# helper functions\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def clean_ds_files(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    '''Removes records with invalid data\n",
        "         and cast x and y to float'''\n",
        "    new_df = df.copy()\n",
        "    # search for invalid x values for removal\n",
        "    indices = new_df.x[lambda x: x == '*NOTE'].index\n",
        "    \n",
        "    for idx in indices:\n",
        "        n_idx = new_df.index.get_loc(idx)\n",
        "        # drop from dataset\n",
        "        new_df.drop(new_df.index[n_idx], inplace=True)\n",
        "        # delete from directory\n",
        "        delete_image(idx)\n",
        "\n",
        "    # make x and y float values\n",
        "    new_df.x = new_df.x.astype(float)\n",
        "    new_df.y = new_df.y.astype(float)\n",
        "    \n",
        "    return new_df\n",
        "\n",
        "\n",
        "def delete_image(filename: str, directory='../all-mias/'):\n",
        "    '''Deletes original image files that won't be initially used'''\n",
        "    paths = Path(directory).glob('**/*.pgm')\n",
        "    filename += '.pgm'\n",
        "    for f_path in sorted(paths):\n",
        "        try:\n",
        "            if f_path.name == filename:\n",
        "                os.remove(f_path)\n",
        "                break\n",
        "        except FileNotFoundError as fnf:\n",
        "            print('{fnf}') \n",
        "            \n",
        "            \n",
        "def create_scan_filenames_dic(path: str) -> dict:\n",
        "    '''Creates a dictionary with image filenames'''\n",
        "    paths = Path(path).glob('**/*.pgm')\n",
        "    img_dic = dict()\n",
        "    for f_path in sorted(paths):\n",
        "        # get full filename\n",
        "        full_fname = f_path.name\n",
        "        # get filename (no extension)\n",
        "        filename = f_path.stem\n",
        "        # create dictionary\n",
        "        img_dic[filename] = f_path.as_posix()\n",
        "    \n",
        "    return img_dic\n",
        "        \n",
        "\n",
        "def save_subsamples(scans_dic: dict(), df: pd.DataFrame) -> pd.DataFrame:\n",
        "    '''Save subsamples to the subsamples folder'''\n",
        "    # define subsamples folder\n",
        "    folder = '../subsamples'\n",
        "    df_sub = pd.DataFrame()\n",
        "    try:\n",
        "        # create if not yet\n",
        "        if not os.path.exists(folder):\n",
        "            os.mkdir(folder)\n",
        "    except:\n",
        "        print('An error occurred when searching for the folder')\n",
        "        \n",
        "    # iterate dictionary of filenames\n",
        "    for scan_name, filename in scans_dic.items():\n",
        "        \n",
        "        # create image and scan info objects\n",
        "        try:\n",
        "            scan = Image.open(filename)\n",
        "        except FileNotFoundError as fnf:\n",
        "            print({fnf})\n",
        "            \n",
        "        scan_info = df.loc[scan_name].copy()\n",
        "        # create the MammoScan object\n",
        "        m_scan = MammoScan(scan, scan_info)\n",
        "        # get the transformations\n",
        "        transf_scans = m_scan.transformations\n",
        "        # create filenames\n",
        "        filenames = create_subsample_filename(scan_name, transf_scans)\n",
        "        # get transformed scans Image objects\n",
        "        imgs = get_transformed_scans(transf_scans)\n",
        "        # prepare for saving\n",
        "        fs_and_is = list(zip(filenames, imgs))\n",
        "        \n",
        "        for filename, image in fs_and_is:\n",
        "            # create new observation with subsample name\n",
        "            # name the series to become an index in the new dataframe\n",
        "            scan_info.name = re.match(r'(.*)\\.[^.]+$', filename).group(1)\n",
        "            # create pixel matrix\n",
        "            pixel_matrix = np.asarray(image)\n",
        "            \n",
        "            scan_info['p_matrix'] = pixel_matrix\n",
        "            \n",
        "            scan_info['subsample_path'] = os.path.join('../subsamples', filename)\n",
        "\n",
        "            # append to dataframe\n",
        "            df_sub = df_sub.append(scan_info.loc[['ab_class', 'bg', \n",
        "                                                  'severity', 'subsample_path',\n",
        "                                                  'p_matrix']])\n",
        "            \n",
        "            #print(scan_name)\n",
        "            path = os.path.join('../subsamples', filename) \n",
        "            #print(path)\n",
        "            try:\n",
        "                image.save(path, compress_level=0)\n",
        "            except ValueError as ve:\n",
        "                print('Output format could not be determined from the file name.')\n",
        "            except OSError as ose:\n",
        "                print('File could not be written.')\n",
        "                print({ose})\n",
        "        \n",
        "    return df_sub\n",
        "\n",
        "\n",
        "def create_subsample_filename(scan_name: str, transf_dic: dict) -> list:\n",
        "    '''Creates suffix pattern filename for transformed scans'''\n",
        "    filename = ''\n",
        "    file_names = list()\n",
        "    for angle, transfs in transf_dic.items():\n",
        "        for tf in transfs.keys():\n",
        "            filename += f'{scan_name}_{angle}_{tf}.png'\n",
        "            #print(filename)\n",
        "            file_names.append(filename)\n",
        "            filename = ''\n",
        "            \n",
        "    return file_names\n",
        "\n",
        "\n",
        "def get_transformed_scans(transf_dic: dict) -> list:\n",
        "    scans = list()\n",
        "    for angle, transfs in transf_dic.items():\n",
        "        for scan in transfs.values():\n",
        "            scans.append(scan)\n",
        "    \n",
        "    return scans\n",
        "\n",
        "\n",
        "def generate_subsamples(path_to_originals: str, mias_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    ''' Generates the subsamples for training and testing.\n",
        "        Files are saved in ../subsamples\n",
        "        it returns a dataframe with the path to each subsample '''\n",
        "    scans_filenames_dic = create_scan_filenames_dic(path_to_originals)\n",
        "    final = save_subsamples(scans_filenames_dic, mias_df)\n",
        "    return final\n",
        "\n",
        "\n",
        "def balance_by_severity(df: pd.DataFrame, ab_class: str) -> pd.DataFrame:\n",
        "    ''' Balances an abnormality class based on severity '''\n",
        "    # deep copy\n",
        "    df = df.copy()\n",
        "    # if class == NORM, reduce norm to the avg class sample amount\n",
        "    if ab_class == 'NORM':\n",
        "        avg = int(df[df.ab_class != 'NORM'].groupby(['ab_class']).severity.size().mean())\n",
        "        return df[(df.ab_class == ab_class)].sample(avg)   \n",
        "    \n",
        "    sev_counts = df[df.ab_class== ab_class].severity.value_counts()\n",
        "    n_benign = sev_counts.loc['B']\n",
        "    n_malign = sev_counts.loc['M']\n",
        "    \n",
        "    if n_benign > n_malign:\n",
        "        # downsize 'B'\n",
        "        benign = df[(df.ab_class == ab_class) & (df.severity == 'B')].sample(n_malign, replace=False)\n",
        "        malign = df[(df.ab_class == ab_class) & (df.severity == 'M')]\n",
        "    else:\n",
        "        benign = df[(df.ab_class == ab_class) & (df.severity == 'B')]\n",
        "        malign = df[(df.ab_class == ab_class) & (df.severity == 'M')].sample(n_benign, replace=False)\n",
        "        \n",
        "    return pd.concat([benign, malign])\n",
        "\n",
        "\n",
        "def create_mias_dataset(file_path: str) -> pd.DataFrame:\n",
        "    ''' Creates a dataset with the data about the scans '''\n",
        "    # create a dataset\n",
        "    mammo = pd.read_table(file_path, delimiter='\\s', engine='python')\n",
        "    # rename the class column to avoid conflicts with the class keyword in python\n",
        "    mammo.columns = ['refnum', 'bg', 'ab_class', 'severity', 'x', 'y', 'radius']\n",
        "    # fill null severity with A for NORM class\n",
        "    mammo.severity = mammo.severity.fillna('A')\n",
        "    # drop duplicates\n",
        "    mammo.drop_duplicates(subset='refnum', keep='first', inplace=True)\n",
        "    # set refnum as index\n",
        "    mammo.set_index(keys='refnum', drop=True, inplace=True)\n",
        "    # return clean df and delete unuseful images\n",
        "    return clean_ds_files(mammo)\n",
        "\n",
        "def plot_results(acc,val_acc,loss, val_loss):\n",
        "    # create grit\n",
        "    fig, (ax1, ax2) = plt.subplots(nrows = 1,\n",
        "                                   ncols = 2,\n",
        "                                   figsize = (15,6),\n",
        "                                   sharex =True)\n",
        "    \n",
        "    # set plots\n",
        "    plot1 = ax1.plot(range(0, len(acc)),\n",
        "                     acc,\n",
        "                     label = 'accuracy')\n",
        "    \n",
        "    \n",
        "    plot2 = ax1.plot(range(0, len(val_acc)),\n",
        "                     val_acc,\n",
        "                     label = 'val_accuracy')\n",
        "\n",
        "    ax1.set(title = 'Accuracy And Val Accuracy progress',\n",
        "            xlabel = 'epoch',\n",
        "            ylabel = 'accuracy/ validation accuracy')\n",
        "\n",
        "    ax1.legend()\n",
        "\n",
        "    plot3 = ax2.plot(range(0, len(loss)),\n",
        "                     loss,\n",
        "                     label = 'loss')\n",
        "    \n",
        "    plot4 = ax2.plot(range(0, len(val_loss)),\n",
        "                     val_loss,\n",
        "                     label = 'val_loss')\n",
        "    \n",
        "    ax2.set(title = 'Loss And Val loss progress',\n",
        "            xlabel = 'epoch',\n",
        "            ylabel = 'loss/ validation loss')\n",
        "\n",
        "    ax2.legend()\n",
        "\n",
        "    fig.suptitle('Result Of Model', fontsize = 20, fontweight = 'bold')\n",
        "    fig.savefig('Accuracy_Loss_figure.png')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "def balance_df_by_severity(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    final_df = pd.DataFrame()\n",
        "    for ab_class in df.ab_class.unique():\n",
        "        class_df = balance_by_severity(df, ab_class)\n",
        "        final_df = pd.concat([class_df, final_df])\n",
        "        \n",
        "    return final_df.sample(len(final_df), replace = False)\n",
        "\n",
        "\n",
        "def full_balance_df_by_severity(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    final_df = pd.DataFrame()\n",
        "    for ab_class in df.ab_class.unique():\n",
        "        if ab_class == 'NORM':\n",
        "            class_df = balance_by_severity(df, ab_class).sample(96, replace = False)\n",
        "        else:\n",
        "            class_df = balance_by_severity(df, ab_class)\n",
        "            class_df_B = class_df[class_df.severity == 'B'].sample(48, replace = False)\n",
        "            class_df_A = class_df[class_df.severity == 'M'].sample(48, replace = False)\n",
        "            class_df = pd.concat([class_df_A, class_df_B])\n",
        "            class_df = class_df.sample(len(class_df), replace = False)\n",
        "        final_df = pd.concat([class_df, final_df])\n",
        "        \n",
        "    return final_df.sample(len(final_df), replace = False)\n",
        "\n",
        "def display_probabilities(prediction):\n",
        "    for index, probability in enumerate(prediction):\n",
        "        print(f'{index}: {probability:.10%}')\n",
        "        \n",
        "def create_final_results_df() -> pd.DataFrame:\n",
        "    # create dataframe for results\n",
        "    data = {\"calcifications\": [0, 0, 0, 0]}\n",
        "    return pd.DataFrame.from_dict(data, orient='index', columns=['test_accuracy', 'precision','recall', 'f1-score'])"
      ],
      "metadata": {
        "id": "MCgfmXqhY6DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the data from GoogleDrive\n",
        "mias = create_mias_dataset('/content/drive/MyDrive/vectorgram/Data/Info.txt')\n",
        "mias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "hOlW6wtHZYOx",
        "outputId": "fd8cf780-3d46-4b74-efa2-a8d6f301a3d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       bg ab_class severity      x      y  radius\n",
              "refnum                                           \n",
              "mdb001  G     CIRC        B  535.0  425.0   197.0\n",
              "mdb002  G     CIRC        B  522.0  280.0    69.0\n",
              "mdb003  D     NORM        A    NaN    NaN     NaN\n",
              "mdb004  D     NORM        A    NaN    NaN     NaN\n",
              "mdb005  F     CIRC        B  477.0  133.0    30.0\n",
              "...    ..      ...      ...    ...    ...     ...\n",
              "mdb318  D     NORM        A    NaN    NaN     NaN\n",
              "mdb319  D     NORM        A    NaN    NaN     NaN\n",
              "mdb320  D     NORM        A    NaN    NaN     NaN\n",
              "mdb321  D     NORM        A    NaN    NaN     NaN\n",
              "mdb322  D     NORM        A    NaN    NaN     NaN\n",
              "\n",
              "[322 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0e53a5b2-f8db-402b-95dc-1dc448a90c28\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bg</th>\n",
              "      <th>ab_class</th>\n",
              "      <th>severity</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>radius</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>refnum</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>mdb001</th>\n",
              "      <td>G</td>\n",
              "      <td>CIRC</td>\n",
              "      <td>B</td>\n",
              "      <td>535.0</td>\n",
              "      <td>425.0</td>\n",
              "      <td>197.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdb002</th>\n",
              "      <td>G</td>\n",
              "      <td>CIRC</td>\n",
              "      <td>B</td>\n",
              "      <td>522.0</td>\n",
              "      <td>280.0</td>\n",
              "      <td>69.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdb003</th>\n",
              "      <td>D</td>\n",
              "      <td>NORM</td>\n",
              "      <td>A</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdb004</th>\n",
              "      <td>D</td>\n",
              "      <td>NORM</td>\n",
              "      <td>A</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdb005</th>\n",
              "      <td>F</td>\n",
              "      <td>CIRC</td>\n",
              "      <td>B</td>\n",
              "      <td>477.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdb318</th>\n",
              "      <td>D</td>\n",
              "      <td>NORM</td>\n",
              "      <td>A</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdb319</th>\n",
              "      <td>D</td>\n",
              "      <td>NORM</td>\n",
              "      <td>A</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdb320</th>\n",
              "      <td>D</td>\n",
              "      <td>NORM</td>\n",
              "      <td>A</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdb321</th>\n",
              "      <td>D</td>\n",
              "      <td>NORM</td>\n",
              "      <td>A</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdb322</th>\n",
              "      <td>D</td>\n",
              "      <td>NORM</td>\n",
              "      <td>A</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>322 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0e53a5b2-f8db-402b-95dc-1dc448a90c28')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0e53a5b2-f8db-402b-95dc-1dc448a90c28 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0e53a5b2-f8db-402b-95dc-1dc448a90c28');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess to remove faulty scans\n",
        "mias = generate_subsamples('/content/drive/MyDrive/vectorgram/Data/all-mias', mias)"
      ],
      "metadata": {
        "id": "RYH4gzhQan9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Q_pMOFCUbNrR",
        "outputId": "f9038eed-a2c2-423c-c448-3c7d0246c484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   ab_class bg severity                        subsample_path  \\\n",
              "mdb001_0_rotated       CIRC  G        B    ../subsamples/mdb001_0_rotated.png   \n",
              "mdb001_0_mirr_lr       CIRC  G        B    ../subsamples/mdb001_0_mirr_lr.png   \n",
              "mdb001_0_mirr_tp       CIRC  G        B    ../subsamples/mdb001_0_mirr_tp.png   \n",
              "mdb001_90_rotated      CIRC  G        B   ../subsamples/mdb001_90_rotated.png   \n",
              "mdb001_90_mirr_lr      CIRC  G        B   ../subsamples/mdb001_90_mirr_lr.png   \n",
              "...                     ... ..      ...                                   ...   \n",
              "mdb322_180_mirr_lr     NORM  D        A  ../subsamples/mdb322_180_mirr_lr.png   \n",
              "mdb322_180_mirr_tp     NORM  D        A  ../subsamples/mdb322_180_mirr_tp.png   \n",
              "mdb322_270_rotated     NORM  D        A  ../subsamples/mdb322_270_rotated.png   \n",
              "mdb322_270_mirr_lr     NORM  D        A  ../subsamples/mdb322_270_mirr_lr.png   \n",
              "mdb322_270_mirr_tp     NORM  D        A  ../subsamples/mdb322_270_mirr_tp.png   \n",
              "\n",
              "                                                             p_matrix  \n",
              "mdb001_0_rotated    [[1, 4, 6, 9, 12, 15, 18, 24, 29, 35, 41, 48, ...  \n",
              "mdb001_0_mirr_lr    [[121, 105, 113, 127, 136, 141, 139, 141, 142,...  \n",
              "mdb001_0_mirr_tp    [[0, 1, 1, 1, 0, 0, 0, 0, 0, 2, 6, 7, 8, 10, 1...  \n",
              "mdb001_90_rotated   [[121, 110, 96, 98, 98, 99, 107, 130, 139, 127...  \n",
              "mdb001_90_mirr_lr   [[62, 74, 77, 74, 75, 90, 109, 115, 125, 116, ...  \n",
              "...                                                               ...  \n",
              "mdb322_180_mirr_lr  [[197, 202, 202, 198, 196, 202, 207, 210, 211,...  \n",
              "mdb322_180_mirr_tp  [[207, 206, 206, 205, 203, 205, 207, 204, 200,...  \n",
              "mdb322_270_rotated  [[197, 199, 198, 195, 196, 198, 197, 199, 200,...  \n",
              "mdb322_270_mirr_lr  [[177, 176, 175, 175, 179, 178, 177, 178, 177,...  \n",
              "mdb322_270_mirr_tp  [[211, 209, 208, 212, 215, 219, 221, 221, 222,...  \n",
              "\n",
              "[3864 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-42583cc4-5bbe-492f-b503-7add127c96dd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ab_class</th>\n",
              "      <th>bg</th>\n",
              "      <th>severity</th>\n",
              "      <th>subsample_path</th>\n",
              "      <th>p_matrix</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>mdb001_0_rotated</th>\n",
              "      <td>CIRC</td>\n",
              "      <td>G</td>\n",
              "      <td>B</td>\n",
              "      <td>../subsamples/mdb001_0_rotated.png</td>\n",
              "      <td>[[1, 4, 6, 9, 12, 15, 18, 24, 29, 35, 41, 48, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdb001_0_mirr_lr</th>\n",
              "      <td>CIRC</td>\n",
              "      <td>G</td>\n",
              "      <td>B</td>\n",
              "      <td>../subsamples/mdb001_0_mirr_lr.png</td>\n",
              "      <td>[[121, 105, 113, 127, 136, 141, 139, 141, 142,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdb001_0_mirr_tp</th>\n",
              "      <td>CIRC</td>\n",
              "      <td>G</td>\n",
              "      <td>B</td>\n",
              "      <td>../subsamples/mdb001_0_mirr_tp.png</td>\n",
              "      <td>[[0, 1, 1, 1, 0, 0, 0, 0, 0, 2, 6, 7, 8, 10, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdb001_90_rotated</th>\n",
              "      <td>CIRC</td>\n",
              "      <td>G</td>\n",
              "      <td>B</td>\n",
              "      <td>../subsamples/mdb001_90_rotated.png</td>\n",
              "      <td>[[121, 110, 96, 98, 98, 99, 107, 130, 139, 127...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdb001_90_mirr_lr</th>\n",
              "      <td>CIRC</td>\n",
              "      <td>G</td>\n",
              "      <td>B</td>\n",
              "      <td>../subsamples/mdb001_90_mirr_lr.png</td>\n",
              "      <td>[[62, 74, 77, 74, 75, 90, 109, 115, 125, 116, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdb322_180_mirr_lr</th>\n",
              "      <td>NORM</td>\n",
              "      <td>D</td>\n",
              "      <td>A</td>\n",
              "      <td>../subsamples/mdb322_180_mirr_lr.png</td>\n",
              "      <td>[[197, 202, 202, 198, 196, 202, 207, 210, 211,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdb322_180_mirr_tp</th>\n",
              "      <td>NORM</td>\n",
              "      <td>D</td>\n",
              "      <td>A</td>\n",
              "      <td>../subsamples/mdb322_180_mirr_tp.png</td>\n",
              "      <td>[[207, 206, 206, 205, 203, 205, 207, 204, 200,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdb322_270_rotated</th>\n",
              "      <td>NORM</td>\n",
              "      <td>D</td>\n",
              "      <td>A</td>\n",
              "      <td>../subsamples/mdb322_270_rotated.png</td>\n",
              "      <td>[[197, 199, 198, 195, 196, 198, 197, 199, 200,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdb322_270_mirr_lr</th>\n",
              "      <td>NORM</td>\n",
              "      <td>D</td>\n",
              "      <td>A</td>\n",
              "      <td>../subsamples/mdb322_270_mirr_lr.png</td>\n",
              "      <td>[[177, 176, 175, 175, 179, 178, 177, 178, 177,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdb322_270_mirr_tp</th>\n",
              "      <td>NORM</td>\n",
              "      <td>D</td>\n",
              "      <td>A</td>\n",
              "      <td>../subsamples/mdb322_270_mirr_tp.png</td>\n",
              "      <td>[[211, 209, 208, 212, 215, 219, 221, 221, 222,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3864 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-42583cc4-5bbe-492f-b503-7add127c96dd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-42583cc4-5bbe-492f-b503-7add127c96dd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-42583cc4-5bbe-492f-b503-7add127c96dd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check proportions of each class distribution\n",
        "mias.groupby(['ab_class']).severity.value_counts() / len(mias.index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlWHpotLbyZp",
        "outputId": "05f5f062-7f49-46ef-8bf9-9014cb34f587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ab_class  severity\n",
              "ARCH      M           0.031056\n",
              "          B           0.027950\n",
              "ASYM      M           0.027950\n",
              "          B           0.018634\n",
              "CALC      M           0.040373\n",
              "          B           0.037267\n",
              "CIRC      B           0.059006\n",
              "          M           0.012422\n",
              "MISC      B           0.021739\n",
              "          M           0.021739\n",
              "NORM      A           0.642857\n",
              "SPIC      B           0.034161\n",
              "          M           0.024845\n",
              "Name: severity, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mias.groupby(['ab_class', 'severity']).size().unstack().plot(kind='bar', \n",
        "                                                               figsize=(10, 8), \n",
        "                                                               xlabel='Abnormality Class', \n",
        "                                                               ylabel='SCANS',\n",
        "                                                               title='SEVERITY BY CLASS');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "yMK6rFhFb0SZ",
        "outputId": "8a6f43fa-cb74-47ce-bb42-98968270110a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mias[mias.ab_class != 'NORM'].groupby(['ab_class', 'severity']).size().unstack().plot(kind='bar', \n",
        "                                                                                      stacked=True, \n",
        "                                                                                      figsize=(10, 8), \n",
        "                                                                                      xlabel='ABNORMALITIES ONLY', \n",
        "                                                                                      ylabel='Percentage',\n",
        "                                                                                      title='SEVERITY BY CLASS');\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "TA3KQPlsfNiP",
        "outputId": "2f92bd4b-0001-4c80-8b7e-8df9d43c8490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#balancing calcifications data\n",
        "calcifications_unbalanced = mias[mias.ab_class == 'CALC']\n",
        "\n",
        "calcifications_unbalanced.severity.value_counts().plot(kind='bar');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "xoBChWyefPVh",
        "outputId": "2725906d-aa20-4bcc-eb81-e57c6b9c5706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#after they're all balanced\n",
        "calcifications_balanced = balance_by_severity(mias, 'CALC')\n",
        "\n",
        "calcifications_balanced.severity.value_counts().plot(kind='bar');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "nvtBTQoffWFM",
        "outputId": "5a626f53-9158-4be7-929e-d9a03da363c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_results = create_final_results_df()"
      ],
      "metadata": {
        "id": "W4Dh-XuffZiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training for Calcifications\n",
        "calcifications = balance_by_severity(mias, 'CALC')\n",
        "calcifications"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "BPD-rJYDffxf",
        "outputId": "ac285ab6-48a8-4603-eb33-4359bba24e0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   ab_class bg severity                        subsample_path  \\\n",
              "mdb212_0_rotated       CALC  G        B    ../subsamples/mdb212_0_rotated.png   \n",
              "mdb212_0_mirr_lr       CALC  G        B    ../subsamples/mdb212_0_mirr_lr.png   \n",
              "mdb212_0_mirr_tp       CALC  G        B    ../subsamples/mdb212_0_mirr_tp.png   \n",
              "mdb212_90_rotated      CALC  G        B   ../subsamples/mdb212_90_rotated.png   \n",
              "mdb212_90_mirr_lr      CALC  G        B   ../subsamples/mdb212_90_mirr_lr.png   \n",
              "...                     ... ..      ...                                   ...   \n",
              "mdb245_180_mirr_tp     CALC  F        M  ../subsamples/mdb245_180_mirr_tp.png   \n",
              "mdb209_270_mirr_tp     CALC  G        M  ../subsamples/mdb209_270_mirr_tp.png   \n",
              "mdb213_270_rotated     CALC  G        M  ../subsamples/mdb213_270_rotated.png   \n",
              "mdb231_270_rotated     CALC  F        M  ../subsamples/mdb231_270_rotated.png   \n",
              "mdb231_180_rotated     CALC  F        M  ../subsamples/mdb231_180_rotated.png   \n",
              "\n",
              "                                                             p_matrix  \n",
              "mdb212_0_rotated    [[3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4,...  \n",
              "mdb212_0_mirr_lr    [[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,...  \n",
              "mdb212_0_mirr_tp    [[3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4,...  \n",
              "mdb212_90_rotated   [[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,...  \n",
              "mdb212_90_mirr_lr   [[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,...  \n",
              "...                                                               ...  \n",
              "mdb245_180_mirr_tp  [[164, 162, 165, 164, 164, 168, 170, 169, 170,...  \n",
              "mdb209_270_mirr_tp  [[125, 120, 116, 120, 124, 134, 132, 133, 132,...  \n",
              "mdb213_270_rotated  [[127, 125, 126, 129, 127, 129, 123, 115, 130,...  \n",
              "mdb231_270_rotated  [[125, 126, 126, 128, 127, 127, 131, 136, 139,...  \n",
              "mdb231_180_rotated  [[152, 156, 156, 159, 164, 163, 161, 161, 165,...  \n",
              "\n",
              "[288 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-232c548d-7678-478b-b6aa-4db80bf522ea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ab_class</th>\n",
              "      <th>bg</th>\n",
              "      <th>severity</th>\n",
              "      <th>subsample_path</th>\n",
              "      <th>p_matrix</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>mdb212_0_rotated</th>\n",
              "      <td>CALC</td>\n",
              "      <td>G</td>\n",
              "      <td>B</td>\n",
              "      <td>../subsamples/mdb212_0_rotated.png</td>\n",
              "      <td>[[3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdb212_0_mirr_lr</th>\n",
              "      <td>CALC</td>\n",
              "      <td>G</td>\n",
              "      <td>B</td>\n",
              "      <td>../subsamples/mdb212_0_mirr_lr.png</td>\n",
              "      <td>[[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdb212_0_mirr_tp</th>\n",
              "      <td>CALC</td>\n",
              "      <td>G</td>\n",
              "      <td>B</td>\n",
              "      <td>../subsamples/mdb212_0_mirr_tp.png</td>\n",
              "      <td>[[3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdb212_90_rotated</th>\n",
              "      <td>CALC</td>\n",
              "      <td>G</td>\n",
              "      <td>B</td>\n",
              "      <td>../subsamples/mdb212_90_rotated.png</td>\n",
              "      <td>[[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdb212_90_mirr_lr</th>\n",
              "      <td>CALC</td>\n",
              "      <td>G</td>\n",
              "      <td>B</td>\n",
              "      <td>../subsamples/mdb212_90_mirr_lr.png</td>\n",
              "      <td>[[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdb245_180_mirr_tp</th>\n",
              "      <td>CALC</td>\n",
              "      <td>F</td>\n",
              "      <td>M</td>\n",
              "      <td>../subsamples/mdb245_180_mirr_tp.png</td>\n",
              "      <td>[[164, 162, 165, 164, 164, 168, 170, 169, 170,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdb209_270_mirr_tp</th>\n",
              "      <td>CALC</td>\n",
              "      <td>G</td>\n",
              "      <td>M</td>\n",
              "      <td>../subsamples/mdb209_270_mirr_tp.png</td>\n",
              "      <td>[[125, 120, 116, 120, 124, 134, 132, 133, 132,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdb213_270_rotated</th>\n",
              "      <td>CALC</td>\n",
              "      <td>G</td>\n",
              "      <td>M</td>\n",
              "      <td>../subsamples/mdb213_270_rotated.png</td>\n",
              "      <td>[[127, 125, 126, 129, 127, 129, 123, 115, 130,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdb231_270_rotated</th>\n",
              "      <td>CALC</td>\n",
              "      <td>F</td>\n",
              "      <td>M</td>\n",
              "      <td>../subsamples/mdb231_270_rotated.png</td>\n",
              "      <td>[[125, 126, 126, 128, 127, 127, 131, 136, 139,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdb231_180_rotated</th>\n",
              "      <td>CALC</td>\n",
              "      <td>F</td>\n",
              "      <td>M</td>\n",
              "      <td>../subsamples/mdb231_180_rotated.png</td>\n",
              "      <td>[[152, 156, 156, 159, 164, 163, 161, 161, 165,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>288 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-232c548d-7678-478b-b6aa-4db80bf522ea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-232c548d-7678-478b-b6aa-4db80bf522ea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-232c548d-7678-478b-b6aa-4db80bf522ea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calcifications = calcifications.sample(len(calcifications), replace=False)"
      ],
      "metadata": {
        "id": "2THMAaSGfkPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generating Augmented Training and Test Data\n",
        "data_generator = ImageDataGenerator(validation_split=.25, \n",
        "                                    height_shift_range=.10, \n",
        "                                    width_shift_range=.10, \n",
        "                                    rotation_range=30, \n",
        "                                    rescale=1/255.)\n",
        "\n",
        "# train data\n",
        "train_data = data_generator.flow_from_dataframe(calcifications, \n",
        "                                                x_col=\"subsample_path\", \n",
        "                                                y_col=\"severity\",\n",
        "                                                class_mode=\"categorical\",\n",
        "                                                target_size=(48,48),\n",
        "                                                subset=\"training\",\n",
        "                                                color_mode=\"grayscale\",\n",
        "                                                shuffle=True)\n",
        "\n",
        "# test data\n",
        "test_data = data_generator.flow_from_dataframe(calcifications, \n",
        "                                               x_col=\"subsample_path\", \n",
        "                                               y_col=\"severity\",\n",
        "                                               class_mode=\"categorical\",\n",
        "                                               target_size=(48,48),\n",
        "                                               subset=\"validation\",\n",
        "                                               color_mode=\"grayscale\",\n",
        "                                               shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jFhr617foXQ",
        "outputId": "85b27734-03ae-41cd-860a-37a7a7966bc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 216 validated image filenames belonging to 2 classes.\n",
            "Found 72 validated image filenames belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define CNN\n",
        "cnn_calc = Sequential()\n",
        "# first layer\n",
        "cnn_calc.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=(48, 48, 1)))\n",
        "cnn_calc.add(Conv2D(64, kernel_size=(3,3),activation='relu'))\n",
        "cnn_calc.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "cnn_calc.add(Conv2D(64, kernel_size=(3,3),activation='relu'))\n",
        "cnn_calc.add(MaxPool2D(pool_size=(2, 2)))\n",
        "cnn_calc.add(Dropout(0.25))\n",
        "# Add dense layer to reduce the number of features\n",
        "cnn_calc.add(Dense(64, activation='relu'))\n",
        "cnn_calc.add(Dropout(0.25))\n",
        "# flattening results\n",
        "cnn_calc.add(Flatten())\n",
        "# Dense layer to produce final output\n",
        "cnn_calc.add(Dense(2, activation='softmax'))\n",
        "# print summary\n",
        "cnn_calc.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTzV2iZlfv8e",
        "outputId": "320ef902-c24e-4f08-a3c6-57bac1be4183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 46, 46, 32)        320       \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 44, 44, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 22, 22, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 20, 20, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 10, 10, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 10, 10, 64)        0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10, 10, 64)        4160      \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 10, 10, 64)        0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 6400)              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 12802     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 72,706\n",
            "Trainable params: 72,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualize the CNN's structure\n",
        "plot_model(cnn_calc, to_file='convnet.png', show_shapes=True, show_layer_names=True)\n",
        "Image.open(fp = 'convnet.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9H9Najhcf5j1",
        "outputId": "0f59e56b-e430-499b-89d6-ae0f51e92f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=587x1180 at 0x7FA08E00EFD0>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compiling the Model to parameter convergence by Adam\n",
        "adam = Adam(0.001)\n",
        "cnn_calc.compile(optimizer=adam,\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "UuwMExnEjjuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train and evaluate the model\n",
        "# set callbacks\n",
        "early_stopping = EarlyStopping(patience=10000, restore_best_weights=True)\n",
        "\n",
        "\n",
        "model_check_point = ModelCheckpoint(filepath= './', \n",
        "                                    monitor='val_loss', verbose=1, \n",
        "                                    save_best_only=True,\n",
        "                                    save_weights_only=False, \n",
        "                                    mode='auto', save_freq='epoch')\n",
        "\n",
        "# fit the model\n",
        "hist = cnn_calc.fit(train_data, \n",
        "                    validation_data=test_data, \n",
        "                    epochs=10000,\n",
        "                    callbacks=[early_stopping, model_check_point])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-nf52swjufx",
        "outputId": "bd00bc7f-c142-462b-e7c8-17bb86478d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.9954\n",
            "Epoch 8751: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 62ms/step - loss: 0.0095 - accuracy: 0.9954 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
            "Epoch 8752/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9954\n",
            "Epoch 8752: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.0069 - val_accuracy: 1.0000\n",
            "Epoch 8753/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 8753: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0069 - val_accuracy: 1.0000\n",
            "Epoch 8754/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0196 - accuracy: 0.9946\n",
            "Epoch 8754: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0173 - accuracy: 0.9954 - val_loss: 0.0538 - val_accuracy: 0.9861\n",
            "Epoch 8755/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0471 - accuracy: 0.9861\n",
            "Epoch 8755: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0471 - accuracy: 0.9861 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
            "Epoch 8756/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9907\n",
            "Epoch 8756: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0254 - accuracy: 0.9907 - val_loss: 0.0727 - val_accuracy: 0.9583\n",
            "Epoch 8757/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0282 - accuracy: 0.9948\n",
            "Epoch 8757: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0288 - accuracy: 0.9907 - val_loss: 0.0174 - val_accuracy: 1.0000\n",
            "Epoch 8758/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0230 - accuracy: 0.9946\n",
            "Epoch 8758: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0197 - accuracy: 0.9954 - val_loss: 0.1672 - val_accuracy: 0.9444\n",
            "Epoch 8759/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9907\n",
            "Epoch 8759: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0226 - accuracy: 0.9907 - val_loss: 0.0437 - val_accuracy: 0.9722\n",
            "Epoch 8760/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0213 - accuracy: 0.9946\n",
            "Epoch 8760: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0268 - accuracy: 0.9907 - val_loss: 0.0204 - val_accuracy: 0.9861\n",
            "Epoch 8761/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0068 - accuracy: 1.0000\n",
            "Epoch 8761: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0567 - val_accuracy: 0.9861\n",
            "Epoch 8762/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0458 - accuracy: 0.9907\n",
            "Epoch 8762: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0458 - accuracy: 0.9907 - val_loss: 0.0085 - val_accuracy: 1.0000\n",
            "Epoch 8763/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 1.0000\n",
            "Epoch 8763: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.0177 - val_accuracy: 0.9861\n",
            "Epoch 8764/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0878 - accuracy: 0.9891\n",
            "Epoch 8764: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0761 - accuracy: 0.9907 - val_loss: 0.0752 - val_accuracy: 0.9583\n",
            "Epoch 8765/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0182 - accuracy: 0.9946\n",
            "Epoch 8765: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0168 - accuracy: 0.9954 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
            "Epoch 8766/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9907\n",
            "Epoch 8766: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 1s 79ms/step - loss: 0.0117 - accuracy: 0.9907 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
            "Epoch 8767/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 1.0000\n",
            "Epoch 8767: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0164 - accuracy: 1.0000 - val_loss: 0.0393 - val_accuracy: 0.9722\n",
            "Epoch 8768/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0328 - accuracy: 0.9837\n",
            "Epoch 8768: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0325 - accuracy: 0.9815 - val_loss: 0.0235 - val_accuracy: 0.9861\n",
            "Epoch 8769/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0870 - accuracy: 0.9907\n",
            "Epoch 8769: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0870 - accuracy: 0.9907 - val_loss: 0.0479 - val_accuracy: 0.9861\n",
            "Epoch 8770/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1011 - accuracy: 0.9815\n",
            "Epoch 8770: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 1s 76ms/step - loss: 0.1011 - accuracy: 0.9815 - val_loss: 0.0484 - val_accuracy: 0.9722\n",
            "Epoch 8771/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0557 - accuracy: 0.9783\n",
            "Epoch 8771: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0480 - accuracy: 0.9815 - val_loss: 0.0406 - val_accuracy: 0.9861\n",
            "Epoch 8772/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0272 - accuracy: 0.9896\n",
            "Epoch 8772: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0247 - accuracy: 0.9907 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
            "Epoch 8773/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 1.0000\n",
            "Epoch 8773: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.0443 - val_accuracy: 0.9722\n",
            "Epoch 8774/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9907\n",
            "Epoch 8774: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 0.0158 - accuracy: 0.9907 - val_loss: 0.0438 - val_accuracy: 0.9861\n",
            "Epoch 8775/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 8775: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0954 - val_accuracy: 0.9583\n",
            "Epoch 8776/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 1.0000\n",
            "Epoch 8776: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 1s 86ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.0344 - val_accuracy: 0.9861\n",
            "Epoch 8777/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 1.0000\n",
            "Epoch 8777: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.0154 - val_accuracy: 0.9861\n",
            "Epoch 8778/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0222 - accuracy: 0.9948\n",
            "Epoch 8778: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0212 - accuracy: 0.9954 - val_loss: 0.1028 - val_accuracy: 0.9583\n",
            "Epoch 8779/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 8779: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0418 - val_accuracy: 0.9722\n",
            "Epoch 8780/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 8780: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0160 - val_accuracy: 1.0000\n",
            "Epoch 8781/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 8781: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0194 - val_accuracy: 0.9861\n",
            "Epoch 8782/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000    \n",
            "Epoch 8782: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0062 - val_accuracy: 1.0000\n",
            "Epoch 8783/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9907\n",
            "Epoch 8783: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 62ms/step - loss: 0.0168 - accuracy: 0.9907 - val_loss: 0.0390 - val_accuracy: 0.9861\n",
            "Epoch 8784/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 8784: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9861\n",
            "Epoch 8785/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 8785: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0627 - val_accuracy: 0.9583\n",
            "Epoch 8786/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 8786: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0181 - val_accuracy: 0.9861\n",
            "Epoch 8787/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9954\n",
            "Epoch 8787: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0170 - accuracy: 0.9954 - val_loss: 0.0225 - val_accuracy: 0.9861\n",
            "Epoch 8788/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 8788: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9861\n",
            "Epoch 8789/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 8789: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0177 - val_accuracy: 0.9861\n",
            "Epoch 8790/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0074 - accuracy: 0.9946\n",
            "Epoch 8790: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0065 - accuracy: 0.9954 - val_loss: 0.1053 - val_accuracy: 0.9861\n",
            "Epoch 8791/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 8791: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
            "Epoch 8792/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0157 - accuracy: 0.9891\n",
            "Epoch 8792: val_loss did not improve from 0.00159\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0171 - accuracy: 0.9861 - val_loss: 0.0090 - val_accuracy: 1.0000\n",
            "Epoch 8793/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 1.0000\n",
            "Epoch 8793: val_loss improved from 0.00159 to 0.00143, saving model to ./\n",
            "INFO:tensorflow:Assets written to: ./assets\n",
            "7/7 [==============================] - 2s 359ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "Epoch 8794/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 8794: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0389 - val_accuracy: 0.9722\n",
            "Epoch 8795/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0120 - accuracy: 0.9946\n",
            "Epoch 8795: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0107 - accuracy: 0.9954 - val_loss: 0.0341 - val_accuracy: 0.9861\n",
            "Epoch 8796/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0412 - accuracy: 0.9783\n",
            "Epoch 8796: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0426 - accuracy: 0.9769 - val_loss: 0.0715 - val_accuracy: 0.9722\n",
            "Epoch 8797/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0419 - accuracy: 0.9861\n",
            "Epoch 8797: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 78ms/step - loss: 0.0419 - accuracy: 0.9861 - val_loss: 0.0203 - val_accuracy: 0.9861\n",
            "Epoch 8798/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0273 - accuracy: 0.9891\n",
            "Epoch 8798: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0235 - accuracy: 0.9907 - val_loss: 0.0212 - val_accuracy: 0.9861\n",
            "Epoch 8799/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0084 - accuracy: 1.0000\n",
            "Epoch 8799: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.3099 - val_accuracy: 0.9583\n",
            "Epoch 8800/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 8800: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0254 - val_accuracy: 0.9861\n",
            "Epoch 8801/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0129 - accuracy: 0.9946\n",
            "Epoch 8801: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0113 - accuracy: 0.9954 - val_loss: 0.0351 - val_accuracy: 0.9861\n",
            "Epoch 8802/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0077 - accuracy: 1.0000\n",
            "Epoch 8802: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0256 - val_accuracy: 1.0000\n",
            "Epoch 8803/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0081 - accuracy: 1.0000\n",
            "Epoch 8803: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.0352 - val_accuracy: 0.9722\n",
            "Epoch 8804/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0062 - accuracy: 1.0000\n",
            "Epoch 8804: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 83ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0581 - val_accuracy: 0.9861\n",
            "Epoch 8805/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 1.0000\n",
            "Epoch 8805: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.0488 - val_accuracy: 0.9861\n",
            "Epoch 8806/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 8806: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0079 - accuracy: 0.9954 - val_loss: 0.0258 - val_accuracy: 0.9861\n",
            "Epoch 8807/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0231 - accuracy: 0.9891\n",
            "Epoch 8807: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0200 - accuracy: 0.9907 - val_loss: 0.0536 - val_accuracy: 0.9861\n",
            "Epoch 8808/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 8808: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 76ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0350 - val_accuracy: 0.9861\n",
            "Epoch 8809/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0078 - accuracy: 0.9946\n",
            "Epoch 8809: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0086 - accuracy: 0.9954 - val_loss: 0.0318 - val_accuracy: 0.9861\n",
            "Epoch 8810/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0123 - accuracy: 0.9946\n",
            "Epoch 8810: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0118 - accuracy: 0.9954 - val_loss: 0.0589 - val_accuracy: 0.9861\n",
            "Epoch 8811/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9954\n",
            "Epoch 8811: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0144 - accuracy: 0.9954 - val_loss: 0.0213 - val_accuracy: 1.0000\n",
            "Epoch 8812/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0130 - accuracy: 0.9946\n",
            "Epoch 8812: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0133 - accuracy: 0.9954 - val_loss: 0.0808 - val_accuracy: 0.9722\n",
            "Epoch 8813/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0232 - accuracy: 0.9946\n",
            "Epoch 8813: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0221 - accuracy: 0.9954 - val_loss: 0.0238 - val_accuracy: 0.9861\n",
            "Epoch 8814/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0107 - accuracy: 0.9954\n",
            "Epoch 8814: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0107 - accuracy: 0.9954 - val_loss: 0.0255 - val_accuracy: 0.9861\n",
            "Epoch 8815/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0170 - accuracy: 0.9946\n",
            "Epoch 8815: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0173 - accuracy: 0.9954 - val_loss: 0.0553 - val_accuracy: 0.9722\n",
            "Epoch 8816/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0517 - accuracy: 0.9946\n",
            "Epoch 8816: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0554 - accuracy: 0.9907 - val_loss: 0.0098 - val_accuracy: 1.0000\n",
            "Epoch 8817/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0402 - accuracy: 0.9946\n",
            "Epoch 8817: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0345 - accuracy: 0.9954 - val_loss: 0.0663 - val_accuracy: 0.9861\n",
            "Epoch 8818/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0273 - accuracy: 0.9837\n",
            "Epoch 8818: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0242 - accuracy: 0.9861 - val_loss: 0.0262 - val_accuracy: 0.9861\n",
            "Epoch 8819/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0252 - accuracy: 0.9946\n",
            "Epoch 8819: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0229 - accuracy: 0.9954 - val_loss: 0.0286 - val_accuracy: 0.9861\n",
            "Epoch 8820/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0108 - accuracy: 1.0000\n",
            "Epoch 8820: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.0395 - val_accuracy: 0.9722\n",
            "Epoch 8821/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0092 - accuracy: 0.9948\n",
            "Epoch 8821: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 71ms/step - loss: 0.0082 - accuracy: 0.9954 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
            "Epoch 8822/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0112 - accuracy: 1.0000\n",
            "Epoch 8822: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.0149 - val_accuracy: 1.0000\n",
            "Epoch 8823/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9954\n",
            "Epoch 8823: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0122 - accuracy: 0.9954 - val_loss: 0.0264 - val_accuracy: 0.9861\n",
            "Epoch 8824/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 1.0000\n",
            "Epoch 8824: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0147 - val_accuracy: 0.9861\n",
            "Epoch 8825/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0178 - accuracy: 0.9946\n",
            "Epoch 8825: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0153 - accuracy: 0.9954 - val_loss: 0.0103 - val_accuracy: 1.0000\n",
            "Epoch 8826/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 8826: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9861\n",
            "Epoch 8827/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 8827: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0326 - val_accuracy: 0.9861\n",
            "Epoch 8828/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0079 - accuracy: 1.0000\n",
            "Epoch 8828: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.0211 - val_accuracy: 0.9861\n",
            "Epoch 8829/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 8829: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0159 - val_accuracy: 1.0000\n",
            "Epoch 8830/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 8830: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0145 - val_accuracy: 0.9861\n",
            "Epoch 8831/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 1.0000\n",
            "Epoch 8831: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.0170 - val_accuracy: 0.9861\n",
            "Epoch 8832/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0053 - accuracy: 1.0000\n",
            "Epoch 8832: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0193 - val_accuracy: 1.0000\n",
            "Epoch 8833/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 8833: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0218 - val_accuracy: 0.9861\n",
            "Epoch 8834/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0320 - accuracy: 0.9948\n",
            "Epoch 8834: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0288 - accuracy: 0.9954 - val_loss: 0.0486 - val_accuracy: 0.9722\n",
            "Epoch 8835/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 8835: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0291 - val_accuracy: 0.9861\n",
            "Epoch 8836/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 8836: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0656 - val_accuracy: 0.9722\n",
            "Epoch 8837/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 8837: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0089 - val_accuracy: 1.0000\n",
            "Epoch 8838/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 8838: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0132 - val_accuracy: 1.0000\n",
            "Epoch 8839/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0058 - accuracy: 1.0000\n",
            "Epoch 8839: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0172 - val_accuracy: 0.9861\n",
            "Epoch 8840/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9954\n",
            "Epoch 8840: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0145 - accuracy: 0.9954 - val_loss: 0.0591 - val_accuracy: 0.9722\n",
            "Epoch 8841/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0048 - accuracy: 1.0000\n",
            "Epoch 8841: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 0.9861\n",
            "Epoch 8842/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0144 - accuracy: 0.9946\n",
            "Epoch 8842: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0126 - accuracy: 0.9954 - val_loss: 0.0513 - val_accuracy: 0.9722\n",
            "Epoch 8843/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0232 - accuracy: 0.9946\n",
            "Epoch 8843: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0210 - accuracy: 0.9954 - val_loss: 0.0241 - val_accuracy: 0.9861\n",
            "Epoch 8844/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9907\n",
            "Epoch 8844: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0174 - accuracy: 0.9907 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
            "Epoch 8845/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9954\n",
            "Epoch 8845: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0127 - accuracy: 0.9954 - val_loss: 0.0758 - val_accuracy: 0.9722\n",
            "Epoch 8846/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0192 - accuracy: 0.9896\n",
            "Epoch 8846: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0173 - accuracy: 0.9907 - val_loss: 0.0127 - val_accuracy: 1.0000\n",
            "Epoch 8847/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0342 - accuracy: 0.9783\n",
            "Epoch 8847: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0302 - accuracy: 0.9815 - val_loss: 0.0362 - val_accuracy: 0.9861\n",
            "Epoch 8848/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0124 - accuracy: 0.9946\n",
            "Epoch 8848: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0297 - accuracy: 0.9907 - val_loss: 0.0357 - val_accuracy: 0.9722\n",
            "Epoch 8849/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0567 - accuracy: 0.9837\n",
            "Epoch 8849: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0483 - accuracy: 0.9861 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
            "Epoch 8850/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9954\n",
            "Epoch 8850: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0226 - accuracy: 0.9954 - val_loss: 0.0989 - val_accuracy: 0.9861\n",
            "Epoch 8851/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 8851: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0565 - val_accuracy: 0.9722\n",
            "Epoch 8852/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 8852: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0084 - accuracy: 0.9954 - val_loss: 0.0462 - val_accuracy: 0.9861\n",
            "Epoch 8853/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.5390 - accuracy: 0.9676\n",
            "Epoch 8853: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.5390 - accuracy: 0.9676 - val_loss: 1.5576 - val_accuracy: 0.7917\n",
            "Epoch 8854/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 1.4357 - accuracy: 0.7826\n",
            "Epoch 8854: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 1.4577 - accuracy: 0.7870 - val_loss: 0.8934 - val_accuracy: 0.8194\n",
            "Epoch 8855/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.9135 - accuracy: 0.8565\n",
            "Epoch 8855: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.9135 - accuracy: 0.8565 - val_loss: 1.0129 - val_accuracy: 0.8194\n",
            "Epoch 8856/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.9774 - accuracy: 0.8152\n",
            "Epoch 8856: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.8792 - accuracy: 0.8333 - val_loss: 0.8689 - val_accuracy: 0.8333\n",
            "Epoch 8857/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.4707 - accuracy: 0.8859\n",
            "Epoch 8857: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.4103 - accuracy: 0.8981 - val_loss: 0.4652 - val_accuracy: 0.8750\n",
            "Epoch 8858/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.3275 - accuracy: 0.9130\n",
            "Epoch 8858: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.3336 - accuracy: 0.9028 - val_loss: 0.5169 - val_accuracy: 0.7778\n",
            "Epoch 8859/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.3244 - accuracy: 0.9076\n",
            "Epoch 8859: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.3277 - accuracy: 0.9074 - val_loss: 0.3312 - val_accuracy: 0.8889\n",
            "Epoch 8860/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.1219 - accuracy: 0.9511\n",
            "Epoch 8860: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 76ms/step - loss: 0.1436 - accuracy: 0.9491 - val_loss: 0.3025 - val_accuracy: 0.8750\n",
            "Epoch 8861/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0836 - accuracy: 0.9620\n",
            "Epoch 8861: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0719 - accuracy: 0.9676 - val_loss: 0.3078 - val_accuracy: 0.9167\n",
            "Epoch 8862/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9815\n",
            "Epoch 8862: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 0.0771 - accuracy: 0.9815 - val_loss: 0.2781 - val_accuracy: 0.9306\n",
            "Epoch 8863/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0721 - accuracy: 0.9674\n",
            "Epoch 8863: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0668 - accuracy: 0.9722 - val_loss: 0.2835 - val_accuracy: 0.9167\n",
            "Epoch 8864/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1931 - accuracy: 0.9398\n",
            "Epoch 8864: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 0.1931 - accuracy: 0.9398 - val_loss: 0.3843 - val_accuracy: 0.9306\n",
            "Epoch 8865/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9676\n",
            "Epoch 8865: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 76ms/step - loss: 0.0718 - accuracy: 0.9676 - val_loss: 0.1273 - val_accuracy: 0.9444\n",
            "Epoch 8866/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1330 - accuracy: 0.9630\n",
            "Epoch 8866: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.1330 - accuracy: 0.9630 - val_loss: 0.1373 - val_accuracy: 0.9167\n",
            "Epoch 8867/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0400 - accuracy: 0.9896\n",
            "Epoch 8867: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0397 - accuracy: 0.9907 - val_loss: 0.0459 - val_accuracy: 0.9861\n",
            "Epoch 8868/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0413 - accuracy: 0.9891\n",
            "Epoch 8868: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 76ms/step - loss: 0.0385 - accuracy: 0.9907 - val_loss: 0.0789 - val_accuracy: 0.9722\n",
            "Epoch 8869/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0159 - accuracy: 1.0000\n",
            "Epoch 8869: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.0458 - val_accuracy: 0.9861\n",
            "Epoch 8870/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0115 - accuracy: 1.0000\n",
            "Epoch 8870: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0164 - accuracy: 0.9954 - val_loss: 0.0822 - val_accuracy: 0.9444\n",
            "Epoch 8871/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0419 - accuracy: 0.9946\n",
            "Epoch 8871: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0371 - accuracy: 0.9954 - val_loss: 0.0950 - val_accuracy: 0.9306\n",
            "Epoch 8872/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0441 - accuracy: 0.9844\n",
            "Epoch 8872: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0393 - accuracy: 0.9861 - val_loss: 0.1365 - val_accuracy: 0.9722\n",
            "Epoch 8873/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0118 - accuracy: 1.0000\n",
            "Epoch 8873: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.1119 - val_accuracy: 0.9583\n",
            "Epoch 8874/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0236 - accuracy: 0.9891\n",
            "Epoch 8874: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0225 - accuracy: 0.9907 - val_loss: 0.0362 - val_accuracy: 0.9861\n",
            "Epoch 8875/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0661 - accuracy: 0.9891\n",
            "Epoch 8875: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0567 - accuracy: 0.9907 - val_loss: 0.0651 - val_accuracy: 0.9583\n",
            "Epoch 8876/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0322 - accuracy: 0.9837\n",
            "Epoch 8876: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0280 - accuracy: 0.9861 - val_loss: 0.0278 - val_accuracy: 0.9861\n",
            "Epoch 8877/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0489 - accuracy: 0.9792\n",
            "Epoch 8877: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0442 - accuracy: 0.9815 - val_loss: 0.0309 - val_accuracy: 1.0000\n",
            "Epoch 8878/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0407 - accuracy: 0.9891\n",
            "Epoch 8878: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0436 - accuracy: 0.9861 - val_loss: 0.0399 - val_accuracy: 0.9861\n",
            "Epoch 8879/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 1.0000\n",
            "Epoch 8879: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9861\n",
            "Epoch 8880/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0250 - accuracy: 0.9891\n",
            "Epoch 8880: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 79ms/step - loss: 0.0228 - accuracy: 0.9907 - val_loss: 0.0493 - val_accuracy: 0.9722\n",
            "Epoch 8881/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0061 - accuracy: 1.0000\n",
            "Epoch 8881: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0372 - val_accuracy: 0.9861\n",
            "Epoch 8882/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9954\n",
            "Epoch 8882: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0120 - accuracy: 0.9954 - val_loss: 0.0532 - val_accuracy: 0.9583\n",
            "Epoch 8883/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 1.0000\n",
            "Epoch 8883: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.0402 - val_accuracy: 0.9861\n",
            "Epoch 8884/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9907\n",
            "Epoch 8884: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 70ms/step - loss: 0.0169 - accuracy: 0.9907 - val_loss: 0.0395 - val_accuracy: 0.9722\n",
            "Epoch 8885/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 0.9954\n",
            "Epoch 8885: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0097 - accuracy: 0.9954 - val_loss: 0.1067 - val_accuracy: 0.9722\n",
            "Epoch 8886/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 8886: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.0263 - val_accuracy: 0.9861\n",
            "Epoch 8887/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0287 - accuracy: 0.9946\n",
            "Epoch 8887: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0250 - accuracy: 0.9954 - val_loss: 0.0949 - val_accuracy: 0.9722\n",
            "Epoch 8888/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0077 - accuracy: 1.0000\n",
            "Epoch 8888: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0267 - val_accuracy: 0.9861\n",
            "Epoch 8889/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0152 - accuracy: 0.9946\n",
            "Epoch 8889: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.0585 - val_accuracy: 0.9722\n",
            "Epoch 8890/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 1.0000\n",
            "Epoch 8890: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0837 - val_accuracy: 0.9722\n",
            "Epoch 8891/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9954\n",
            "Epoch 8891: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0128 - accuracy: 0.9954 - val_loss: 0.0347 - val_accuracy: 0.9861\n",
            "Epoch 8892/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0147 - accuracy: 0.9946\n",
            "Epoch 8892: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0134 - accuracy: 0.9954 - val_loss: 0.1629 - val_accuracy: 0.9583\n",
            "Epoch 8893/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0202 - accuracy: 0.9891\n",
            "Epoch 8893: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0190 - accuracy: 0.9907 - val_loss: 0.1043 - val_accuracy: 0.9583\n",
            "Epoch 8894/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9954\n",
            "Epoch 8894: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0100 - accuracy: 0.9954 - val_loss: 0.0215 - val_accuracy: 0.9861\n",
            "Epoch 8895/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0068 - accuracy: 1.0000\n",
            "Epoch 8895: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0448 - val_accuracy: 0.9722\n",
            "Epoch 8896/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0080 - accuracy: 1.0000\n",
            "Epoch 8896: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.0156 - val_accuracy: 1.0000\n",
            "Epoch 8897/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 1.0000\n",
            "Epoch 8897: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0742 - val_accuracy: 0.9583\n",
            "Epoch 8898/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0096 - accuracy: 1.0000\n",
            "Epoch 8898: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.0664 - val_accuracy: 0.9722\n",
            "Epoch 8899/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 1.0000\n",
            "Epoch 8899: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9861\n",
            "Epoch 8900/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0314 - accuracy: 0.9837\n",
            "Epoch 8900: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0280 - accuracy: 0.9861 - val_loss: 0.0616 - val_accuracy: 0.9722\n",
            "Epoch 8901/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 1.0000\n",
            "Epoch 8901: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.0751 - val_accuracy: 0.9583\n",
            "Epoch 8902/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0151 - accuracy: 0.9946\n",
            "Epoch 8902: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.0481 - val_accuracy: 0.9861\n",
            "Epoch 8903/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0113 - accuracy: 0.9946\n",
            "Epoch 8903: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0097 - accuracy: 0.9954 - val_loss: 0.0527 - val_accuracy: 0.9722\n",
            "Epoch 8904/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0226 - accuracy: 0.9948\n",
            "Epoch 8904: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0205 - accuracy: 0.9954 - val_loss: 0.0228 - val_accuracy: 0.9861\n",
            "Epoch 8905/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0170 - accuracy: 0.9891\n",
            "Epoch 8905: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0146 - accuracy: 0.9907 - val_loss: 0.0254 - val_accuracy: 0.9861\n",
            "Epoch 8906/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9954\n",
            "Epoch 8906: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0124 - accuracy: 0.9954 - val_loss: 0.0324 - val_accuracy: 0.9861\n",
            "Epoch 8907/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9907\n",
            "Epoch 8907: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0211 - accuracy: 0.9907 - val_loss: 0.0244 - val_accuracy: 0.9861\n",
            "Epoch 8908/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 0.9861\n",
            "Epoch 8908: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0343 - accuracy: 0.9861 - val_loss: 0.0592 - val_accuracy: 0.9722\n",
            "Epoch 8909/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9954\n",
            "Epoch 8909: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0180 - accuracy: 0.9954 - val_loss: 0.0947 - val_accuracy: 0.9583\n",
            "Epoch 8910/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0240 - accuracy: 0.9946\n",
            "Epoch 8910: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0212 - accuracy: 0.9954 - val_loss: 0.0178 - val_accuracy: 1.0000\n",
            "Epoch 8911/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0058 - accuracy: 1.0000\n",
            "Epoch 8911: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0135 - val_accuracy: 1.0000\n",
            "Epoch 8912/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0051 - accuracy: 1.0000\n",
            "Epoch 8912: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0430 - val_accuracy: 0.9861\n",
            "Epoch 8913/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0076 - accuracy: 1.0000\n",
            "Epoch 8913: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0321 - val_accuracy: 0.9861\n",
            "Epoch 8914/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0158 - accuracy: 0.9946\n",
            "Epoch 8914: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 62ms/step - loss: 0.0145 - accuracy: 0.9954 - val_loss: 0.0311 - val_accuracy: 0.9861\n",
            "Epoch 8915/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0058 - accuracy: 1.0000\n",
            "Epoch 8915: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0368 - val_accuracy: 0.9861\n",
            "Epoch 8916/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9954\n",
            "Epoch 8916: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0199 - accuracy: 0.9954 - val_loss: 0.0247 - val_accuracy: 0.9861\n",
            "Epoch 8917/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0082 - accuracy: 1.0000\n",
            "Epoch 8917: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 62ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0281 - val_accuracy: 0.9861\n",
            "Epoch 8918/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0534 - accuracy: 0.9728\n",
            "Epoch 8918: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0472 - accuracy: 0.9769 - val_loss: 0.0215 - val_accuracy: 0.9861\n",
            "Epoch 8919/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9954\n",
            "Epoch 8919: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0169 - accuracy: 0.9954 - val_loss: 0.1630 - val_accuracy: 0.9583\n",
            "Epoch 8920/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 0.9861\n",
            "Epoch 8920: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0354 - accuracy: 0.9861 - val_loss: 0.0669 - val_accuracy: 0.9722\n",
            "Epoch 8921/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0126 - accuracy: 0.9946\n",
            "Epoch 8921: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0205 - accuracy: 0.9907 - val_loss: 0.0755 - val_accuracy: 0.9583\n",
            "Epoch 8922/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9769\n",
            "Epoch 8922: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0749 - accuracy: 0.9769 - val_loss: 0.1495 - val_accuracy: 0.9583\n",
            "Epoch 8923/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0760 - accuracy: 0.9815\n",
            "Epoch 8923: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0760 - accuracy: 0.9815 - val_loss: 0.0716 - val_accuracy: 0.9861\n",
            "Epoch 8924/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0667 - accuracy: 0.9620\n",
            "Epoch 8924: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0584 - accuracy: 0.9676 - val_loss: 0.0763 - val_accuracy: 0.9306\n",
            "Epoch 8925/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0767 - accuracy: 0.9728\n",
            "Epoch 8925: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0685 - accuracy: 0.9769 - val_loss: 0.0450 - val_accuracy: 0.9722\n",
            "Epoch 8926/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0215 - accuracy: 0.9891\n",
            "Epoch 8926: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0195 - accuracy: 0.9907 - val_loss: 0.1320 - val_accuracy: 0.9722\n",
            "Epoch 8927/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0283 - accuracy: 0.9896\n",
            "Epoch 8927: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0256 - accuracy: 0.9907 - val_loss: 0.0524 - val_accuracy: 0.9722\n",
            "Epoch 8928/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9861\n",
            "Epoch 8928: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0199 - accuracy: 0.9861 - val_loss: 0.0358 - val_accuracy: 0.9861\n",
            "Epoch 8929/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9769\n",
            "Epoch 8929: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0431 - accuracy: 0.9769 - val_loss: 0.0570 - val_accuracy: 0.9722\n",
            "Epoch 8930/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9907\n",
            "Epoch 8930: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0292 - accuracy: 0.9907 - val_loss: 0.0358 - val_accuracy: 0.9722\n",
            "Epoch 8931/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0090 - accuracy: 1.0000\n",
            "Epoch 8931: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.0109 - val_accuracy: 1.0000\n",
            "Epoch 8932/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9954\n",
            "Epoch 8932: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0186 - accuracy: 0.9954 - val_loss: 0.0259 - val_accuracy: 0.9861\n",
            "Epoch 8933/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 8933: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9861\n",
            "Epoch 8934/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0149 - accuracy: 0.9891\n",
            "Epoch 8934: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0130 - accuracy: 0.9907 - val_loss: 0.1054 - val_accuracy: 0.9583\n",
            "Epoch 8935/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0149 - accuracy: 0.9946\n",
            "Epoch 8935: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0133 - accuracy: 0.9954 - val_loss: 0.0657 - val_accuracy: 0.9722\n",
            "Epoch 8936/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0111 - accuracy: 1.0000\n",
            "Epoch 8936: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.0325 - val_accuracy: 0.9722\n",
            "Epoch 8937/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9954\n",
            "Epoch 8937: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0238 - accuracy: 0.9954 - val_loss: 0.0067 - val_accuracy: 1.0000\n",
            "Epoch 8938/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 8938: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0126 - accuracy: 0.9954 - val_loss: 0.0149 - val_accuracy: 1.0000\n",
            "Epoch 8939/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 8939: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0357 - val_accuracy: 0.9722\n",
            "Epoch 8940/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9907\n",
            "Epoch 8940: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0122 - accuracy: 0.9907 - val_loss: 0.0433 - val_accuracy: 0.9861\n",
            "Epoch 8941/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9907\n",
            "Epoch 8941: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0164 - accuracy: 0.9907 - val_loss: 0.0903 - val_accuracy: 0.9722\n",
            "Epoch 8942/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0087 - accuracy: 1.0000\n",
            "Epoch 8942: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.0216 - val_accuracy: 0.9861\n",
            "Epoch 8943/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0112 - accuracy: 0.9946\n",
            "Epoch 8943: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0096 - accuracy: 0.9954 - val_loss: 0.0180 - val_accuracy: 1.0000\n",
            "Epoch 8944/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9861\n",
            "Epoch 8944: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0219 - accuracy: 0.9861 - val_loss: 0.0407 - val_accuracy: 0.9861\n",
            "Epoch 8945/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 1.0000\n",
            "Epoch 8945: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.1555 - val_accuracy: 0.9583\n",
            "Epoch 8946/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0079 - accuracy: 1.0000\n",
            "Epoch 8946: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 81ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0385 - val_accuracy: 0.9861\n",
            "Epoch 8947/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 1.0000\n",
            "Epoch 8947: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.0194 - val_accuracy: 1.0000\n",
            "Epoch 8948/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 0.9954\n",
            "Epoch 8948: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0078 - accuracy: 0.9954 - val_loss: 0.0140 - val_accuracy: 1.0000\n",
            "Epoch 8949/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 8949: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0300 - val_accuracy: 0.9861\n",
            "Epoch 8950/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0156 - accuracy: 0.9946\n",
            "Epoch 8950: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0164 - accuracy: 0.9954 - val_loss: 0.0206 - val_accuracy: 0.9861\n",
            "Epoch 8951/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0053 - accuracy: 1.0000\n",
            "Epoch 8951: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
            "Epoch 8952/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 8952: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0337 - val_accuracy: 0.9861\n",
            "Epoch 8953/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 8953: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0191 - val_accuracy: 1.0000\n",
            "Epoch 8954/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 8954: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
            "Epoch 8955/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.9954\n",
            "Epoch 8955: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0114 - accuracy: 0.9954 - val_loss: 0.0427 - val_accuracy: 0.9722\n",
            "Epoch 8956/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0079 - accuracy: 0.9946\n",
            "Epoch 8956: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0085 - accuracy: 0.9954 - val_loss: 0.0254 - val_accuracy: 0.9861\n",
            "Epoch 8957/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0147 - accuracy: 0.9946\n",
            "Epoch 8957: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0127 - accuracy: 0.9954 - val_loss: 0.0190 - val_accuracy: 1.0000\n",
            "Epoch 8958/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0051 - accuracy: 1.0000\n",
            "Epoch 8958: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0451 - val_accuracy: 0.9861\n",
            "Epoch 8959/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9954\n",
            "Epoch 8959: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.0619 - val_accuracy: 0.9583\n",
            "Epoch 8960/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0126 - accuracy: 1.0000\n",
            "Epoch 8960: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.0099 - val_accuracy: 1.0000\n",
            "Epoch 8961/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0112 - accuracy: 0.9946\n",
            "Epoch 8961: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0105 - accuracy: 0.9954 - val_loss: 0.0149 - val_accuracy: 1.0000\n",
            "Epoch 8962/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0055 - accuracy: 1.0000\n",
            "Epoch 8962: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 61ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.0274 - val_accuracy: 0.9861\n",
            "Epoch 8963/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0102 - accuracy: 1.0000\n",
            "Epoch 8963: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0280 - accuracy: 0.9954 - val_loss: 0.0707 - val_accuracy: 0.9722\n",
            "Epoch 8964/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9954\n",
            "Epoch 8964: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 62ms/step - loss: 0.0141 - accuracy: 0.9954 - val_loss: 0.0088 - val_accuracy: 1.0000\n",
            "Epoch 8965/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0058 - accuracy: 1.0000\n",
            "Epoch 8965: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
            "Epoch 8966/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9954\n",
            "Epoch 8966: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0167 - accuracy: 0.9954 - val_loss: 0.0485 - val_accuracy: 0.9722\n",
            "Epoch 8967/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9861\n",
            "Epoch 8967: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0296 - accuracy: 0.9861 - val_loss: 0.0249 - val_accuracy: 0.9861\n",
            "Epoch 8968/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0074 - accuracy: 1.0000\n",
            "Epoch 8968: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.0491 - val_accuracy: 0.9861\n",
            "Epoch 8969/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0398 - accuracy: 0.9907\n",
            "Epoch 8969: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0398 - accuracy: 0.9907 - val_loss: 0.0240 - val_accuracy: 0.9861\n",
            "Epoch 8970/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0128 - accuracy: 1.0000\n",
            "Epoch 8970: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.0309 - val_accuracy: 0.9861\n",
            "Epoch 8971/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 1.0000\n",
            "Epoch 8971: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0242 - val_accuracy: 0.9861\n",
            "Epoch 8972/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 8972: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0299 - val_accuracy: 0.9861\n",
            "Epoch 8973/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.9954\n",
            "Epoch 8973: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0098 - accuracy: 0.9954 - val_loss: 0.0241 - val_accuracy: 0.9861\n",
            "Epoch 8974/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 8974: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9861\n",
            "Epoch 8975/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 1.0000\n",
            "Epoch 8975: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
            "Epoch 8976/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0075 - accuracy: 0.9954\n",
            "Epoch 8976: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0075 - accuracy: 0.9954 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
            "Epoch 8977/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0102 - accuracy: 0.9954\n",
            "Epoch 8977: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0102 - accuracy: 0.9954 - val_loss: 0.0095 - val_accuracy: 1.0000\n",
            "Epoch 8978/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0319 - accuracy: 0.9891\n",
            "Epoch 8978: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0288 - accuracy: 0.9907 - val_loss: 0.0097 - val_accuracy: 1.0000\n",
            "Epoch 8979/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9907\n",
            "Epoch 8979: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0159 - accuracy: 0.9907 - val_loss: 0.0350 - val_accuracy: 0.9861\n",
            "Epoch 8980/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0184 - accuracy: 0.9946\n",
            "Epoch 8980: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0183 - accuracy: 0.9954 - val_loss: 0.0431 - val_accuracy: 0.9861\n",
            "Epoch 8981/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9954\n",
            "Epoch 8981: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0073 - accuracy: 0.9954 - val_loss: 0.0559 - val_accuracy: 0.9722\n",
            "Epoch 8982/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0089 - accuracy: 1.0000\n",
            "Epoch 8982: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.0180 - val_accuracy: 0.9861\n",
            "Epoch 8983/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9954\n",
            "Epoch 8983: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0162 - accuracy: 0.9954 - val_loss: 0.0184 - val_accuracy: 0.9861\n",
            "Epoch 8984/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 8984: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0301 - val_accuracy: 0.9861\n",
            "Epoch 8985/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9954\n",
            "Epoch 8985: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0120 - accuracy: 0.9954 - val_loss: 0.0376 - val_accuracy: 0.9722\n",
            "Epoch 8986/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0227 - accuracy: 0.9946\n",
            "Epoch 8986: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0213 - accuracy: 0.9954 - val_loss: 0.0300 - val_accuracy: 0.9861\n",
            "Epoch 8987/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0107 - accuracy: 0.9954\n",
            "Epoch 8987: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0107 - accuracy: 0.9954 - val_loss: 0.0332 - val_accuracy: 0.9861\n",
            "Epoch 8988/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0070 - accuracy: 0.9946\n",
            "Epoch 8988: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0060 - accuracy: 0.9954 - val_loss: 0.0404 - val_accuracy: 0.9722\n",
            "Epoch 8989/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 1.0000\n",
            "Epoch 8989: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 71ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0412 - val_accuracy: 0.9722\n",
            "Epoch 8990/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0193 - accuracy: 0.9837\n",
            "Epoch 8990: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0166 - accuracy: 0.9861 - val_loss: 0.0062 - val_accuracy: 1.0000\n",
            "Epoch 8991/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.9954\n",
            "Epoch 8991: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0083 - accuracy: 0.9954 - val_loss: 0.0069 - val_accuracy: 1.0000\n",
            "Epoch 8992/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0233 - accuracy: 0.9946\n",
            "Epoch 8992: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0300 - accuracy: 0.9907 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
            "Epoch 8993/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9861\n",
            "Epoch 8993: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0310 - accuracy: 0.9861 - val_loss: 0.0407 - val_accuracy: 0.9861\n",
            "Epoch 8994/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0081 - accuracy: 1.0000\n",
            "Epoch 8994: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0216 - val_accuracy: 0.9861\n",
            "Epoch 8995/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0171 - accuracy: 0.9948\n",
            "Epoch 8995: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0163 - accuracy: 0.9954 - val_loss: 0.0680 - val_accuracy: 0.9722\n",
            "Epoch 8996/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0137 - accuracy: 0.9948\n",
            "Epoch 8996: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0127 - accuracy: 0.9954 - val_loss: 0.0128 - val_accuracy: 1.0000\n",
            "Epoch 8997/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 1.0000\n",
            "Epoch 8997: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0184 - val_accuracy: 1.0000\n",
            "Epoch 8998/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9907\n",
            "Epoch 8998: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 78ms/step - loss: 0.0190 - accuracy: 0.9907 - val_loss: 0.0332 - val_accuracy: 0.9861\n",
            "Epoch 8999/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0088 - accuracy: 1.0000\n",
            "Epoch 8999: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0410 - val_accuracy: 0.9861\n",
            "Epoch 9000/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 1.0000\n",
            "Epoch 9000: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.0091 - val_accuracy: 1.0000\n",
            "Epoch 9001/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0082 - accuracy: 1.0000\n",
            "Epoch 9001: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
            "Epoch 9002/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0168 - accuracy: 0.9948\n",
            "Epoch 9002: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0150 - accuracy: 0.9954 - val_loss: 0.0435 - val_accuracy: 0.9861\n",
            "Epoch 9003/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0176 - accuracy: 0.9948\n",
            "Epoch 9003: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0158 - accuracy: 0.9954 - val_loss: 0.0595 - val_accuracy: 0.9583\n",
            "Epoch 9004/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9954\n",
            "Epoch 9004: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0133 - accuracy: 0.9954 - val_loss: 0.0088 - val_accuracy: 1.0000\n",
            "Epoch 9005/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0233 - accuracy: 0.9896\n",
            "Epoch 9005: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0211 - accuracy: 0.9907 - val_loss: 0.0283 - val_accuracy: 0.9861\n",
            "Epoch 9006/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0160 - accuracy: 0.9948\n",
            "Epoch 9006: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0207 - accuracy: 0.9907 - val_loss: 0.0696 - val_accuracy: 0.9861\n",
            "Epoch 9007/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0145 - accuracy: 0.9946\n",
            "Epoch 9007: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0128 - accuracy: 0.9954 - val_loss: 0.0157 - val_accuracy: 1.0000\n",
            "Epoch 9008/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0128 - accuracy: 0.9946\n",
            "Epoch 9008: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0148 - accuracy: 0.9954 - val_loss: 0.0056 - val_accuracy: 1.0000\n",
            "Epoch 9009/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9954\n",
            "Epoch 9009: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0118 - accuracy: 0.9954 - val_loss: 0.0161 - val_accuracy: 0.9861\n",
            "Epoch 9010/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0130 - accuracy: 0.9946\n",
            "Epoch 9010: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0111 - accuracy: 0.9954 - val_loss: 0.0088 - val_accuracy: 1.0000\n",
            "Epoch 9011/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 9011: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0140 - val_accuracy: 1.0000\n",
            "Epoch 9012/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 1.0000\n",
            "Epoch 9012: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.0180 - val_accuracy: 0.9861\n",
            "Epoch 9013/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9907\n",
            "Epoch 9013: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0260 - accuracy: 0.9907 - val_loss: 0.0377 - val_accuracy: 0.9722\n",
            "Epoch 9014/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0069 - accuracy: 1.0000\n",
            "Epoch 9014: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.0435 - val_accuracy: 0.9722\n",
            "Epoch 9015/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9907\n",
            "Epoch 9015: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0127 - accuracy: 0.9907 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
            "Epoch 9016/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0065 - accuracy: 1.0000\n",
            "Epoch 9016: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.1021 - val_accuracy: 0.9722\n",
            "Epoch 9017/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0099 - accuracy: 0.9946\n",
            "Epoch 9017: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0110 - accuracy: 0.9954 - val_loss: 0.0626 - val_accuracy: 0.9583\n",
            "Epoch 9018/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0103 - accuracy: 1.0000\n",
            "Epoch 9018: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.0256 - val_accuracy: 0.9861\n",
            "Epoch 9019/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 1.0000\n",
            "Epoch 9019: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.0216 - val_accuracy: 1.0000\n",
            "Epoch 9020/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 9020: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.1096 - val_accuracy: 0.9722\n",
            "Epoch 9021/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 1.0000\n",
            "Epoch 9021: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.0483 - val_accuracy: 0.9722\n",
            "Epoch 9022/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0048 - accuracy: 1.0000\n",
            "Epoch 9022: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.0467 - val_accuracy: 0.9861\n",
            "Epoch 9023/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9907\n",
            "Epoch 9023: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0178 - accuracy: 0.9907 - val_loss: 0.0048 - val_accuracy: 1.0000\n",
            "Epoch 9024/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0150 - accuracy: 0.9946\n",
            "Epoch 9024: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 78ms/step - loss: 0.0129 - accuracy: 0.9954 - val_loss: 0.0273 - val_accuracy: 0.9861\n",
            "Epoch 9025/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0108 - accuracy: 1.0000\n",
            "Epoch 9025: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.0313 - val_accuracy: 0.9861\n",
            "Epoch 9026/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0164 - accuracy: 0.9946\n",
            "Epoch 9026: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0171 - accuracy: 0.9954 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
            "Epoch 9027/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 1.0000\n",
            "Epoch 9027: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
            "Epoch 9028/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 9028: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0427 - val_accuracy: 0.9722\n",
            "Epoch 9029/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.9907\n",
            "Epoch 9029: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0098 - accuracy: 0.9907 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
            "Epoch 9030/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0225 - accuracy: 0.9896\n",
            "Epoch 9030: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0205 - accuracy: 0.9907 - val_loss: 0.0156 - val_accuracy: 1.0000\n",
            "Epoch 9031/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0078 - accuracy: 1.0000\n",
            "Epoch 9031: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0175 - accuracy: 0.9954 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
            "Epoch 9032/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 9032: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.0978 - val_accuracy: 0.9583\n",
            "Epoch 9033/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9907\n",
            "Epoch 9033: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0160 - accuracy: 0.9907 - val_loss: 0.0863 - val_accuracy: 0.9583\n",
            "Epoch 9034/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0096 - accuracy: 1.0000\n",
            "Epoch 9034: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.0144 - val_accuracy: 1.0000\n",
            "Epoch 9035/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0135 - accuracy: 0.9946\n",
            "Epoch 9035: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 62ms/step - loss: 0.0195 - accuracy: 0.9907 - val_loss: 0.0387 - val_accuracy: 0.9722\n",
            "Epoch 9036/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 9036: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.0137 - val_accuracy: 1.0000\n",
            "Epoch 9037/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9907\n",
            "Epoch 9037: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0124 - accuracy: 0.9907 - val_loss: 0.0527 - val_accuracy: 0.9722\n",
            "Epoch 9038/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 9038: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 0.9861\n",
            "Epoch 9039/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 9039: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9861\n",
            "Epoch 9040/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 9040: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0449 - val_accuracy: 0.9861\n",
            "Epoch 9041/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0110 - accuracy: 0.9946\n",
            "Epoch 9041: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0095 - accuracy: 0.9954 - val_loss: 0.0356 - val_accuracy: 0.9861\n",
            "Epoch 9042/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0053 - accuracy: 1.0000\n",
            "Epoch 9042: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0336 - val_accuracy: 0.9722\n",
            "Epoch 9043/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 9043: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0094 - val_accuracy: 1.0000\n",
            "Epoch 9044/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9954\n",
            "Epoch 9044: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0220 - accuracy: 0.9954 - val_loss: 0.0262 - val_accuracy: 0.9861\n",
            "Epoch 9045/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0075 - accuracy: 1.0000\n",
            "Epoch 9045: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0298 - val_accuracy: 1.0000\n",
            "Epoch 9046/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0303 - accuracy: 0.9891\n",
            "Epoch 9046: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0281 - accuracy: 0.9907 - val_loss: 0.0121 - val_accuracy: 1.0000\n",
            "Epoch 9047/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9954\n",
            "Epoch 9047: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0223 - accuracy: 0.9954 - val_loss: 0.0533 - val_accuracy: 0.9722\n",
            "Epoch 9048/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0521 - accuracy: 0.9891\n",
            "Epoch 9048: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0490 - accuracy: 0.9861 - val_loss: 0.1088 - val_accuracy: 0.9722\n",
            "Epoch 9049/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0847 - accuracy: 0.9565\n",
            "Epoch 9049: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0812 - accuracy: 0.9583 - val_loss: 0.0717 - val_accuracy: 0.9583\n",
            "Epoch 9050/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0121 - accuracy: 1.0000\n",
            "Epoch 9050: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.0066 - val_accuracy: 1.0000\n",
            "Epoch 9051/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0445 - accuracy: 0.9946\n",
            "Epoch 9051: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0419 - accuracy: 0.9907 - val_loss: 0.0209 - val_accuracy: 1.0000\n",
            "Epoch 9052/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9907\n",
            "Epoch 9052: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0219 - accuracy: 0.9907 - val_loss: 0.0468 - val_accuracy: 0.9722\n",
            "Epoch 9053/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9954\n",
            "Epoch 9053: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0184 - accuracy: 0.9954 - val_loss: 0.0163 - val_accuracy: 1.0000\n",
            "Epoch 9054/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0089 - accuracy: 0.9946\n",
            "Epoch 9054: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0095 - accuracy: 0.9954 - val_loss: 0.0346 - val_accuracy: 0.9861\n",
            "Epoch 9055/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0061 - accuracy: 1.0000\n",
            "Epoch 9055: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0431 - val_accuracy: 0.9861\n",
            "Epoch 9056/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0274 - accuracy: 0.9891\n",
            "Epoch 9056: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0239 - accuracy: 0.9907 - val_loss: 0.0220 - val_accuracy: 1.0000\n",
            "Epoch 9057/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 9057: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0340 - val_accuracy: 0.9722\n",
            "Epoch 9058/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 9058: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0190 - val_accuracy: 1.0000\n",
            "Epoch 9059/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9954\n",
            "Epoch 9059: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0159 - accuracy: 0.9954 - val_loss: 0.0309 - val_accuracy: 0.9861\n",
            "Epoch 9060/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0086 - accuracy: 0.9946\n",
            "Epoch 9060: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0083 - accuracy: 0.9954 - val_loss: 0.0060 - val_accuracy: 1.0000\n",
            "Epoch 9061/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0390 - accuracy: 0.9896\n",
            "Epoch 9061: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0354 - accuracy: 0.9907 - val_loss: 0.1329 - val_accuracy: 0.9722\n",
            "Epoch 9062/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9954\n",
            "Epoch 9062: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0188 - accuracy: 0.9954 - val_loss: 0.0698 - val_accuracy: 0.9722\n",
            "Epoch 9063/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0106 - accuracy: 0.9954\n",
            "Epoch 9063: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0106 - accuracy: 0.9954 - val_loss: 0.0109 - val_accuracy: 1.0000\n",
            "Epoch 9064/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9954\n",
            "Epoch 9064: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0241 - accuracy: 0.9954 - val_loss: 0.0721 - val_accuracy: 0.9722\n",
            "Epoch 9065/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 0.9861\n",
            "Epoch 9065: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0364 - accuracy: 0.9861 - val_loss: 0.0927 - val_accuracy: 0.9444\n",
            "Epoch 9066/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9907\n",
            "Epoch 9066: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 71ms/step - loss: 0.0236 - accuracy: 0.9907 - val_loss: 0.1527 - val_accuracy: 0.9306\n",
            "Epoch 9067/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0449 - accuracy: 0.9946\n",
            "Epoch 9067: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0402 - accuracy: 0.9954 - val_loss: 0.0281 - val_accuracy: 0.9861\n",
            "Epoch 9068/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0215 - accuracy: 0.9837\n",
            "Epoch 9068: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0185 - accuracy: 0.9861 - val_loss: 0.0299 - val_accuracy: 0.9861\n",
            "Epoch 9069/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0330 - accuracy: 0.9896\n",
            "Epoch 9069: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0299 - accuracy: 0.9907 - val_loss: 0.0095 - val_accuracy: 1.0000\n",
            "Epoch 9070/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 0.9907\n",
            "Epoch 9070: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0322 - accuracy: 0.9907 - val_loss: 0.0058 - val_accuracy: 1.0000\n",
            "Epoch 9071/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0141 - accuracy: 0.9946\n",
            "Epoch 9071: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0121 - accuracy: 0.9954 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
            "Epoch 9072/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0109 - accuracy: 1.0000\n",
            "Epoch 9072: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
            "Epoch 9073/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9954\n",
            "Epoch 9073: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0124 - accuracy: 0.9954 - val_loss: 0.0140 - val_accuracy: 1.0000\n",
            "Epoch 9074/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9907\n",
            "Epoch 9074: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0155 - accuracy: 0.9907 - val_loss: 0.0232 - val_accuracy: 0.9861\n",
            "Epoch 9075/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 9075: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "Epoch 9076/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0178 - accuracy: 0.9946\n",
            "Epoch 9076: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0160 - accuracy: 0.9954 - val_loss: 0.0069 - val_accuracy: 1.0000\n",
            "Epoch 9077/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0133 - accuracy: 1.0000\n",
            "Epoch 9077: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.0442 - val_accuracy: 0.9861\n",
            "Epoch 9078/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9861\n",
            "Epoch 9078: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0267 - accuracy: 0.9861 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
            "Epoch 9079/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 9079: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0075 - val_accuracy: 1.0000\n",
            "Epoch 9080/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0251 - accuracy: 0.9946\n",
            "Epoch 9080: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0216 - accuracy: 0.9954 - val_loss: 0.0514 - val_accuracy: 0.9722\n",
            "Epoch 9081/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0048 - accuracy: 1.0000\n",
            "Epoch 9081: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.0502 - val_accuracy: 0.9722\n",
            "Epoch 9082/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0063 - accuracy: 1.0000\n",
            "Epoch 9082: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0152 - val_accuracy: 1.0000\n",
            "Epoch 9083/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0048 - accuracy: 1.0000\n",
            "Epoch 9083: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.0499 - val_accuracy: 0.9722\n",
            "Epoch 9084/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0243 - accuracy: 0.9783\n",
            "Epoch 9084: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0237 - accuracy: 0.9815 - val_loss: 0.0288 - val_accuracy: 0.9861\n",
            "Epoch 9085/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.9954\n",
            "Epoch 9085: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0080 - accuracy: 0.9954 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
            "Epoch 9086/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0057 - accuracy: 1.0000\n",
            "Epoch 9086: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9861\n",
            "Epoch 9087/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9907\n",
            "Epoch 9087: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0223 - accuracy: 0.9907 - val_loss: 0.0139 - val_accuracy: 1.0000\n",
            "Epoch 9088/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0119 - accuracy: 0.9948\n",
            "Epoch 9088: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0106 - accuracy: 0.9954 - val_loss: 0.0225 - val_accuracy: 0.9861\n",
            "Epoch 9089/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 9089: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.0057 - val_accuracy: 1.0000\n",
            "Epoch 9090/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 9090: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0372 - val_accuracy: 0.9722\n",
            "Epoch 9091/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0061 - accuracy: 1.0000\n",
            "Epoch 9091: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 76ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0188 - val_accuracy: 0.9861\n",
            "Epoch 9092/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0067 - accuracy: 0.9948\n",
            "Epoch 9092: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0062 - accuracy: 0.9954 - val_loss: 0.0809 - val_accuracy: 0.9444\n",
            "Epoch 9093/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9954\n",
            "Epoch 9093: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0121 - accuracy: 0.9954 - val_loss: 0.0212 - val_accuracy: 1.0000\n",
            "Epoch 9094/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 1.0000\n",
            "Epoch 9094: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.0177 - val_accuracy: 0.9861\n",
            "Epoch 9095/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 1.0000\n",
            "Epoch 9095: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 79ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.0403 - val_accuracy: 0.9861\n",
            "Epoch 9096/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0053 - accuracy: 1.0000\n",
            "Epoch 9096: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0096 - val_accuracy: 1.0000\n",
            "Epoch 9097/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0106 - accuracy: 1.0000\n",
            "Epoch 9097: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.0155 - val_accuracy: 1.0000\n",
            "Epoch 9098/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0292 - accuracy: 0.9946\n",
            "Epoch 9098: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0259 - accuracy: 0.9954 - val_loss: 0.0546 - val_accuracy: 0.9722\n",
            "Epoch 9099/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9954\n",
            "Epoch 9099: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0307 - accuracy: 0.9954 - val_loss: 0.0752 - val_accuracy: 0.9722\n",
            "Epoch 9100/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0065 - accuracy: 1.0000\n",
            "Epoch 9100: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0340 - val_accuracy: 0.9722\n",
            "Epoch 9101/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0086 - accuracy: 1.0000\n",
            "Epoch 9101: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.0057 - val_accuracy: 1.0000\n",
            "Epoch 9102/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 9102: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0059 - val_accuracy: 1.0000\n",
            "Epoch 9103/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 9103: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0198 - val_accuracy: 0.9861\n",
            "Epoch 9104/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.9954\n",
            "Epoch 9104: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0089 - accuracy: 0.9954 - val_loss: 0.0155 - val_accuracy: 1.0000\n",
            "Epoch 9105/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 1.0000\n",
            "Epoch 9105: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.1240 - val_accuracy: 0.9722\n",
            "Epoch 9106/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 9106: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0066 - accuracy: 0.9954 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
            "Epoch 9107/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0471 - accuracy: 0.9907\n",
            "Epoch 9107: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 0.0471 - accuracy: 0.9907 - val_loss: 0.1291 - val_accuracy: 0.9861\n",
            "Epoch 9108/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0895 - accuracy: 0.9728\n",
            "Epoch 9108: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0782 - accuracy: 0.9769 - val_loss: 0.0302 - val_accuracy: 0.9861\n",
            "Epoch 9109/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0758 - accuracy: 0.9769\n",
            "Epoch 9109: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0758 - accuracy: 0.9769 - val_loss: 0.0150 - val_accuracy: 1.0000\n",
            "Epoch 9110/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0921 - accuracy: 0.9583\n",
            "Epoch 9110: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0921 - accuracy: 0.9583 - val_loss: 0.1154 - val_accuracy: 0.9444\n",
            "Epoch 9111/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0263 - accuracy: 0.9946\n",
            "Epoch 9111: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0300 - accuracy: 0.9907 - val_loss: 0.0490 - val_accuracy: 0.9722\n",
            "Epoch 9112/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0204 - accuracy: 0.9946\n",
            "Epoch 9112: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0238 - accuracy: 0.9954 - val_loss: 0.0124 - val_accuracy: 1.0000\n",
            "Epoch 9113/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0291 - accuracy: 0.9946\n",
            "Epoch 9113: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0252 - accuracy: 0.9954 - val_loss: 0.0888 - val_accuracy: 0.9861\n",
            "Epoch 9114/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9954\n",
            "Epoch 9114: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0129 - accuracy: 0.9954 - val_loss: 0.0160 - val_accuracy: 1.0000\n",
            "Epoch 9115/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9907\n",
            "Epoch 9115: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0187 - accuracy: 0.9907 - val_loss: 0.0483 - val_accuracy: 0.9722\n",
            "Epoch 9116/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9861\n",
            "Epoch 9116: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0373 - accuracy: 0.9861 - val_loss: 0.0856 - val_accuracy: 0.9444\n",
            "Epoch 9117/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0124 - accuracy: 0.9948\n",
            "Epoch 9117: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0120 - accuracy: 0.9954 - val_loss: 0.0496 - val_accuracy: 0.9583\n",
            "Epoch 9118/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 9118: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
            "Epoch 9119/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0097 - accuracy: 1.0000\n",
            "Epoch 9119: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 1.0000\n",
            "Epoch 9120/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 9120: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0217 - val_accuracy: 0.9861\n",
            "Epoch 9121/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9954\n",
            "Epoch 9121: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0265 - accuracy: 0.9954 - val_loss: 0.0181 - val_accuracy: 1.0000\n",
            "Epoch 9122/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 9122: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
            "Epoch 9123/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0066 - accuracy: 1.0000\n",
            "Epoch 9123: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0642 - val_accuracy: 0.9722\n",
            "Epoch 9124/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 1.0000\n",
            "Epoch 9124: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0404 - val_accuracy: 0.9722\n",
            "Epoch 9125/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0214 - accuracy: 0.9891\n",
            "Epoch 9125: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0218 - accuracy: 0.9907 - val_loss: 0.0149 - val_accuracy: 1.0000\n",
            "Epoch 9126/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9954\n",
            "Epoch 9126: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0139 - accuracy: 0.9954 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
            "Epoch 9127/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0057 - accuracy: 1.0000\n",
            "Epoch 9127: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0313 - val_accuracy: 0.9861\n",
            "Epoch 9128/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9954\n",
            "Epoch 9128: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 62ms/step - loss: 0.0142 - accuracy: 0.9954 - val_loss: 0.0217 - val_accuracy: 0.9861\n",
            "Epoch 9129/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0269 - accuracy: 0.9891\n",
            "Epoch 9129: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0232 - accuracy: 0.9907 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
            "Epoch 9130/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0118 - accuracy: 0.9946\n",
            "Epoch 9130: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0103 - accuracy: 0.9954 - val_loss: 0.1229 - val_accuracy: 0.9722\n",
            "Epoch 9131/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0269 - accuracy: 0.9896\n",
            "Epoch 9131: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0247 - accuracy: 0.9907 - val_loss: 0.0149 - val_accuracy: 1.0000\n",
            "Epoch 9132/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0114 - accuracy: 0.9946\n",
            "Epoch 9132: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0100 - accuracy: 0.9954 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
            "Epoch 9133/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0217 - accuracy: 0.9946\n",
            "Epoch 9133: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0203 - accuracy: 0.9954 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
            "Epoch 9134/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 9134: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
            "Epoch 9135/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0067 - accuracy: 1.0000\n",
            "Epoch 9135: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.0284 - val_accuracy: 0.9861\n",
            "Epoch 9136/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9907\n",
            "Epoch 9136: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0197 - accuracy: 0.9907 - val_loss: 0.0298 - val_accuracy: 0.9861\n",
            "Epoch 9137/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 9137: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.1154 - val_accuracy: 0.9444\n",
            "Epoch 9138/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 9138: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 1.0000\n",
            "Epoch 9139/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 9139: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0199 - val_accuracy: 1.0000\n",
            "Epoch 9140/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 9140: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
            "Epoch 9141/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 9141: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0163 - val_accuracy: 0.9861\n",
            "Epoch 9142/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 9142: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 71ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 1.0000\n",
            "Epoch 9143/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 9143: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0871 - val_accuracy: 0.9583\n",
            "Epoch 9144/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 9.2465e-04 - accuracy: 1.0000\n",
            "Epoch 9144: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0725 - val_accuracy: 0.9722\n",
            "Epoch 9145/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 9145: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
            "Epoch 9146/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 9146: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "Epoch 9147/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 9147: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0211 - val_accuracy: 0.9861\n",
            "Epoch 9148/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0056 - accuracy: 0.9946\n",
            "Epoch 9148: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0048 - accuracy: 0.9954 - val_loss: 0.0139 - val_accuracy: 1.0000\n",
            "Epoch 9149/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 9149: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0126 - accuracy: 0.9954 - val_loss: 0.0190 - val_accuracy: 0.9861\n",
            "Epoch 9150/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0061 - accuracy: 1.0000\n",
            "Epoch 9150: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
            "Epoch 9151/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 9151: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0403 - val_accuracy: 0.9861\n",
            "Epoch 9152/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0074 - accuracy: 0.9948\n",
            "Epoch 9152: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0066 - accuracy: 0.9954 - val_loss: 0.0128 - val_accuracy: 1.0000\n",
            "Epoch 9153/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0053 - accuracy: 1.0000\n",
            "Epoch 9153: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0219 - val_accuracy: 1.0000\n",
            "Epoch 9154/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0097 - accuracy: 1.0000\n",
            "Epoch 9154: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0324 - val_accuracy: 0.9861\n",
            "Epoch 9155/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 9155: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.0537 - val_accuracy: 0.9722\n",
            "Epoch 9156/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 9156: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 1.0000\n",
            "Epoch 9157/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0164 - accuracy: 0.9946\n",
            "Epoch 9157: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0147 - accuracy: 0.9954 - val_loss: 0.0198 - val_accuracy: 0.9861\n",
            "Epoch 9158/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0152 - accuracy: 0.9948\n",
            "Epoch 9158: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0138 - accuracy: 0.9954 - val_loss: 0.0125 - val_accuracy: 0.9861\n",
            "Epoch 9159/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9907\n",
            "Epoch 9159: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0184 - accuracy: 0.9907 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
            "Epoch 9160/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 9160: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0605 - val_accuracy: 0.9722\n",
            "Epoch 9161/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0077 - accuracy: 0.9954\n",
            "Epoch 9161: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0077 - accuracy: 0.9954 - val_loss: 0.0227 - val_accuracy: 0.9861\n",
            "Epoch 9162/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 9162: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
            "Epoch 9163/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 9163: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0391 - val_accuracy: 0.9861\n",
            "Epoch 9164/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0072 - accuracy: 0.9948\n",
            "Epoch 9164: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0073 - accuracy: 0.9954 - val_loss: 0.0388 - val_accuracy: 0.9861\n",
            "Epoch 9165/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 9165: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
            "Epoch 9166/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0070 - accuracy: 1.0000\n",
            "Epoch 9166: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0397 - val_accuracy: 0.9861\n",
            "Epoch 9167/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0611 - accuracy: 0.9948\n",
            "Epoch 9167: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0603 - accuracy: 0.9954 - val_loss: 0.6609 - val_accuracy: 0.8611\n",
            "Epoch 9168/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.3920 - accuracy: 0.9074\n",
            "Epoch 9168: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.3920 - accuracy: 0.9074 - val_loss: 0.2912 - val_accuracy: 0.9028\n",
            "Epoch 9169/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.1073 - accuracy: 0.9728\n",
            "Epoch 9169: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.2342 - accuracy: 0.9676 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
            "Epoch 9170/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9815\n",
            "Epoch 9170: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0722 - accuracy: 0.9815 - val_loss: 0.1953 - val_accuracy: 0.9583\n",
            "Epoch 9171/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1036 - accuracy: 0.9676\n",
            "Epoch 9171: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 71ms/step - loss: 0.1036 - accuracy: 0.9676 - val_loss: 0.0774 - val_accuracy: 0.9861\n",
            "Epoch 9172/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0638 - accuracy: 0.9861\n",
            "Epoch 9172: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0638 - accuracy: 0.9861 - val_loss: 0.0993 - val_accuracy: 0.9583\n",
            "Epoch 9173/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0438 - accuracy: 0.9896\n",
            "Epoch 9173: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0593 - accuracy: 0.9815 - val_loss: 0.0525 - val_accuracy: 0.9583\n",
            "Epoch 9174/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9861\n",
            "Epoch 9174: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0295 - accuracy: 0.9861 - val_loss: 0.0389 - val_accuracy: 0.9861\n",
            "Epoch 9175/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0218 - accuracy: 0.9946\n",
            "Epoch 9175: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0235 - accuracy: 0.9954 - val_loss: 0.0419 - val_accuracy: 0.9861\n",
            "Epoch 9176/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9907\n",
            "Epoch 9176: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0124 - accuracy: 0.9907 - val_loss: 0.0313 - val_accuracy: 0.9722\n",
            "Epoch 9177/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0111 - accuracy: 1.0000\n",
            "Epoch 9177: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0177 - accuracy: 0.9954 - val_loss: 0.0086 - val_accuracy: 1.0000\n",
            "Epoch 9178/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0256 - accuracy: 0.9946\n",
            "Epoch 9178: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0224 - accuracy: 0.9954 - val_loss: 0.0068 - val_accuracy: 1.0000\n",
            "Epoch 9179/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9954\n",
            "Epoch 9179: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 71ms/step - loss: 0.0230 - accuracy: 0.9954 - val_loss: 0.0273 - val_accuracy: 0.9861\n",
            "Epoch 9180/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0091 - accuracy: 1.0000\n",
            "Epoch 9180: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.0153 - val_accuracy: 0.9861\n",
            "Epoch 9181/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9954\n",
            "Epoch 9181: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 79ms/step - loss: 0.0332 - accuracy: 0.9954 - val_loss: 0.0653 - val_accuracy: 0.9861\n",
            "Epoch 9182/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.9954\n",
            "Epoch 9182: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0082 - accuracy: 0.9954 - val_loss: 0.0375 - val_accuracy: 0.9861\n",
            "Epoch 9183/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0298 - accuracy: 0.9891\n",
            "Epoch 9183: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0262 - accuracy: 0.9907 - val_loss: 0.0408 - val_accuracy: 0.9861\n",
            "Epoch 9184/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0362 - accuracy: 0.9907\n",
            "Epoch 9184: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 71ms/step - loss: 0.0362 - accuracy: 0.9907 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
            "Epoch 9185/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 1.0000\n",
            "Epoch 9185: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.0255 - val_accuracy: 0.9861\n",
            "Epoch 9186/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9907\n",
            "Epoch 9186: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0316 - accuracy: 0.9907 - val_loss: 0.1502 - val_accuracy: 0.9444\n",
            "Epoch 9187/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0338 - accuracy: 0.9896\n",
            "Epoch 9187: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0359 - accuracy: 0.9907 - val_loss: 0.0363 - val_accuracy: 0.9861\n",
            "Epoch 9188/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9861\n",
            "Epoch 9188: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0315 - accuracy: 0.9861 - val_loss: 0.0080 - val_accuracy: 1.0000\n",
            "Epoch 9189/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0157 - accuracy: 0.9837\n",
            "Epoch 9189: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0164 - accuracy: 0.9861 - val_loss: 0.0510 - val_accuracy: 0.9722\n",
            "Epoch 9190/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9907\n",
            "Epoch 9190: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 79ms/step - loss: 0.0198 - accuracy: 0.9907 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
            "Epoch 9191/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0251 - accuracy: 0.9946\n",
            "Epoch 9191: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0217 - accuracy: 0.9954 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
            "Epoch 9192/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9954\n",
            "Epoch 9192: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0140 - accuracy: 0.9954 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
            "Epoch 9193/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.9954\n",
            "Epoch 9193: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0110 - accuracy: 0.9954 - val_loss: 0.0610 - val_accuracy: 0.9722\n",
            "Epoch 9194/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.9954\n",
            "Epoch 9194: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0086 - accuracy: 0.9954 - val_loss: 0.0141 - val_accuracy: 1.0000\n",
            "Epoch 9195/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 9195: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0108 - accuracy: 0.9954 - val_loss: 0.0345 - val_accuracy: 0.9722\n",
            "Epoch 9196/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 0.9954\n",
            "Epoch 9196: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 71ms/step - loss: 0.0078 - accuracy: 0.9954 - val_loss: 0.1317 - val_accuracy: 0.9444\n",
            "Epoch 9197/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 1.0000\n",
            "Epoch 9197: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.0803 - val_accuracy: 0.9722\n",
            "Epoch 9198/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0242 - accuracy: 0.9837\n",
            "Epoch 9198: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0214 - accuracy: 0.9861 - val_loss: 0.0930 - val_accuracy: 0.9861\n",
            "Epoch 9199/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0081 - accuracy: 1.0000\n",
            "Epoch 9199: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0373 - val_accuracy: 0.9722\n",
            "Epoch 9200/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0354 - accuracy: 0.9946\n",
            "Epoch 9200: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0302 - accuracy: 0.9954 - val_loss: 0.0440 - val_accuracy: 0.9861\n",
            "Epoch 9201/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.9907\n",
            "Epoch 9201: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0528 - accuracy: 0.9907 - val_loss: 0.0368 - val_accuracy: 0.9861\n",
            "Epoch 9202/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0092 - accuracy: 1.0000\n",
            "Epoch 9202: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0933 - val_accuracy: 0.9583\n",
            "Epoch 9203/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9954\n",
            "Epoch 9203: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0140 - accuracy: 0.9954 - val_loss: 0.0483 - val_accuracy: 0.9861\n",
            "Epoch 9204/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0093 - accuracy: 1.0000\n",
            "Epoch 9204: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.0138 - val_accuracy: 1.0000\n",
            "Epoch 9205/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0077 - accuracy: 1.0000\n",
            "Epoch 9205: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0393 - val_accuracy: 0.9861\n",
            "Epoch 9206/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0082 - accuracy: 1.0000\n",
            "Epoch 9206: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.1264 - val_accuracy: 0.9722\n",
            "Epoch 9207/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 1.0000\n",
            "Epoch 9207: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0658 - val_accuracy: 0.9861\n",
            "Epoch 9208/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0464 - accuracy: 0.9728\n",
            "Epoch 9208: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0399 - accuracy: 0.9769 - val_loss: 0.0221 - val_accuracy: 0.9861\n",
            "Epoch 9209/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9861\n",
            "Epoch 9209: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0264 - accuracy: 0.9861 - val_loss: 0.0404 - val_accuracy: 0.9861\n",
            "Epoch 9210/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9815\n",
            "Epoch 9210: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0351 - accuracy: 0.9815 - val_loss: 0.0454 - val_accuracy: 0.9861\n",
            "Epoch 9211/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9954\n",
            "Epoch 9211: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 0.0208 - accuracy: 0.9954 - val_loss: 0.0702 - val_accuracy: 0.9722\n",
            "Epoch 9212/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9907\n",
            "Epoch 9212: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0266 - accuracy: 0.9907 - val_loss: 0.1932 - val_accuracy: 0.9306\n",
            "Epoch 9213/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0395 - accuracy: 0.9861\n",
            "Epoch 9213: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0395 - accuracy: 0.9861 - val_loss: 0.0398 - val_accuracy: 0.9861\n",
            "Epoch 9214/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0230 - accuracy: 0.9896\n",
            "Epoch 9214: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0224 - accuracy: 0.9907 - val_loss: 0.0101 - val_accuracy: 1.0000\n",
            "Epoch 9215/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0430 - accuracy: 0.9769\n",
            "Epoch 9215: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0430 - accuracy: 0.9769 - val_loss: 0.0098 - val_accuracy: 1.0000\n",
            "Epoch 9216/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0135 - accuracy: 0.9946\n",
            "Epoch 9216: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0200 - accuracy: 0.9907 - val_loss: 0.0185 - val_accuracy: 1.0000\n",
            "Epoch 9217/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0068 - accuracy: 1.0000\n",
            "Epoch 9217: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0348 - val_accuracy: 0.9861\n",
            "Epoch 9218/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0071 - accuracy: 1.0000\n",
            "Epoch 9218: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.0295 - val_accuracy: 0.9861\n",
            "Epoch 9219/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0061 - accuracy: 1.0000\n",
            "Epoch 9219: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0261 - val_accuracy: 0.9861\n",
            "Epoch 9220/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0119 - accuracy: 1.0000\n",
            "Epoch 9220: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.0704 - val_accuracy: 0.9722\n",
            "Epoch 9221/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0137 - accuracy: 0.9896\n",
            "Epoch 9221: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0123 - accuracy: 0.9907 - val_loss: 0.0763 - val_accuracy: 0.9722\n",
            "Epoch 9222/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0160 - accuracy: 0.9946\n",
            "Epoch 9222: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0149 - accuracy: 0.9954 - val_loss: 0.0207 - val_accuracy: 0.9861\n",
            "Epoch 9223/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 9223: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 62ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0209 - val_accuracy: 0.9861\n",
            "Epoch 9224/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 9224: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0403 - val_accuracy: 0.9861\n",
            "Epoch 9225/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0104 - accuracy: 1.0000\n",
            "Epoch 9225: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.0144 - val_accuracy: 1.0000\n",
            "Epoch 9226/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0129 - accuracy: 1.0000\n",
            "Epoch 9226: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.0188 - val_accuracy: 0.9861\n",
            "Epoch 9227/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 9227: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0442 - val_accuracy: 0.9861\n",
            "Epoch 9228/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0558 - accuracy: 0.9891\n",
            "Epoch 9228: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0494 - accuracy: 0.9907 - val_loss: 0.0606 - val_accuracy: 0.9722\n",
            "Epoch 9229/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9907\n",
            "Epoch 9229: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0701 - accuracy: 0.9907 - val_loss: 0.1055 - val_accuracy: 0.9722\n",
            "Epoch 9230/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0178 - accuracy: 0.9891\n",
            "Epoch 9230: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0163 - accuracy: 0.9907 - val_loss: 0.0391 - val_accuracy: 0.9861\n",
            "Epoch 9231/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0282 - accuracy: 0.9837\n",
            "Epoch 9231: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0271 - accuracy: 0.9861 - val_loss: 0.0403 - val_accuracy: 0.9861\n",
            "Epoch 9232/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0310 - accuracy: 0.9891\n",
            "Epoch 9232: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0307 - accuracy: 0.9907 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
            "Epoch 9233/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9907\n",
            "Epoch 9233: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0254 - accuracy: 0.9907 - val_loss: 0.0697 - val_accuracy: 0.9722\n",
            "Epoch 9234/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9907\n",
            "Epoch 9234: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0367 - accuracy: 0.9907 - val_loss: 0.0086 - val_accuracy: 1.0000\n",
            "Epoch 9235/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0204 - accuracy: 0.9891\n",
            "Epoch 9235: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0176 - accuracy: 0.9907 - val_loss: 0.0779 - val_accuracy: 0.9722\n",
            "Epoch 9236/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0452 - accuracy: 0.9907\n",
            "Epoch 9236: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0452 - accuracy: 0.9907 - val_loss: 0.0307 - val_accuracy: 0.9722\n",
            "Epoch 9237/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0114 - accuracy: 0.9946\n",
            "Epoch 9237: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0105 - accuracy: 0.9954 - val_loss: 0.0169 - val_accuracy: 1.0000\n",
            "Epoch 9238/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0303 - accuracy: 0.9891\n",
            "Epoch 9238: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0282 - accuracy: 0.9907 - val_loss: 0.0172 - val_accuracy: 1.0000\n",
            "Epoch 9239/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9954\n",
            "Epoch 9239: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0195 - accuracy: 0.9954 - val_loss: 0.0487 - val_accuracy: 0.9861\n",
            "Epoch 9240/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9954\n",
            "Epoch 9240: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0127 - accuracy: 0.9954 - val_loss: 0.0129 - val_accuracy: 1.0000\n",
            "Epoch 9241/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0404 - accuracy: 0.9946\n",
            "Epoch 9241: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0358 - accuracy: 0.9954 - val_loss: 0.0372 - val_accuracy: 0.9722\n",
            "Epoch 9242/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0216 - accuracy: 0.9891\n",
            "Epoch 9242: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0371 - accuracy: 0.9861 - val_loss: 0.0244 - val_accuracy: 0.9861\n",
            "Epoch 9243/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1057 - accuracy: 0.9769\n",
            "Epoch 9243: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.1057 - accuracy: 0.9769 - val_loss: 0.0109 - val_accuracy: 1.0000\n",
            "Epoch 9244/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0078 - accuracy: 1.0000\n",
            "Epoch 9244: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.3678 - val_accuracy: 0.9306\n",
            "Epoch 9245/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.1154 - accuracy: 0.9565\n",
            "Epoch 9245: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0985 - accuracy: 0.9630 - val_loss: 0.1543 - val_accuracy: 0.9306\n",
            "Epoch 9246/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.2705 - accuracy: 0.9565\n",
            "Epoch 9246: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.2895 - accuracy: 0.9491 - val_loss: 0.4240 - val_accuracy: 0.8611\n",
            "Epoch 9247/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.1984 - accuracy: 0.9130\n",
            "Epoch 9247: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.1914 - accuracy: 0.9167 - val_loss: 0.2565 - val_accuracy: 0.9444\n",
            "Epoch 9248/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.1669 - accuracy: 0.9457\n",
            "Epoch 9248: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.1475 - accuracy: 0.9491 - val_loss: 0.1312 - val_accuracy: 0.9722\n",
            "Epoch 9249/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9722\n",
            "Epoch 9249: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0716 - accuracy: 0.9722 - val_loss: 0.0372 - val_accuracy: 1.0000\n",
            "Epoch 9250/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0442 - accuracy: 0.9783\n",
            "Epoch 9250: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0390 - accuracy: 0.9815 - val_loss: 0.0711 - val_accuracy: 0.9722\n",
            "Epoch 9251/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9954\n",
            "Epoch 9251: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0201 - accuracy: 0.9954 - val_loss: 0.0852 - val_accuracy: 0.9583\n",
            "Epoch 9252/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9954\n",
            "Epoch 9252: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.0377 - val_accuracy: 0.9722\n",
            "Epoch 9253/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0586 - accuracy: 0.9728\n",
            "Epoch 9253: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0524 - accuracy: 0.9769 - val_loss: 0.0708 - val_accuracy: 0.9722\n",
            "Epoch 9254/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0343 - accuracy: 0.9837\n",
            "Epoch 9254: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0298 - accuracy: 0.9861 - val_loss: 0.0533 - val_accuracy: 0.9861\n",
            "Epoch 9255/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 9255: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0174 - val_accuracy: 1.0000\n",
            "Epoch 9256/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9954\n",
            "Epoch 9256: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0126 - accuracy: 0.9954 - val_loss: 0.0323 - val_accuracy: 0.9861\n",
            "Epoch 9257/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0153 - accuracy: 0.9946\n",
            "Epoch 9257: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0133 - accuracy: 0.9954 - val_loss: 0.0903 - val_accuracy: 0.9583\n",
            "Epoch 9258/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 9258: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0690 - val_accuracy: 0.9583\n",
            "Epoch 9259/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0105 - accuracy: 1.0000\n",
            "Epoch 9259: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0327 - accuracy: 0.9954 - val_loss: 0.0467 - val_accuracy: 0.9861\n",
            "Epoch 9260/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9954\n",
            "Epoch 9260: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0111 - accuracy: 0.9954 - val_loss: 0.0154 - val_accuracy: 1.0000\n",
            "Epoch 9261/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 1.0000\n",
            "Epoch 9261: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0208 - val_accuracy: 1.0000\n",
            "Epoch 9262/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 1.0000\n",
            "Epoch 9262: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 84ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.0465 - val_accuracy: 0.9722\n",
            "Epoch 9263/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0086 - accuracy: 1.0000\n",
            "Epoch 9263: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0168 - val_accuracy: 0.9861\n",
            "Epoch 9264/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 1.0000\n",
            "Epoch 9264: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.0097 - val_accuracy: 1.0000\n",
            "Epoch 9265/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 9265: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0329 - val_accuracy: 0.9861\n",
            "Epoch 9266/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0066 - accuracy: 1.0000\n",
            "Epoch 9266: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0094 - accuracy: 0.9954 - val_loss: 0.0209 - val_accuracy: 0.9861\n",
            "Epoch 9267/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 9267: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0713 - val_accuracy: 0.9722\n",
            "Epoch 9268/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0368 - accuracy: 0.9896\n",
            "Epoch 9268: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0330 - accuracy: 0.9907 - val_loss: 0.0183 - val_accuracy: 0.9861\n",
            "Epoch 9269/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 9269: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9861\n",
            "Epoch 9270/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0162 - accuracy: 0.9891\n",
            "Epoch 9270: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0202 - accuracy: 0.9861 - val_loss: 0.0366 - val_accuracy: 0.9861\n",
            "Epoch 9271/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 9271: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0089 - val_accuracy: 1.0000\n",
            "Epoch 9272/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0084 - accuracy: 0.9946\n",
            "Epoch 9272: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0073 - accuracy: 0.9954 - val_loss: 0.0103 - val_accuracy: 1.0000\n",
            "Epoch 9273/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9954\n",
            "Epoch 9273: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 79ms/step - loss: 0.0129 - accuracy: 0.9954 - val_loss: 0.0197 - val_accuracy: 0.9861\n",
            "Epoch 9274/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0185 - accuracy: 0.9948\n",
            "Epoch 9274: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0188 - accuracy: 0.9954 - val_loss: 0.0518 - val_accuracy: 0.9722\n",
            "Epoch 9275/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 9275: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0207 - val_accuracy: 0.9861\n",
            "Epoch 9276/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0075 - accuracy: 1.0000\n",
            "Epoch 9276: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0163 - val_accuracy: 1.0000\n",
            "Epoch 9277/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 9277: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0421 - val_accuracy: 0.9861\n",
            "Epoch 9278/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0058 - accuracy: 1.0000\n",
            "Epoch 9278: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0729 - val_accuracy: 0.9861\n",
            "Epoch 9279/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0115 - accuracy: 0.9954\n",
            "Epoch 9279: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0115 - accuracy: 0.9954 - val_loss: 0.0207 - val_accuracy: 0.9861\n",
            "Epoch 9280/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0077 - accuracy: 1.0000\n",
            "Epoch 9280: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
            "Epoch 9281/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 9281: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0086 - accuracy: 0.9954 - val_loss: 0.1191 - val_accuracy: 0.9722\n",
            "Epoch 9282/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0091 - accuracy: 1.0000\n",
            "Epoch 9282: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.0529 - val_accuracy: 0.9861\n",
            "Epoch 9283/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0115 - accuracy: 0.9946\n",
            "Epoch 9283: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0100 - accuracy: 0.9954 - val_loss: 0.0158 - val_accuracy: 1.0000\n",
            "Epoch 9284/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 9284: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
            "Epoch 9285/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 9285: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
            "Epoch 9286/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0097 - accuracy: 0.9946\n",
            "Epoch 9286: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0083 - accuracy: 0.9954 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
            "Epoch 9287/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 9287: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 74ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0084 - val_accuracy: 1.0000\n",
            "Epoch 9288/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0096 - accuracy: 1.0000\n",
            "Epoch 9288: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.0727 - val_accuracy: 0.9861\n",
            "Epoch 9289/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 9289: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0191 - val_accuracy: 0.9861\n",
            "Epoch 9290/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0076 - accuracy: 1.0000\n",
            "Epoch 9290: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0150 - val_accuracy: 1.0000\n",
            "Epoch 9291/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 9291: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0337 - val_accuracy: 0.9861\n",
            "Epoch 9292/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 9292: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0784 - val_accuracy: 0.9722\n",
            "Epoch 9293/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0065 - accuracy: 0.9946\n",
            "Epoch 9293: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0061 - accuracy: 0.9954 - val_loss: 0.0421 - val_accuracy: 0.9583\n",
            "Epoch 9294/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0125 - accuracy: 0.9946\n",
            "Epoch 9294: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0108 - accuracy: 0.9954 - val_loss: 0.0134 - val_accuracy: 1.0000\n",
            "Epoch 9295/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 9295: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.0328 - val_accuracy: 0.9861\n",
            "Epoch 9296/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0055 - accuracy: 1.0000\n",
            "Epoch 9296: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.0935 - val_accuracy: 0.9583\n",
            "Epoch 9297/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0082 - accuracy: 1.0000\n",
            "Epoch 9297: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0339 - val_accuracy: 0.9861\n",
            "Epoch 9298/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9954\n",
            "Epoch 9298: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0175 - accuracy: 0.9954 - val_loss: 0.0371 - val_accuracy: 0.9861\n",
            "Epoch 9299/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0107 - accuracy: 0.9946\n",
            "Epoch 9299: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0092 - accuracy: 0.9954 - val_loss: 0.0128 - val_accuracy: 1.0000\n",
            "Epoch 9300/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0121 - accuracy: 0.9948\n",
            "Epoch 9300: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0111 - accuracy: 0.9954 - val_loss: 0.0083 - val_accuracy: 1.0000\n",
            "Epoch 9301/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0048 - accuracy: 1.0000\n",
            "Epoch 9301: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0128 - val_accuracy: 0.9861\n",
            "Epoch 9302/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9907\n",
            "Epoch 9302: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0144 - accuracy: 0.9907 - val_loss: 0.0398 - val_accuracy: 0.9861\n",
            "Epoch 9303/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0154 - accuracy: 0.9896\n",
            "Epoch 9303: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0180 - accuracy: 0.9861 - val_loss: 0.0250 - val_accuracy: 0.9861\n",
            "Epoch 9304/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0200 - accuracy: 0.9946\n",
            "Epoch 9304: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0260 - accuracy: 0.9907 - val_loss: 0.0507 - val_accuracy: 0.9722\n",
            "Epoch 9305/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0133 - accuracy: 1.0000\n",
            "Epoch 9305: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0160 - accuracy: 0.9954 - val_loss: 0.0390 - val_accuracy: 0.9722\n",
            "Epoch 9306/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0071 - accuracy: 1.0000\n",
            "Epoch 9306: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0633 - val_accuracy: 0.9722\n",
            "Epoch 9307/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0280 - accuracy: 0.9891\n",
            "Epoch 9307: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0248 - accuracy: 0.9907 - val_loss: 0.0284 - val_accuracy: 0.9861\n",
            "Epoch 9308/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.9954\n",
            "Epoch 9308: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0103 - accuracy: 0.9954 - val_loss: 0.0110 - val_accuracy: 1.0000\n",
            "Epoch 9309/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 9309: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0104 - val_accuracy: 1.0000\n",
            "Epoch 9310/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9907\n",
            "Epoch 9310: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0188 - accuracy: 0.9907 - val_loss: 0.0108 - val_accuracy: 1.0000\n",
            "Epoch 9311/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0135 - accuracy: 0.9946\n",
            "Epoch 9311: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0119 - accuracy: 0.9954 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
            "Epoch 9312/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 1.0000\n",
            "Epoch 9312: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
            "Epoch 9313/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0323 - accuracy: 0.9946\n",
            "Epoch 9313: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0277 - accuracy: 0.9954 - val_loss: 0.0138 - val_accuracy: 1.0000\n",
            "Epoch 9314/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0118 - accuracy: 0.9946\n",
            "Epoch 9314: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0110 - accuracy: 0.9954 - val_loss: 0.0297 - val_accuracy: 0.9861\n",
            "Epoch 9315/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0129 - accuracy: 0.9948\n",
            "Epoch 9315: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0118 - accuracy: 0.9954 - val_loss: 0.0247 - val_accuracy: 0.9861\n",
            "Epoch 9316/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 9316: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0139 - val_accuracy: 1.0000\n",
            "Epoch 9317/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0353 - accuracy: 0.9907\n",
            "Epoch 9317: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0353 - accuracy: 0.9907 - val_loss: 0.0152 - val_accuracy: 1.0000\n",
            "Epoch 9318/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 9318: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0102 - val_accuracy: 1.0000\n",
            "Epoch 9319/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 9319: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.0510 - val_accuracy: 0.9722\n",
            "Epoch 9320/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0126 - accuracy: 0.9946\n",
            "Epoch 9320: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0116 - accuracy: 0.9954 - val_loss: 0.0096 - val_accuracy: 1.0000\n",
            "Epoch 9321/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 9321: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0158 - val_accuracy: 1.0000\n",
            "Epoch 9322/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0089 - accuracy: 1.0000\n",
            "Epoch 9322: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0403 - val_accuracy: 0.9722\n",
            "Epoch 9323/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0111 - accuracy: 0.9891\n",
            "Epoch 9323: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0108 - accuracy: 0.9907 - val_loss: 0.0186 - val_accuracy: 1.0000\n",
            "Epoch 9324/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 9324: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0266 - val_accuracy: 0.9861\n",
            "Epoch 9325/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 9325: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
            "Epoch 9326/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0183 - accuracy: 0.9896\n",
            "Epoch 9326: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0176 - accuracy: 0.9907 - val_loss: 0.0394 - val_accuracy: 0.9722\n",
            "Epoch 9327/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 9327: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
            "Epoch 9328/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0053 - accuracy: 1.0000\n",
            "Epoch 9328: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
            "Epoch 9329/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 9329: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0203 - val_accuracy: 0.9861\n",
            "Epoch 9330/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 9330: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0084 - val_accuracy: 1.0000\n",
            "Epoch 9331/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0067 - accuracy: 1.0000\n",
            "Epoch 9331: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0255 - val_accuracy: 0.9861\n",
            "Epoch 9332/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 9332: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0457 - val_accuracy: 0.9722\n",
            "Epoch 9333/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0199 - accuracy: 0.9946\n",
            "Epoch 9333: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0173 - accuracy: 0.9954 - val_loss: 0.0230 - val_accuracy: 0.9861\n",
            "Epoch 9334/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 9334: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0162 - accuracy: 0.9954 - val_loss: 0.0525 - val_accuracy: 0.9722\n",
            "Epoch 9335/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0317 - accuracy: 0.9891\n",
            "Epoch 9335: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0314 - accuracy: 0.9907 - val_loss: 0.0754 - val_accuracy: 0.9722\n",
            "Epoch 9336/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9954\n",
            "Epoch 9336: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0177 - accuracy: 0.9954 - val_loss: 0.1342 - val_accuracy: 0.9444\n",
            "Epoch 9337/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0285 - accuracy: 0.9946\n",
            "Epoch 9337: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0373 - accuracy: 0.9815 - val_loss: 0.0119 - val_accuracy: 1.0000\n",
            "Epoch 9338/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0283 - accuracy: 0.9948\n",
            "Epoch 9338: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0253 - accuracy: 0.9954 - val_loss: 0.0095 - val_accuracy: 1.0000\n",
            "Epoch 9339/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0303 - accuracy: 0.9815\n",
            "Epoch 9339: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0303 - accuracy: 0.9815 - val_loss: 0.1388 - val_accuracy: 0.9583\n",
            "Epoch 9340/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 1.0000\n",
            "Epoch 9340: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9861\n",
            "Epoch 9341/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0088 - accuracy: 1.0000\n",
            "Epoch 9341: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.0132 - val_accuracy: 1.0000\n",
            "Epoch 9342/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0108 - accuracy: 1.0000\n",
            "Epoch 9342: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.0269 - val_accuracy: 0.9861\n",
            "Epoch 9343/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9954\n",
            "Epoch 9343: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0167 - accuracy: 0.9954 - val_loss: 0.0267 - val_accuracy: 0.9861\n",
            "Epoch 9344/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0076 - accuracy: 1.0000\n",
            "Epoch 9344: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0245 - val_accuracy: 0.9861\n",
            "Epoch 9345/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0134 - accuracy: 0.9946\n",
            "Epoch 9345: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0142 - accuracy: 0.9954 - val_loss: 0.0491 - val_accuracy: 0.9861\n",
            "Epoch 9346/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0103 - accuracy: 0.9948\n",
            "Epoch 9346: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0143 - accuracy: 0.9907 - val_loss: 0.0500 - val_accuracy: 0.9722\n",
            "Epoch 9347/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 9347: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0289 - val_accuracy: 0.9722\n",
            "Epoch 9348/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.9954\n",
            "Epoch 9348: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0116 - accuracy: 0.9954 - val_loss: 0.0059 - val_accuracy: 1.0000\n",
            "Epoch 9349/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0071 - accuracy: 1.0000\n",
            "Epoch 9349: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
            "Epoch 9350/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9954\n",
            "Epoch 9350: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0174 - accuracy: 0.9954 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
            "Epoch 9351/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 1.0000\n",
            "Epoch 9351: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.0265 - val_accuracy: 0.9722\n",
            "Epoch 9352/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 1.0000\n",
            "Epoch 9352: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 62ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0535 - val_accuracy: 0.9722\n",
            "Epoch 9353/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0792 - accuracy: 0.9676\n",
            "Epoch 9353: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0792 - accuracy: 0.9676 - val_loss: 1.2978 - val_accuracy: 0.8333\n",
            "Epoch 9354/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1805 - accuracy: 0.9583\n",
            "Epoch 9354: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.1805 - accuracy: 0.9583 - val_loss: 0.1548 - val_accuracy: 0.9722\n",
            "Epoch 9355/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.1678 - accuracy: 0.9583\n",
            "Epoch 9355: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 71ms/step - loss: 0.1504 - accuracy: 0.9630 - val_loss: 0.3312 - val_accuracy: 0.8611\n",
            "Epoch 9356/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.1007 - accuracy: 0.9511\n",
            "Epoch 9356: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.1210 - accuracy: 0.9491 - val_loss: 0.4876 - val_accuracy: 0.9444\n",
            "Epoch 9357/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.2014 - accuracy: 0.9583\n",
            "Epoch 9357: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.2014 - accuracy: 0.9583 - val_loss: 0.1090 - val_accuracy: 0.9583\n",
            "Epoch 9358/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0929 - accuracy: 0.9676\n",
            "Epoch 9358: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0929 - accuracy: 0.9676 - val_loss: 0.0297 - val_accuracy: 0.9861\n",
            "Epoch 9359/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0538 - accuracy: 0.9728\n",
            "Epoch 9359: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0475 - accuracy: 0.9769 - val_loss: 0.0142 - val_accuracy: 1.0000\n",
            "Epoch 9360/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0502 - accuracy: 0.9954\n",
            "Epoch 9360: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0502 - accuracy: 0.9954 - val_loss: 0.0623 - val_accuracy: 0.9583\n",
            "Epoch 9361/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9907\n",
            "Epoch 9361: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0296 - accuracy: 0.9907 - val_loss: 0.0489 - val_accuracy: 0.9861\n",
            "Epoch 9362/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9907\n",
            "Epoch 9362: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0191 - accuracy: 0.9907 - val_loss: 0.0542 - val_accuracy: 0.9861\n",
            "Epoch 9363/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0288 - accuracy: 0.9891\n",
            "Epoch 9363: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0247 - accuracy: 0.9907 - val_loss: 0.0157 - val_accuracy: 1.0000\n",
            "Epoch 9364/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 9364: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
            "Epoch 9365/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9954\n",
            "Epoch 9365: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0203 - accuracy: 0.9954 - val_loss: 0.0205 - val_accuracy: 0.9861\n",
            "Epoch 9366/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0093 - accuracy: 1.0000\n",
            "Epoch 9366: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.0281 - val_accuracy: 0.9861\n",
            "Epoch 9367/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 9367: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0083 - val_accuracy: 1.0000\n",
            "Epoch 9368/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 9368: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0069 - val_accuracy: 1.0000\n",
            "Epoch 9369/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0062 - accuracy: 1.0000\n",
            "Epoch 9369: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0314 - val_accuracy: 0.9861\n",
            "Epoch 9370/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 1.0000\n",
            "Epoch 9370: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.0201 - val_accuracy: 1.0000\n",
            "Epoch 9371/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0068 - accuracy: 1.0000\n",
            "Epoch 9371: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0137 - val_accuracy: 1.0000\n",
            "Epoch 9372/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9907\n",
            "Epoch 9372: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0277 - accuracy: 0.9907 - val_loss: 0.0249 - val_accuracy: 0.9861\n",
            "Epoch 9373/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 9373: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0334 - val_accuracy: 0.9861\n",
            "Epoch 9374/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0063 - accuracy: 1.0000\n",
            "Epoch 9374: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0181 - val_accuracy: 0.9861\n",
            "Epoch 9375/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0300 - accuracy: 0.9837\n",
            "Epoch 9375: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 76ms/step - loss: 0.0279 - accuracy: 0.9861 - val_loss: 0.0144 - val_accuracy: 1.0000\n",
            "Epoch 9376/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 1.0000\n",
            "Epoch 9376: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.0483 - val_accuracy: 0.9722\n",
            "Epoch 9377/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9954\n",
            "Epoch 9377: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0205 - accuracy: 0.9954 - val_loss: 0.0361 - val_accuracy: 0.9861\n",
            "Epoch 9378/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0227 - accuracy: 0.9891\n",
            "Epoch 9378: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0230 - accuracy: 0.9907 - val_loss: 0.0322 - val_accuracy: 0.9861\n",
            "Epoch 9379/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0559 - accuracy: 0.9769\n",
            "Epoch 9379: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 71ms/step - loss: 0.0559 - accuracy: 0.9769 - val_loss: 0.0825 - val_accuracy: 0.9861\n",
            "Epoch 9380/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1501 - accuracy: 0.9861\n",
            "Epoch 9380: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.1501 - accuracy: 0.9861 - val_loss: 0.0440 - val_accuracy: 0.9861\n",
            "Epoch 9381/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0401 - accuracy: 0.9837\n",
            "Epoch 9381: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0359 - accuracy: 0.9861 - val_loss: 0.0877 - val_accuracy: 0.9722\n",
            "Epoch 9382/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0755 - accuracy: 0.9635\n",
            "Epoch 9382: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0677 - accuracy: 0.9676 - val_loss: 0.0198 - val_accuracy: 0.9861\n",
            "Epoch 9383/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0284 - accuracy: 0.9891\n",
            "Epoch 9383: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0260 - accuracy: 0.9907 - val_loss: 0.0503 - val_accuracy: 0.9722\n",
            "Epoch 9384/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0548 - accuracy: 0.9769\n",
            "Epoch 9384: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0548 - accuracy: 0.9769 - val_loss: 0.0258 - val_accuracy: 0.9861\n",
            "Epoch 9385/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0121 - accuracy: 1.0000\n",
            "Epoch 9385: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 71ms/step - loss: 0.0306 - accuracy: 0.9954 - val_loss: 0.0198 - val_accuracy: 1.0000\n",
            "Epoch 9386/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9954\n",
            "Epoch 9386: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 78ms/step - loss: 0.0246 - accuracy: 0.9954 - val_loss: 0.0400 - val_accuracy: 0.9861\n",
            "Epoch 9387/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0067 - accuracy: 1.0000\n",
            "Epoch 9387: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0268 - val_accuracy: 1.0000\n",
            "Epoch 9388/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 9388: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0129 - val_accuracy: 1.0000\n",
            "Epoch 9389/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9954\n",
            "Epoch 9389: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0171 - accuracy: 0.9954 - val_loss: 0.0241 - val_accuracy: 0.9861\n",
            "Epoch 9390/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9954\n",
            "Epoch 9390: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0203 - accuracy: 0.9954 - val_loss: 0.0647 - val_accuracy: 0.9583\n",
            "Epoch 9391/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0345 - accuracy: 0.9891\n",
            "Epoch 9391: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0296 - accuracy: 0.9907 - val_loss: 0.0707 - val_accuracy: 0.9583\n",
            "Epoch 9392/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 1.0000\n",
            "Epoch 9392: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 76ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.1016 - val_accuracy: 0.9583\n",
            "Epoch 9393/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0081 - accuracy: 0.9946\n",
            "Epoch 9393: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0074 - accuracy: 0.9954 - val_loss: 0.0216 - val_accuracy: 0.9861\n",
            "Epoch 9394/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0070 - accuracy: 1.0000\n",
            "Epoch 9394: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0906 - val_accuracy: 0.9583\n",
            "Epoch 9395/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0081 - accuracy: 1.0000\n",
            "Epoch 9395: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 0.9861\n",
            "Epoch 9396/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0079 - accuracy: 0.9946\n",
            "Epoch 9396: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0070 - accuracy: 0.9954 - val_loss: 0.0829 - val_accuracy: 0.9722\n",
            "Epoch 9397/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0098 - accuracy: 0.9946\n",
            "Epoch 9397: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 80ms/step - loss: 0.0122 - accuracy: 0.9907 - val_loss: 0.0169 - val_accuracy: 1.0000\n",
            "Epoch 9398/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0075 - accuracy: 1.0000\n",
            "Epoch 9398: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0600 - val_accuracy: 0.9861\n",
            "Epoch 9399/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 9399: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.0446 - val_accuracy: 0.9722\n",
            "Epoch 9400/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0048 - accuracy: 1.0000\n",
            "Epoch 9400: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.0164 - val_accuracy: 0.9861\n",
            "Epoch 9401/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0077 - accuracy: 0.9946\n",
            "Epoch 9401: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0068 - accuracy: 0.9954 - val_loss: 0.0325 - val_accuracy: 0.9722\n",
            "Epoch 9402/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0105 - accuracy: 0.9946    \n",
            "Epoch 9402: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0092 - accuracy: 0.9954 - val_loss: 0.0756 - val_accuracy: 0.9722\n",
            "Epoch 9403/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9907\n",
            "Epoch 9403: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0174 - accuracy: 0.9907 - val_loss: 0.0177 - val_accuracy: 1.0000\n",
            "Epoch 9404/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0092 - accuracy: 0.9948\n",
            "Epoch 9404: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0166 - accuracy: 0.9907 - val_loss: 0.0318 - val_accuracy: 0.9861\n",
            "Epoch 9405/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9907\n",
            "Epoch 9405: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0288 - accuracy: 0.9907 - val_loss: 0.0691 - val_accuracy: 0.9583\n",
            "Epoch 9406/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0076 - accuracy: 1.0000\n",
            "Epoch 9406: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.0264 - val_accuracy: 0.9861\n",
            "Epoch 9407/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 1.0000\n",
            "Epoch 9407: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.0399 - val_accuracy: 0.9722\n",
            "Epoch 9408/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0161 - accuracy: 0.9946\n",
            "Epoch 9408: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0139 - accuracy: 0.9954 - val_loss: 0.0343 - val_accuracy: 1.0000\n",
            "Epoch 9409/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9954\n",
            "Epoch 9409: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0155 - accuracy: 0.9954 - val_loss: 0.0890 - val_accuracy: 0.9444\n",
            "Epoch 9410/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0108 - accuracy: 0.9946\n",
            "Epoch 9410: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0098 - accuracy: 0.9954 - val_loss: 0.0260 - val_accuracy: 0.9861\n",
            "Epoch 9411/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9954\n",
            "Epoch 9411: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0128 - accuracy: 0.9954 - val_loss: 0.0433 - val_accuracy: 0.9861\n",
            "Epoch 9412/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9954\n",
            "Epoch 9412: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0122 - accuracy: 0.9954 - val_loss: 0.0110 - val_accuracy: 1.0000\n",
            "Epoch 9413/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9954\n",
            "Epoch 9413: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0191 - accuracy: 0.9954 - val_loss: 0.0636 - val_accuracy: 0.9861\n",
            "Epoch 9414/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0143 - accuracy: 0.9891\n",
            "Epoch 9414: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0127 - accuracy: 0.9907 - val_loss: 0.0716 - val_accuracy: 0.9722\n",
            "Epoch 9415/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0099 - accuracy: 0.9946\n",
            "Epoch 9415: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0090 - accuracy: 0.9954 - val_loss: 0.0509 - val_accuracy: 0.9861\n",
            "Epoch 9416/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0058 - accuracy: 1.0000\n",
            "Epoch 9416: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0306 - val_accuracy: 0.9861\n",
            "Epoch 9417/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 9417: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9861\n",
            "Epoch 9418/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 9418: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0482 - val_accuracy: 0.9861\n",
            "Epoch 9419/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 9419: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0110 - val_accuracy: 1.0000\n",
            "Epoch 9420/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9907\n",
            "Epoch 9420: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0165 - accuracy: 0.9907 - val_loss: 0.0494 - val_accuracy: 0.9861\n",
            "Epoch 9421/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9907\n",
            "Epoch 9421: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0239 - accuracy: 0.9907 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
            "Epoch 9422/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0095 - accuracy: 0.9946\n",
            "Epoch 9422: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0117 - accuracy: 0.9954 - val_loss: 0.0732 - val_accuracy: 0.9722\n",
            "Epoch 9423/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0109 - accuracy: 1.0000\n",
            "Epoch 9423: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 78ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.0432 - val_accuracy: 0.9861\n",
            "Epoch 9424/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0061 - accuracy: 1.0000\n",
            "Epoch 9424: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0327 - val_accuracy: 0.9861\n",
            "Epoch 9425/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 9425: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0590 - val_accuracy: 0.9722\n",
            "Epoch 9426/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0274 - accuracy: 0.9896\n",
            "Epoch 9426: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0277 - accuracy: 0.9861 - val_loss: 0.0691 - val_accuracy: 0.9722\n",
            "Epoch 9427/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 9427: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.1285 - val_accuracy: 0.9722\n",
            "Epoch 9428/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0244 - accuracy: 0.9891\n",
            "Epoch 9428: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0211 - accuracy: 0.9907 - val_loss: 0.0630 - val_accuracy: 0.9583\n",
            "Epoch 9429/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9954\n",
            "Epoch 9429: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0132 - accuracy: 0.9954 - val_loss: 0.0370 - val_accuracy: 0.9722\n",
            "Epoch 9430/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0088 - accuracy: 0.9946\n",
            "Epoch 9430: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0076 - accuracy: 0.9954 - val_loss: 0.0342 - val_accuracy: 0.9722\n",
            "Epoch 9431/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.9954\n",
            "Epoch 9431: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0088 - accuracy: 0.9954 - val_loss: 0.0097 - val_accuracy: 1.0000\n",
            "Epoch 9432/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9954\n",
            "Epoch 9432: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0179 - accuracy: 0.9954 - val_loss: 0.0309 - val_accuracy: 0.9861\n",
            "Epoch 9433/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0192 - accuracy: 0.9946\n",
            "Epoch 9433: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0192 - accuracy: 0.9954 - val_loss: 0.0296 - val_accuracy: 0.9861\n",
            "Epoch 9434/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9954\n",
            "Epoch 9434: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0122 - accuracy: 0.9954 - val_loss: 0.0648 - val_accuracy: 0.9722\n",
            "Epoch 9435/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0075 - accuracy: 1.0000\n",
            "Epoch 9435: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0402 - val_accuracy: 0.9722\n",
            "Epoch 9436/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 9436: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0104 - accuracy: 0.9954 - val_loss: 0.1212 - val_accuracy: 0.9444\n",
            "Epoch 9437/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 1.0000\n",
            "Epoch 9437: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 71ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.0316 - val_accuracy: 0.9722\n",
            "Epoch 9438/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0148 - accuracy: 0.9946\n",
            "Epoch 9438: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0153 - accuracy: 0.9954 - val_loss: 0.0679 - val_accuracy: 0.9583\n",
            "Epoch 9439/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0248 - accuracy: 0.9837\n",
            "Epoch 9439: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0213 - accuracy: 0.9861 - val_loss: 0.0092 - val_accuracy: 1.0000\n",
            "Epoch 9440/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 9440: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0335 - val_accuracy: 0.9861\n",
            "Epoch 9441/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9861\n",
            "Epoch 9441: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0392 - accuracy: 0.9861 - val_loss: 0.0230 - val_accuracy: 0.9861\n",
            "Epoch 9442/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 9442: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0453 - accuracy: 0.9861 - val_loss: 0.0453 - val_accuracy: 0.9861\n",
            "Epoch 9443/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0417 - accuracy: 0.9907\n",
            "Epoch 9443: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0417 - accuracy: 0.9907 - val_loss: 0.1041 - val_accuracy: 0.9583\n",
            "Epoch 9444/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 0.9815\n",
            "Epoch 9444: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0365 - accuracy: 0.9815 - val_loss: 0.0969 - val_accuracy: 0.9583\n",
            "Epoch 9445/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0149 - accuracy: 1.0000\n",
            "Epoch 9445: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
            "Epoch 9446/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9907\n",
            "Epoch 9446: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 80ms/step - loss: 0.0261 - accuracy: 0.9907 - val_loss: 0.0067 - val_accuracy: 1.0000\n",
            "Epoch 9447/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0084 - accuracy: 1.0000\n",
            "Epoch 9447: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
            "Epoch 9448/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0080 - accuracy: 1.0000\n",
            "Epoch 9448: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0094 - val_accuracy: 1.0000\n",
            "Epoch 9449/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0271 - accuracy: 0.9948\n",
            "Epoch 9449: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0242 - accuracy: 0.9954 - val_loss: 0.0097 - val_accuracy: 1.0000\n",
            "Epoch 9450/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0068 - accuracy: 1.0000\n",
            "Epoch 9450: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0167 - val_accuracy: 0.9861\n",
            "Epoch 9451/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 9451: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0337 - val_accuracy: 0.9861\n",
            "Epoch 9452/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0048 - accuracy: 1.0000\n",
            "Epoch 9452: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 76ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0426 - val_accuracy: 0.9861\n",
            "Epoch 9453/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 9453: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0215 - val_accuracy: 0.9861\n",
            "Epoch 9454/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 9454: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0110 - val_accuracy: 1.0000\n",
            "Epoch 9455/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.9954\n",
            "Epoch 9455: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0110 - accuracy: 0.9954 - val_loss: 0.0109 - val_accuracy: 1.0000\n",
            "Epoch 9456/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 9456: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0111 - accuracy: 0.9954 - val_loss: 0.0482 - val_accuracy: 0.9861\n",
            "Epoch 9457/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 0.9954\n",
            "Epoch 9457: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0064 - accuracy: 0.9954 - val_loss: 0.0220 - val_accuracy: 0.9861\n",
            "Epoch 9458/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0078 - accuracy: 1.0000\n",
            "Epoch 9458: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
            "Epoch 9459/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 9459: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0692 - val_accuracy: 0.9722\n",
            "Epoch 9460/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0090 - accuracy: 0.9946\n",
            "Epoch 9460: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0077 - accuracy: 0.9954 - val_loss: 0.0227 - val_accuracy: 0.9861\n",
            "Epoch 9461/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 9461: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0696 - val_accuracy: 0.9861\n",
            "Epoch 9462/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9954\n",
            "Epoch 9462: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0141 - accuracy: 0.9954 - val_loss: 0.0350 - val_accuracy: 0.9861\n",
            "Epoch 9463/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0515 - accuracy: 0.9907\n",
            "Epoch 9463: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0515 - accuracy: 0.9907 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
            "Epoch 9464/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0264 - accuracy: 0.9891\n",
            "Epoch 9464: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0247 - accuracy: 0.9907 - val_loss: 0.0544 - val_accuracy: 0.9722\n",
            "Epoch 9465/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 9465: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0162 - val_accuracy: 1.0000\n",
            "Epoch 9466/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0051 - accuracy: 1.0000\n",
            "Epoch 9466: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0251 - val_accuracy: 0.9861\n",
            "Epoch 9467/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9907\n",
            "Epoch 9467: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0155 - accuracy: 0.9907 - val_loss: 0.0426 - val_accuracy: 0.9861\n",
            "Epoch 9468/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 9468: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0308 - val_accuracy: 0.9861\n",
            "Epoch 9469/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0068 - accuracy: 1.0000\n",
            "Epoch 9469: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0433 - val_accuracy: 0.9861\n",
            "Epoch 9470/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0078 - accuracy: 0.9946\n",
            "Epoch 9470: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0077 - accuracy: 0.9954 - val_loss: 0.0547 - val_accuracy: 0.9583\n",
            "Epoch 9471/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.9954\n",
            "Epoch 9471: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0080 - accuracy: 0.9954 - val_loss: 0.0124 - val_accuracy: 1.0000\n",
            "Epoch 9472/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0156 - accuracy: 0.9891\n",
            "Epoch 9472: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0145 - accuracy: 0.9907 - val_loss: 0.0064 - val_accuracy: 1.0000\n",
            "Epoch 9473/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0051 - accuracy: 1.0000\n",
            "Epoch 9473: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 76ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0302 - val_accuracy: 0.9861\n",
            "Epoch 9474/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9954\n",
            "Epoch 9474: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0242 - accuracy: 0.9954 - val_loss: 0.0227 - val_accuracy: 1.0000\n",
            "Epoch 9475/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0062 - accuracy: 1.0000\n",
            "Epoch 9475: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9861\n",
            "Epoch 9476/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 9476: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0199 - val_accuracy: 0.9861\n",
            "Epoch 9477/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 9477: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0211 - val_accuracy: 0.9861\n",
            "Epoch 9478/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9954\n",
            "Epoch 9478: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0150 - accuracy: 0.9954 - val_loss: 0.0265 - val_accuracy: 0.9861\n",
            "Epoch 9479/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0070 - accuracy: 1.0000\n",
            "Epoch 9479: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 79ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0694 - val_accuracy: 0.9722\n",
            "Epoch 9480/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9954\n",
            "Epoch 9480: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0113 - accuracy: 0.9954 - val_loss: 0.0416 - val_accuracy: 0.9861\n",
            "Epoch 9481/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0124 - accuracy: 0.9946\n",
            "Epoch 9481: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0106 - accuracy: 0.9954 - val_loss: 0.0498 - val_accuracy: 0.9722\n",
            "Epoch 9482/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9907\n",
            "Epoch 9482: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0206 - accuracy: 0.9907 - val_loss: 0.0205 - val_accuracy: 0.9861\n",
            "Epoch 9483/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9954\n",
            "Epoch 9483: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0129 - accuracy: 0.9954 - val_loss: 0.0355 - val_accuracy: 0.9861\n",
            "Epoch 9484/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 9484: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0474 - val_accuracy: 0.9861\n",
            "Epoch 9485/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 9485: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0260 - val_accuracy: 0.9861\n",
            "Epoch 9486/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0689 - accuracy: 0.9891\n",
            "Epoch 9486: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0589 - accuracy: 0.9907 - val_loss: 0.0300 - val_accuracy: 0.9861\n",
            "Epoch 9487/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0241 - accuracy: 0.9891\n",
            "Epoch 9487: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 78ms/step - loss: 0.0261 - accuracy: 0.9861 - val_loss: 0.1260 - val_accuracy: 0.9444\n",
            "Epoch 9488/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9861\n",
            "Epoch 9488: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0784 - accuracy: 0.9861 - val_loss: 0.0846 - val_accuracy: 0.9444\n",
            "Epoch 9489/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0488 - accuracy: 0.9954\n",
            "Epoch 9489: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0488 - accuracy: 0.9954 - val_loss: 0.0323 - val_accuracy: 0.9861\n",
            "Epoch 9490/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0570 - accuracy: 0.9837\n",
            "Epoch 9490: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0506 - accuracy: 0.9861 - val_loss: 0.0532 - val_accuracy: 0.9861\n",
            "Epoch 9491/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 9491: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0447 - val_accuracy: 0.9861\n",
            "Epoch 9492/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 9492: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0661 - val_accuracy: 0.9583\n",
            "Epoch 9493/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9907\n",
            "Epoch 9493: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 71ms/step - loss: 0.0214 - accuracy: 0.9907 - val_loss: 0.0209 - val_accuracy: 0.9861\n",
            "Epoch 9494/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0080 - accuracy: 1.0000\n",
            "Epoch 9494: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.0089 - val_accuracy: 1.0000\n",
            "Epoch 9495/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9907\n",
            "Epoch 9495: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 0.0295 - accuracy: 0.9907 - val_loss: 0.0218 - val_accuracy: 0.9861\n",
            "Epoch 9496/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 9496: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.0718 - val_accuracy: 0.9861\n",
            "Epoch 9497/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0065 - accuracy: 1.0000\n",
            "Epoch 9497: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0583 - val_accuracy: 0.9722\n",
            "Epoch 9498/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9861\n",
            "Epoch 9498: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0198 - accuracy: 0.9861 - val_loss: 0.0315 - val_accuracy: 0.9861\n",
            "Epoch 9499/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9954\n",
            "Epoch 9499: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0152 - accuracy: 0.9954 - val_loss: 0.0617 - val_accuracy: 0.9861\n",
            "Epoch 9500/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.9954\n",
            "Epoch 9500: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0103 - accuracy: 0.9954 - val_loss: 0.0220 - val_accuracy: 0.9861\n",
            "Epoch 9501/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0067 - accuracy: 1.0000\n",
            "Epoch 9501: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0157 - val_accuracy: 0.9861\n",
            "Epoch 9502/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 9502: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0089 - val_accuracy: 1.0000\n",
            "Epoch 9503/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0088 - accuracy: 0.9946\n",
            "Epoch 9503: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0076 - accuracy: 0.9954 - val_loss: 0.0194 - val_accuracy: 0.9861\n",
            "Epoch 9504/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9907\n",
            "Epoch 9504: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0192 - accuracy: 0.9907 - val_loss: 0.0230 - val_accuracy: 0.9861\n",
            "Epoch 9505/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 0.9907\n",
            "Epoch 9505: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 78ms/step - loss: 0.0333 - accuracy: 0.9907 - val_loss: 0.1142 - val_accuracy: 0.9722\n",
            "Epoch 9506/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0106 - accuracy: 1.0000\n",
            "Epoch 9506: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 79ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.0516 - val_accuracy: 0.9583\n",
            "Epoch 9507/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9861\n",
            "Epoch 9507: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0157 - accuracy: 0.9861 - val_loss: 0.0525 - val_accuracy: 0.9861\n",
            "Epoch 9508/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0192 - accuracy: 0.9946\n",
            "Epoch 9508: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0166 - accuracy: 0.9954 - val_loss: 0.0319 - val_accuracy: 0.9861\n",
            "Epoch 9509/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0196 - accuracy: 0.9946\n",
            "Epoch 9509: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0169 - accuracy: 0.9954 - val_loss: 0.0436 - val_accuracy: 0.9861\n",
            "Epoch 9510/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 9510: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0354 - val_accuracy: 0.9861\n",
            "Epoch 9511/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9907\n",
            "Epoch 9511: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0251 - accuracy: 0.9907 - val_loss: 0.0063 - val_accuracy: 1.0000\n",
            "Epoch 9512/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9954\n",
            "Epoch 9512: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0201 - accuracy: 0.9954 - val_loss: 0.0277 - val_accuracy: 0.9722\n",
            "Epoch 9513/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9954\n",
            "Epoch 9513: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0294 - accuracy: 0.9954 - val_loss: 0.0330 - val_accuracy: 0.9861\n",
            "Epoch 9514/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0151 - accuracy: 0.9946\n",
            "Epoch 9514: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0144 - accuracy: 0.9954 - val_loss: 0.0155 - val_accuracy: 1.0000\n",
            "Epoch 9515/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9954\n",
            "Epoch 9515: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0258 - accuracy: 0.9954 - val_loss: 0.0355 - val_accuracy: 0.9861\n",
            "Epoch 9516/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0048 - accuracy: 1.0000\n",
            "Epoch 9516: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 78ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.0448 - val_accuracy: 0.9861\n",
            "Epoch 9517/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0100 - accuracy: 0.9946\n",
            "Epoch 9517: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0086 - accuracy: 0.9954 - val_loss: 0.0131 - val_accuracy: 1.0000\n",
            "Epoch 9518/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 9518: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0517 - val_accuracy: 0.9861\n",
            "Epoch 9519/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0057 - accuracy: 1.0000\n",
            "Epoch 9519: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0194 - val_accuracy: 0.9861\n",
            "Epoch 9520/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 9520: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0129 - val_accuracy: 1.0000\n",
            "Epoch 9521/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0068 - accuracy: 1.0000\n",
            "Epoch 9521: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0126 - val_accuracy: 0.9861\n",
            "Epoch 9522/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9954\n",
            "Epoch 9522: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0275 - accuracy: 0.9954 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
            "Epoch 9523/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 9523: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0283 - val_accuracy: 0.9861\n",
            "Epoch 9524/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0067 - accuracy: 0.9946\n",
            "Epoch 9524: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0111 - accuracy: 0.9907 - val_loss: 0.0222 - val_accuracy: 0.9861\n",
            "Epoch 9525/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0148 - accuracy: 0.9946\n",
            "Epoch 9525: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0135 - accuracy: 0.9954 - val_loss: 0.0706 - val_accuracy: 0.9722\n",
            "Epoch 9526/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0077 - accuracy: 1.0000\n",
            "Epoch 9526: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0212 - val_accuracy: 0.9861\n",
            "Epoch 9527/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0064 - accuracy: 1.0000    \n",
            "Epoch 9527: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0239 - val_accuracy: 0.9861\n",
            "Epoch 9528/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0099 - accuracy: 0.9946\n",
            "Epoch 9528: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0088 - accuracy: 0.9954 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "Epoch 9529/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9954\n",
            "Epoch 9529: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0140 - accuracy: 0.9954 - val_loss: 0.0203 - val_accuracy: 0.9861\n",
            "Epoch 9530/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 0.9769\n",
            "Epoch 9530: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0369 - accuracy: 0.9769 - val_loss: 0.0539 - val_accuracy: 0.9861\n",
            "Epoch 9531/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0528 - accuracy: 0.9896\n",
            "Epoch 9531: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0470 - accuracy: 0.9907 - val_loss: 0.1499 - val_accuracy: 0.9583\n",
            "Epoch 9532/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9907\n",
            "Epoch 9532: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0203 - accuracy: 0.9907 - val_loss: 0.0666 - val_accuracy: 0.9861\n",
            "Epoch 9533/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9861\n",
            "Epoch 9533: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0244 - accuracy: 0.9861 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
            "Epoch 9534/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 1.0000\n",
            "Epoch 9534: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.0215 - val_accuracy: 0.9861\n",
            "Epoch 9535/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0501 - accuracy: 0.9861\n",
            "Epoch 9535: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0501 - accuracy: 0.9861 - val_loss: 0.0239 - val_accuracy: 1.0000\n",
            "Epoch 9536/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0290 - accuracy: 0.9792\n",
            "Epoch 9536: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 81ms/step - loss: 0.0260 - accuracy: 0.9815 - val_loss: 0.0134 - val_accuracy: 1.0000\n",
            "Epoch 9537/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 1.0000\n",
            "Epoch 9537: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.0067 - val_accuracy: 1.0000\n",
            "Epoch 9538/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0305 - accuracy: 0.9946\n",
            "Epoch 9538: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0261 - accuracy: 0.9954 - val_loss: 0.0624 - val_accuracy: 0.9583\n",
            "Epoch 9539/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0051 - accuracy: 1.0000\n",
            "Epoch 9539: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0387 - val_accuracy: 0.9722\n",
            "Epoch 9540/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 1.0000\n",
            "Epoch 9540: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.1020 - val_accuracy: 0.9583\n",
            "Epoch 9541/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0242 - accuracy: 0.9891\n",
            "Epoch 9541: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0207 - accuracy: 0.9907 - val_loss: 0.0192 - val_accuracy: 0.9861\n",
            "Epoch 9542/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0171 - accuracy: 0.9896\n",
            "Epoch 9542: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0174 - accuracy: 0.9907 - val_loss: 0.0113 - val_accuracy: 1.0000\n",
            "Epoch 9543/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0115 - accuracy: 1.0000\n",
            "Epoch 9543: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.0685 - val_accuracy: 0.9722\n",
            "Epoch 9544/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 9544: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0788 - val_accuracy: 0.9722\n",
            "Epoch 9545/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0091 - accuracy: 0.9946\n",
            "Epoch 9545: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0089 - accuracy: 0.9954 - val_loss: 0.0689 - val_accuracy: 0.9722\n",
            "Epoch 9546/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 9546: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0457 - val_accuracy: 0.9722\n",
            "Epoch 9547/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0105 - accuracy: 0.9954\n",
            "Epoch 9547: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0105 - accuracy: 0.9954 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 9548/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0114 - accuracy: 0.9948\n",
            "Epoch 9548: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0150 - accuracy: 0.9907 - val_loss: 0.0266 - val_accuracy: 0.9861\n",
            "Epoch 9549/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 0.9954\n",
            "Epoch 9549: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0112 - accuracy: 0.9954 - val_loss: 0.0161 - val_accuracy: 1.0000\n",
            "Epoch 9550/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9907\n",
            "Epoch 9550: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 0.0225 - accuracy: 0.9907 - val_loss: 0.0490 - val_accuracy: 0.9722\n",
            "Epoch 9551/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0285 - accuracy: 0.9891\n",
            "Epoch 9551: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0249 - accuracy: 0.9907 - val_loss: 0.0098 - val_accuracy: 1.0000\n",
            "Epoch 9552/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0223 - accuracy: 0.9891\n",
            "Epoch 9552: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0193 - accuracy: 0.9907 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
            "Epoch 9553/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9861\n",
            "Epoch 9553: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0243 - accuracy: 0.9861 - val_loss: 0.0102 - val_accuracy: 1.0000\n",
            "Epoch 9554/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 9554: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0283 - val_accuracy: 0.9861\n",
            "Epoch 9555/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9861\n",
            "Epoch 9555: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0250 - accuracy: 0.9861 - val_loss: 0.0380 - val_accuracy: 0.9861\n",
            "Epoch 9556/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0302 - accuracy: 0.9837\n",
            "Epoch 9556: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0270 - accuracy: 0.9861 - val_loss: 0.0197 - val_accuracy: 0.9861\n",
            "Epoch 9557/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0163 - accuracy: 0.9891\n",
            "Epoch 9557: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0145 - accuracy: 0.9907 - val_loss: 0.0461 - val_accuracy: 0.9861\n",
            "Epoch 9558/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0273 - accuracy: 0.9891\n",
            "Epoch 9558: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0239 - accuracy: 0.9907 - val_loss: 0.0175 - val_accuracy: 0.9861\n",
            "Epoch 9559/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9861\n",
            "Epoch 9559: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0218 - accuracy: 0.9861 - val_loss: 0.0093 - val_accuracy: 1.0000\n",
            "Epoch 9560/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9769\n",
            "Epoch 9560: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0459 - accuracy: 0.9769 - val_loss: 0.0615 - val_accuracy: 0.9722\n",
            "Epoch 9561/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 0.9861\n",
            "Epoch 9561: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0384 - accuracy: 0.9861 - val_loss: 0.0535 - val_accuracy: 0.9722\n",
            "Epoch 9562/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9861\n",
            "Epoch 9562: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0199 - accuracy: 0.9861 - val_loss: 0.0236 - val_accuracy: 0.9861\n",
            "Epoch 9563/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9954\n",
            "Epoch 9563: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0124 - accuracy: 0.9954 - val_loss: 0.0284 - val_accuracy: 0.9861\n",
            "Epoch 9564/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0115 - accuracy: 0.9946\n",
            "Epoch 9564: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0113 - accuracy: 0.9954 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
            "Epoch 9565/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 9565: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0260 - val_accuracy: 0.9861\n",
            "Epoch 9566/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0179 - accuracy: 0.9946\n",
            "Epoch 9566: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0208 - accuracy: 0.9907 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
            "Epoch 9567/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.9954\n",
            "Epoch 9567: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0079 - accuracy: 0.9954 - val_loss: 0.0404 - val_accuracy: 0.9722\n",
            "Epoch 9568/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0229 - accuracy: 0.9891\n",
            "Epoch 9568: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0218 - accuracy: 0.9907 - val_loss: 0.0423 - val_accuracy: 0.9722\n",
            "Epoch 9569/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 9569: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0138 - val_accuracy: 1.0000\n",
            "Epoch 9570/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0072 - accuracy: 1.0000\n",
            "Epoch 9570: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0158 - val_accuracy: 1.0000\n",
            "Epoch 9571/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 9571: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0321 - val_accuracy: 0.9861\n",
            "Epoch 9572/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0114 - accuracy: 0.9946\n",
            "Epoch 9572: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0107 - accuracy: 0.9954 - val_loss: 0.0098 - val_accuracy: 1.0000\n",
            "Epoch 9573/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 9573: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0947 - val_accuracy: 0.9861\n",
            "Epoch 9574/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0147 - accuracy: 0.9946\n",
            "Epoch 9574: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0130 - accuracy: 0.9954 - val_loss: 0.0315 - val_accuracy: 0.9861\n",
            "Epoch 9575/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0099 - accuracy: 0.9954\n",
            "Epoch 9575: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 79ms/step - loss: 0.0099 - accuracy: 0.9954 - val_loss: 0.0632 - val_accuracy: 0.9722\n",
            "Epoch 9576/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 9576: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 70ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0655 - val_accuracy: 0.9722\n",
            "Epoch 9577/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0070 - accuracy: 1.0000\n",
            "Epoch 9577: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
            "Epoch 9578/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0122 - accuracy: 0.9946\n",
            "Epoch 9578: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0312 - accuracy: 0.9907 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
            "Epoch 9579/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 9579: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.0170 - val_accuracy: 0.9861\n",
            "Epoch 9580/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9954\n",
            "Epoch 9580: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0138 - accuracy: 0.9954 - val_loss: 0.0345 - val_accuracy: 0.9861\n",
            "Epoch 9581/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 8.8996e-04 - accuracy: 1.0000\n",
            "Epoch 9581: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0060 - val_accuracy: 1.0000\n",
            "Epoch 9582/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 9582: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0119 - val_accuracy: 1.0000\n",
            "Epoch 9583/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0061 - accuracy: 0.9946\n",
            "Epoch 9583: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0177 - accuracy: 0.9907 - val_loss: 0.0143 - val_accuracy: 1.0000\n",
            "Epoch 9584/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9907\n",
            "Epoch 9584: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0380 - accuracy: 0.9907 - val_loss: 0.0113 - val_accuracy: 1.0000\n",
            "Epoch 9585/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0328 - accuracy: 0.9837\n",
            "Epoch 9585: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0285 - accuracy: 0.9861 - val_loss: 0.0282 - val_accuracy: 0.9861\n",
            "Epoch 9586/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0219 - accuracy: 0.9946\n",
            "Epoch 9586: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0208 - accuracy: 0.9954 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
            "Epoch 9587/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9954\n",
            "Epoch 9587: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0308 - accuracy: 0.9954 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
            "Epoch 9588/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0579 - accuracy: 0.9891\n",
            "Epoch 9588: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0507 - accuracy: 0.9907 - val_loss: 0.0224 - val_accuracy: 0.9861\n",
            "Epoch 9589/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9954\n",
            "Epoch 9589: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0132 - accuracy: 0.9954 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
            "Epoch 9590/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0188 - accuracy: 0.9891\n",
            "Epoch 9590: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0193 - accuracy: 0.9907 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
            "Epoch 9591/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 9591: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0493 - val_accuracy: 0.9722\n",
            "Epoch 9592/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 9592: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 79ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0612 - val_accuracy: 0.9722\n",
            "Epoch 9593/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9907\n",
            "Epoch 9593: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0187 - accuracy: 0.9907 - val_loss: 0.0201 - val_accuracy: 0.9861\n",
            "Epoch 9594/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9907\n",
            "Epoch 9594: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0155 - accuracy: 0.9907 - val_loss: 0.0119 - val_accuracy: 1.0000\n",
            "Epoch 9595/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 1.0000\n",
            "Epoch 9595: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
            "Epoch 9596/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0080 - accuracy: 0.9946\n",
            "Epoch 9596: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0188 - accuracy: 0.9907 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
            "Epoch 9597/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9861\n",
            "Epoch 9597: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0257 - accuracy: 0.9861 - val_loss: 0.0473 - val_accuracy: 0.9861\n",
            "Epoch 9598/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0169 - accuracy: 0.9948\n",
            "Epoch 9598: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0153 - accuracy: 0.9954 - val_loss: 0.0460 - val_accuracy: 0.9722\n",
            "Epoch 9599/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9907\n",
            "Epoch 9599: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0166 - accuracy: 0.9907 - val_loss: 0.0399 - val_accuracy: 0.9722\n",
            "Epoch 9600/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0353 - accuracy: 0.9861\n",
            "Epoch 9600: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0353 - accuracy: 0.9861 - val_loss: 0.1044 - val_accuracy: 0.9306\n",
            "Epoch 9601/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 1.0000\n",
            "Epoch 9601: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.0099 - val_accuracy: 1.0000\n",
            "Epoch 9602/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0259 - accuracy: 0.9891\n",
            "Epoch 9602: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0230 - accuracy: 0.9907 - val_loss: 0.0441 - val_accuracy: 0.9722\n",
            "Epoch 9603/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0348 - accuracy: 0.9837\n",
            "Epoch 9603: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0357 - accuracy: 0.9815 - val_loss: 0.0206 - val_accuracy: 0.9861\n",
            "Epoch 9604/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0145 - accuracy: 0.9946\n",
            "Epoch 9604: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0293 - accuracy: 0.9861 - val_loss: 0.0095 - val_accuracy: 1.0000\n",
            "Epoch 9605/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9861\n",
            "Epoch 9605: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0236 - accuracy: 0.9861 - val_loss: 0.0529 - val_accuracy: 0.9722\n",
            "Epoch 9606/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0145 - accuracy: 0.9946\n",
            "Epoch 9606: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0133 - accuracy: 0.9954 - val_loss: 0.0378 - val_accuracy: 0.9722\n",
            "Epoch 9607/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0651 - accuracy: 0.9891\n",
            "Epoch 9607: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0567 - accuracy: 0.9907 - val_loss: 0.0109 - val_accuracy: 1.0000\n",
            "Epoch 9608/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9907\n",
            "Epoch 9608: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0206 - accuracy: 0.9907 - val_loss: 0.0215 - val_accuracy: 1.0000\n",
            "Epoch 9609/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0398 - accuracy: 0.9861\n",
            "Epoch 9609: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0398 - accuracy: 0.9861 - val_loss: 0.0206 - val_accuracy: 0.9861\n",
            "Epoch 9610/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0204 - accuracy: 0.9891\n",
            "Epoch 9610: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0188 - accuracy: 0.9907 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
            "Epoch 9611/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9954\n",
            "Epoch 9611: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 76ms/step - loss: 0.0159 - accuracy: 0.9954 - val_loss: 0.0276 - val_accuracy: 0.9861\n",
            "Epoch 9612/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0207 - accuracy: 0.9946\n",
            "Epoch 9612: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0192 - accuracy: 0.9954 - val_loss: 0.0675 - val_accuracy: 0.9722\n",
            "Epoch 9613/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0113 - accuracy: 1.0000\n",
            "Epoch 9613: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.0345 - val_accuracy: 0.9861\n",
            "Epoch 9614/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0210 - accuracy: 0.9946\n",
            "Epoch 9614: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0181 - accuracy: 0.9954 - val_loss: 0.0185 - val_accuracy: 1.0000\n",
            "Epoch 9615/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0137 - accuracy: 0.9948\n",
            "Epoch 9615: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 71ms/step - loss: 0.0132 - accuracy: 0.9954 - val_loss: 0.0232 - val_accuracy: 1.0000\n",
            "Epoch 9616/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0122 - accuracy: 0.9946\n",
            "Epoch 9616: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0109 - accuracy: 0.9954 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
            "Epoch 9617/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0259 - accuracy: 0.9891\n",
            "Epoch 9617: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0251 - accuracy: 0.9907 - val_loss: 0.0064 - val_accuracy: 1.0000\n",
            "Epoch 9618/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 1.0000\n",
            "Epoch 9618: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0209 - val_accuracy: 1.0000\n",
            "Epoch 9619/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0333 - accuracy: 0.9891\n",
            "Epoch 9619: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0298 - accuracy: 0.9907 - val_loss: 0.0146 - val_accuracy: 1.0000\n",
            "Epoch 9620/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0226 - accuracy: 0.9946\n",
            "Epoch 9620: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0194 - accuracy: 0.9954 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
            "Epoch 9621/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0212 - accuracy: 0.9891\n",
            "Epoch 9621: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0572 - accuracy: 0.9815 - val_loss: 0.0253 - val_accuracy: 0.9861\n",
            "Epoch 9622/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0276 - accuracy: 0.9891\n",
            "Epoch 9622: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0251 - accuracy: 0.9907 - val_loss: 0.0160 - val_accuracy: 1.0000\n",
            "Epoch 9623/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0330 - accuracy: 0.9891\n",
            "Epoch 9623: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0313 - accuracy: 0.9907 - val_loss: 0.0779 - val_accuracy: 0.9861\n",
            "Epoch 9624/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0729 - accuracy: 0.9837\n",
            "Epoch 9624: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0929 - accuracy: 0.9769 - val_loss: 0.0070 - val_accuracy: 1.0000\n",
            "Epoch 9625/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.1939 - accuracy: 0.9674\n",
            "Epoch 9625: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.1689 - accuracy: 0.9722 - val_loss: 0.2993 - val_accuracy: 0.9306\n",
            "Epoch 9626/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.1961 - accuracy: 0.9620\n",
            "Epoch 9626: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.1678 - accuracy: 0.9676 - val_loss: 0.0680 - val_accuracy: 0.9722\n",
            "Epoch 9627/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9954\n",
            "Epoch 9627: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0356 - accuracy: 0.9954 - val_loss: 0.0598 - val_accuracy: 0.9861\n",
            "Epoch 9628/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0175 - accuracy: 0.9891\n",
            "Epoch 9628: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0311 - accuracy: 0.9815 - val_loss: 0.0287 - val_accuracy: 0.9861\n",
            "Epoch 9629/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9907\n",
            "Epoch 9629: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0407 - accuracy: 0.9907 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
            "Epoch 9630/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0527 - accuracy: 0.9815\n",
            "Epoch 9630: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0527 - accuracy: 0.9815 - val_loss: 0.0669 - val_accuracy: 0.9722\n",
            "Epoch 9631/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0168 - accuracy: 0.9896\n",
            "Epoch 9631: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0150 - accuracy: 0.9907 - val_loss: 0.1484 - val_accuracy: 0.9722\n",
            "Epoch 9632/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1389 - accuracy: 0.9769\n",
            "Epoch 9632: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.1389 - accuracy: 0.9769 - val_loss: 0.0563 - val_accuracy: 0.9861\n",
            "Epoch 9633/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0571 - accuracy: 0.9844\n",
            "Epoch 9633: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0521 - accuracy: 0.9861 - val_loss: 0.0380 - val_accuracy: 0.9861\n",
            "Epoch 9634/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0262 - accuracy: 0.9891\n",
            "Epoch 9634: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0230 - accuracy: 0.9907 - val_loss: 0.0108 - val_accuracy: 1.0000\n",
            "Epoch 9635/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0160 - accuracy: 0.9948\n",
            "Epoch 9635: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0209 - accuracy: 0.9907 - val_loss: 0.0137 - val_accuracy: 0.9861\n",
            "Epoch 9636/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0110 - accuracy: 0.9946\n",
            "Epoch 9636: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0095 - accuracy: 0.9954 - val_loss: 0.0272 - val_accuracy: 0.9861\n",
            "Epoch 9637/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0063 - accuracy: 1.0000\n",
            "Epoch 9637: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0169 - val_accuracy: 0.9861\n",
            "Epoch 9638/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9907\n",
            "Epoch 9638: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0214 - accuracy: 0.9907 - val_loss: 0.0148 - val_accuracy: 1.0000\n",
            "Epoch 9639/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0057 - accuracy: 1.0000\n",
            "Epoch 9639: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0357 - val_accuracy: 0.9861\n",
            "Epoch 9640/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9907\n",
            "Epoch 9640: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0200 - accuracy: 0.9907 - val_loss: 0.0280 - val_accuracy: 0.9861\n",
            "Epoch 9641/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 9641: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0160 - val_accuracy: 1.0000\n",
            "Epoch 9642/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0061 - accuracy: 1.0000\n",
            "Epoch 9642: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0667 - val_accuracy: 0.9583\n",
            "Epoch 9643/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 9643: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0151 - val_accuracy: 0.9861\n",
            "Epoch 9644/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0084 - accuracy: 0.9946\n",
            "Epoch 9644: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0076 - accuracy: 0.9954 - val_loss: 0.0220 - val_accuracy: 0.9861\n",
            "Epoch 9645/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0072 - accuracy: 1.0000\n",
            "Epoch 9645: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0197 - val_accuracy: 0.9861\n",
            "Epoch 9646/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0080 - accuracy: 0.9946\n",
            "Epoch 9646: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0073 - accuracy: 0.9954 - val_loss: 0.0165 - val_accuracy: 1.0000\n",
            "Epoch 9647/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 9647: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.0332 - val_accuracy: 0.9861\n",
            "Epoch 9648/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 9648: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0181 - val_accuracy: 0.9861\n",
            "Epoch 9649/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0131 - accuracy: 0.9946\n",
            "Epoch 9649: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0116 - accuracy: 0.9954 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
            "Epoch 9650/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 9650: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0326 - val_accuracy: 0.9861\n",
            "Epoch 9651/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9954\n",
            "Epoch 9651: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0129 - accuracy: 0.9954 - val_loss: 0.0230 - val_accuracy: 0.9861\n",
            "Epoch 9652/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0053 - accuracy: 1.0000\n",
            "Epoch 9652: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0250 - val_accuracy: 0.9861\n",
            "Epoch 9653/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 9653: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 1.0000\n",
            "Epoch 9654/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0048 - accuracy: 1.0000\n",
            "Epoch 9654: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 61ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0056 - val_accuracy: 1.0000\n",
            "Epoch 9655/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 9655: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0274 - val_accuracy: 0.9861\n",
            "Epoch 9656/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 9656: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0403 - val_accuracy: 0.9722\n",
            "Epoch 9657/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9954\n",
            "Epoch 9657: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0202 - accuracy: 0.9954 - val_loss: 0.0571 - val_accuracy: 0.9722\n",
            "Epoch 9658/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9907\n",
            "Epoch 9658: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0230 - accuracy: 0.9907 - val_loss: 0.0230 - val_accuracy: 0.9861\n",
            "Epoch 9659/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 9659: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0607 - val_accuracy: 0.9583\n",
            "Epoch 9660/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9861\n",
            "Epoch 9660: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0478 - accuracy: 0.9861 - val_loss: 0.0532 - val_accuracy: 0.9861\n",
            "Epoch 9661/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0364 - accuracy: 0.9837\n",
            "Epoch 9661: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0313 - accuracy: 0.9861 - val_loss: 0.0579 - val_accuracy: 0.9722\n",
            "Epoch 9662/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9954\n",
            "Epoch 9662: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0218 - accuracy: 0.9954 - val_loss: 0.0154 - val_accuracy: 1.0000\n",
            "Epoch 9663/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0063 - accuracy: 1.0000\n",
            "Epoch 9663: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0328 - val_accuracy: 0.9861\n",
            "Epoch 9664/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0121 - accuracy: 0.9946\n",
            "Epoch 9664: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0122 - accuracy: 0.9954 - val_loss: 0.0460 - val_accuracy: 0.9722\n",
            "Epoch 9665/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9907\n",
            "Epoch 9665: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0159 - accuracy: 0.9907 - val_loss: 0.0060 - val_accuracy: 1.0000\n",
            "Epoch 9666/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0433 - accuracy: 0.9769\n",
            "Epoch 9666: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0433 - accuracy: 0.9769 - val_loss: 0.0152 - val_accuracy: 1.0000\n",
            "Epoch 9667/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9861\n",
            "Epoch 9667: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0271 - accuracy: 0.9861 - val_loss: 0.0062 - val_accuracy: 1.0000\n",
            "Epoch 9668/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 1.0000\n",
            "Epoch 9668: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.0196 - val_accuracy: 0.9861\n",
            "Epoch 9669/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 9669: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 61ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
            "Epoch 9670/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0080 - accuracy: 1.0000\n",
            "Epoch 9670: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.0215 - val_accuracy: 1.0000\n",
            "Epoch 9671/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9907\n",
            "Epoch 9671: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0214 - accuracy: 0.9907 - val_loss: 0.0259 - val_accuracy: 1.0000\n",
            "Epoch 9672/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 9672: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.1051 - val_accuracy: 0.9583\n",
            "Epoch 9673/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 1.0000\n",
            "Epoch 9673: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.0059 - val_accuracy: 1.0000\n",
            "Epoch 9674/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 0.9954\n",
            "Epoch 9674: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0104 - accuracy: 0.9954 - val_loss: 0.0122 - val_accuracy: 1.0000\n",
            "Epoch 9675/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 9675: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0254 - val_accuracy: 0.9861\n",
            "Epoch 9676/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 9676: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0365 - val_accuracy: 0.9722\n",
            "Epoch 9677/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0126 - accuracy: 0.9948\n",
            "Epoch 9677: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0114 - accuracy: 0.9954 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
            "Epoch 9678/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 9678: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0291 - val_accuracy: 0.9861\n",
            "Epoch 9679/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0055 - accuracy: 1.0000\n",
            "Epoch 9679: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 71ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
            "Epoch 9680/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 1.0000\n",
            "Epoch 9680: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.0464 - val_accuracy: 0.9722\n",
            "Epoch 9681/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9954\n",
            "Epoch 9681: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0214 - accuracy: 0.9954 - val_loss: 0.0146 - val_accuracy: 1.0000\n",
            "Epoch 9682/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9861\n",
            "Epoch 9682: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0296 - accuracy: 0.9861 - val_loss: 0.0524 - val_accuracy: 0.9722\n",
            "Epoch 9683/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 9683: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0079 - val_accuracy: 1.0000\n",
            "Epoch 9684/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0096 - accuracy: 0.9946\n",
            "Epoch 9684: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0097 - accuracy: 0.9954 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
            "Epoch 9685/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 9685: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0063 - val_accuracy: 1.0000\n",
            "Epoch 9686/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9954\n",
            "Epoch 9686: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0123 - accuracy: 0.9954 - val_loss: 0.0150 - val_accuracy: 0.9861\n",
            "Epoch 9687/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0084 - accuracy: 0.9946\n",
            "Epoch 9687: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0074 - accuracy: 0.9954 - val_loss: 0.0144 - val_accuracy: 1.0000\n",
            "Epoch 9688/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 9688: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
            "Epoch 9689/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 1.0000\n",
            "Epoch 9689: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 78ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0066 - val_accuracy: 1.0000\n",
            "Epoch 9690/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0050 - accuracy: 0.9948\n",
            "Epoch 9690: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0046 - accuracy: 0.9954 - val_loss: 0.0532 - val_accuracy: 0.9722\n",
            "Epoch 9691/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0048 - accuracy: 1.0000\n",
            "Epoch 9691: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
            "Epoch 9692/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0189 - accuracy: 0.9946\n",
            "Epoch 9692: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
            "Epoch 9693/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 9693: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 9694/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0058 - accuracy: 1.0000\n",
            "Epoch 9694: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0296 - val_accuracy: 0.9861\n",
            "Epoch 9695/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 9695: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9861\n",
            "Epoch 9696/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000    \n",
            "Epoch 9696: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 0.9861\n",
            "Epoch 9697/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0157 - accuracy: 0.9946\n",
            "Epoch 9697: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0135 - accuracy: 0.9954 - val_loss: 0.1104 - val_accuracy: 0.9722\n",
            "Epoch 9698/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 9698: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0844 - val_accuracy: 0.9722\n",
            "Epoch 9699/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 9699: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.0069 - val_accuracy: 1.0000\n",
            "Epoch 9700/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 9700: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 9701/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0067 - accuracy: 1.0000\n",
            "Epoch 9701: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9861\n",
            "Epoch 9702/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0206 - accuracy: 0.9891\n",
            "Epoch 9702: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0179 - accuracy: 0.9907 - val_loss: 0.0501 - val_accuracy: 0.9722\n",
            "Epoch 9703/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 9703: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.0781 - val_accuracy: 0.9722\n",
            "Epoch 9704/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0158 - accuracy: 0.9896\n",
            "Epoch 9704: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0212 - accuracy: 0.9861 - val_loss: 0.0131 - val_accuracy: 1.0000\n",
            "Epoch 9705/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0444 - accuracy: 0.9891\n",
            "Epoch 9705: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0379 - accuracy: 0.9907 - val_loss: 0.0205 - val_accuracy: 0.9861\n",
            "Epoch 9706/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0058 - accuracy: 1.0000\n",
            "Epoch 9706: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.1904 - val_accuracy: 0.9722\n",
            "Epoch 9707/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0551 - accuracy: 0.9946\n",
            "Epoch 9707: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0547 - accuracy: 0.9907 - val_loss: 0.0494 - val_accuracy: 0.9722\n",
            "Epoch 9708/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9954\n",
            "Epoch 9708: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0178 - accuracy: 0.9954 - val_loss: 0.0211 - val_accuracy: 0.9861\n",
            "Epoch 9709/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 9709: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "Epoch 9710/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0063 - accuracy: 1.0000\n",
            "Epoch 9710: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0760 - val_accuracy: 0.9722\n",
            "Epoch 9711/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 9711: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0063 - val_accuracy: 1.0000\n",
            "Epoch 9712/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0174 - accuracy: 0.9891\n",
            "Epoch 9712: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0149 - accuracy: 0.9907 - val_loss: 0.0574 - val_accuracy: 0.9722\n",
            "Epoch 9713/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9861\n",
            "Epoch 9713: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0263 - accuracy: 0.9861 - val_loss: 0.0185 - val_accuracy: 1.0000\n",
            "Epoch 9714/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9954\n",
            "Epoch 9714: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 78ms/step - loss: 0.0291 - accuracy: 0.9954 - val_loss: 0.0640 - val_accuracy: 0.9583\n",
            "Epoch 9715/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0403 - accuracy: 0.9891\n",
            "Epoch 9715: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0343 - accuracy: 0.9907 - val_loss: 0.1247 - val_accuracy: 0.9861\n",
            "Epoch 9716/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0350 - accuracy: 0.9948\n",
            "Epoch 9716: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0322 - accuracy: 0.9954 - val_loss: 0.1235 - val_accuracy: 0.9583\n",
            "Epoch 9717/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9907\n",
            "Epoch 9717: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0260 - accuracy: 0.9907 - val_loss: 0.0133 - val_accuracy: 0.9861\n",
            "Epoch 9718/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9954\n",
            "Epoch 9718: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0148 - accuracy: 0.9954 - val_loss: 0.0539 - val_accuracy: 0.9722\n",
            "Epoch 9719/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 9719: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0276 - val_accuracy: 0.9722\n",
            "Epoch 9720/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9954\n",
            "Epoch 9720: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0155 - accuracy: 0.9954 - val_loss: 0.0578 - val_accuracy: 0.9861\n",
            "Epoch 9721/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.2341 - accuracy: 0.9891\n",
            "Epoch 9721: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 78ms/step - loss: 0.2002 - accuracy: 0.9907 - val_loss: 0.5116 - val_accuracy: 0.8472\n",
            "Epoch 9722/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.4455 - accuracy: 0.8913\n",
            "Epoch 9722: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.4123 - accuracy: 0.8935 - val_loss: 0.5003 - val_accuracy: 0.8750\n",
            "Epoch 9723/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.2666 - accuracy: 0.9402\n",
            "Epoch 9723: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.2648 - accuracy: 0.9352 - val_loss: 0.3151 - val_accuracy: 0.9167\n",
            "Epoch 9724/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.3333 - accuracy: 0.9348\n",
            "Epoch 9724: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.2842 - accuracy: 0.9444 - val_loss: 0.1431 - val_accuracy: 0.9444\n",
            "Epoch 9725/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1734 - accuracy: 0.9676\n",
            "Epoch 9725: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.1734 - accuracy: 0.9676 - val_loss: 0.0733 - val_accuracy: 0.9583\n",
            "Epoch 9726/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1994 - accuracy: 0.9398\n",
            "Epoch 9726: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 62ms/step - loss: 0.1994 - accuracy: 0.9398 - val_loss: 0.0356 - val_accuracy: 0.9861\n",
            "Epoch 9727/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0620 - accuracy: 0.9676\n",
            "Epoch 9727: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0620 - accuracy: 0.9676 - val_loss: 0.0592 - val_accuracy: 0.9722\n",
            "Epoch 9728/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0371 - accuracy: 0.9815\n",
            "Epoch 9728: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0371 - accuracy: 0.9815 - val_loss: 0.0666 - val_accuracy: 0.9722\n",
            "Epoch 9729/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0786 - accuracy: 0.9837\n",
            "Epoch 9729: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0718 - accuracy: 0.9815 - val_loss: 0.0374 - val_accuracy: 0.9861\n",
            "Epoch 9730/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0428 - accuracy: 0.9891\n",
            "Epoch 9730: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0572 - accuracy: 0.9815 - val_loss: 0.0128 - val_accuracy: 1.0000\n",
            "Epoch 9731/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0112 - accuracy: 1.0000\n",
            "Epoch 9731: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 0.9861\n",
            "Epoch 9732/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0601 - accuracy: 0.9783\n",
            "Epoch 9732: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0515 - accuracy: 0.9815 - val_loss: 0.0188 - val_accuracy: 0.9861\n",
            "Epoch 9733/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0280 - accuracy: 0.9946\n",
            "Epoch 9733: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0593 - accuracy: 0.9815 - val_loss: 0.0212 - val_accuracy: 1.0000\n",
            "Epoch 9734/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9954\n",
            "Epoch 9734: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0211 - accuracy: 0.9954 - val_loss: 0.0277 - val_accuracy: 1.0000\n",
            "Epoch 9735/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0104 - accuracy: 1.0000\n",
            "Epoch 9735: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0086 - val_accuracy: 1.0000\n",
            "Epoch 9736/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0350 - accuracy: 0.9891\n",
            "Epoch 9736: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0321 - accuracy: 0.9907 - val_loss: 0.0212 - val_accuracy: 1.0000\n",
            "Epoch 9737/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9954\n",
            "Epoch 9737: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0173 - accuracy: 0.9954 - val_loss: 0.0275 - val_accuracy: 0.9861\n",
            "Epoch 9738/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 1.0000\n",
            "Epoch 9738: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.0289 - val_accuracy: 0.9861\n",
            "Epoch 9739/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9954\n",
            "Epoch 9739: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0117 - accuracy: 0.9954 - val_loss: 0.0167 - val_accuracy: 1.0000\n",
            "Epoch 9740/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 9740: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0209 - val_accuracy: 0.9861\n",
            "Epoch 9741/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9954\n",
            "Epoch 9741: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 76ms/step - loss: 0.0118 - accuracy: 0.9954 - val_loss: 0.0368 - val_accuracy: 0.9722\n",
            "Epoch 9742/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 9742: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0136 - val_accuracy: 1.0000\n",
            "Epoch 9743/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9907\n",
            "Epoch 9743: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0197 - accuracy: 0.9907 - val_loss: 0.0346 - val_accuracy: 0.9861\n",
            "Epoch 9744/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0105 - accuracy: 0.9954\n",
            "Epoch 9744: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0105 - accuracy: 0.9954 - val_loss: 0.0181 - val_accuracy: 0.9861\n",
            "Epoch 9745/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9954\n",
            "Epoch 9745: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0223 - accuracy: 0.9954 - val_loss: 0.0209 - val_accuracy: 0.9861\n",
            "Epoch 9746/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0125 - accuracy: 0.9948\n",
            "Epoch 9746: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 61ms/step - loss: 0.0123 - accuracy: 0.9954 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "Epoch 9747/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0094 - accuracy: 1.0000\n",
            "Epoch 9747: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.0188 - val_accuracy: 0.9861\n",
            "Epoch 9748/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0057 - accuracy: 1.0000\n",
            "Epoch 9748: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0333 - val_accuracy: 0.9861\n",
            "Epoch 9749/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 1.0000\n",
            "Epoch 9749: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.0079 - val_accuracy: 1.0000\n",
            "Epoch 9750/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 9750: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0135 - val_accuracy: 1.0000\n",
            "Epoch 9751/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 9751: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
            "Epoch 9752/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 0.9954\n",
            "Epoch 9752: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0104 - accuracy: 0.9954 - val_loss: 0.0628 - val_accuracy: 0.9722\n",
            "Epoch 9753/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 9753: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0321 - val_accuracy: 0.9861\n",
            "Epoch 9754/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 0.9954\n",
            "Epoch 9754: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 62ms/step - loss: 0.0093 - accuracy: 0.9954 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
            "Epoch 9755/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 9755: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0817 - val_accuracy: 0.9722\n",
            "Epoch 9756/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 9756: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0165 - val_accuracy: 0.9861\n",
            "Epoch 9757/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0078 - accuracy: 1.0000\n",
            "Epoch 9757: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0057 - val_accuracy: 1.0000\n",
            "Epoch 9758/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0071 - accuracy: 1.0000\n",
            "Epoch 9758: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0568 - val_accuracy: 0.9583\n",
            "Epoch 9759/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 0.9954\n",
            "Epoch 9759: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0101 - accuracy: 0.9954 - val_loss: 0.1137 - val_accuracy: 0.9722\n",
            "Epoch 9760/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.9907\n",
            "Epoch 9760: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0338 - accuracy: 0.9907 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
            "Epoch 9761/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 9761: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0293 - val_accuracy: 0.9861\n",
            "Epoch 9762/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0305 - accuracy: 0.9891\n",
            "Epoch 9762: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0670 - accuracy: 0.9861 - val_loss: 0.0658 - val_accuracy: 0.9583\n",
            "Epoch 9763/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9861\n",
            "Epoch 9763: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0201 - accuracy: 0.9861 - val_loss: 0.0315 - val_accuracy: 0.9861\n",
            "Epoch 9764/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0071 - accuracy: 1.0000\n",
            "Epoch 9764: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0253 - val_accuracy: 0.9861\n",
            "Epoch 9765/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.9954\n",
            "Epoch 9765: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0086 - accuracy: 0.9954 - val_loss: 0.1106 - val_accuracy: 0.9722\n",
            "Epoch 9766/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 9766: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0116 - accuracy: 0.9954 - val_loss: 0.0172 - val_accuracy: 1.0000\n",
            "Epoch 9767/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 1.0000\n",
            "Epoch 9767: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 79ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.0410 - val_accuracy: 0.9861\n",
            "Epoch 9768/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0082 - accuracy: 0.9946\n",
            "Epoch 9768: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0075 - accuracy: 0.9954 - val_loss: 0.0609 - val_accuracy: 0.9722\n",
            "Epoch 9769/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.9954\n",
            "Epoch 9769: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0087 - accuracy: 0.9954 - val_loss: 0.0568 - val_accuracy: 0.9722\n",
            "Epoch 9770/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 9770: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
            "Epoch 9771/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9861\n",
            "Epoch 9771: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 76ms/step - loss: 0.0691 - accuracy: 0.9861 - val_loss: 0.0257 - val_accuracy: 0.9861\n",
            "Epoch 9772/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 0.9954\n",
            "Epoch 9772: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0322 - accuracy: 0.9954 - val_loss: 0.0073 - val_accuracy: 1.0000\n",
            "Epoch 9773/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 0.9954\n",
            "Epoch 9773: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0078 - accuracy: 0.9954 - val_loss: 0.0083 - val_accuracy: 1.0000\n",
            "Epoch 9774/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0111 - accuracy: 0.9946\n",
            "Epoch 9774: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0101 - accuracy: 0.9954 - val_loss: 0.0199 - val_accuracy: 0.9861\n",
            "Epoch 9775/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0057 - accuracy: 1.0000\n",
            "Epoch 9775: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0312 - val_accuracy: 0.9861\n",
            "Epoch 9776/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0084 - accuracy: 0.9948\n",
            "Epoch 9776: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0075 - accuracy: 0.9954 - val_loss: 0.0099 - val_accuracy: 1.0000\n",
            "Epoch 9777/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9907\n",
            "Epoch 9777: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0169 - accuracy: 0.9907 - val_loss: 0.0294 - val_accuracy: 0.9861\n",
            "Epoch 9778/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0299 - accuracy: 0.9861\n",
            "Epoch 9778: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 70ms/step - loss: 0.0299 - accuracy: 0.9861 - val_loss: 0.0110 - val_accuracy: 1.0000\n",
            "Epoch 9779/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0099 - accuracy: 0.9948\n",
            "Epoch 9779: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0097 - accuracy: 0.9954 - val_loss: 0.0750 - val_accuracy: 0.9722\n",
            "Epoch 9780/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0127 - accuracy: 0.9946\n",
            "Epoch 9780: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0115 - accuracy: 0.9954 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
            "Epoch 9781/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9861\n",
            "Epoch 9781: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0240 - accuracy: 0.9861 - val_loss: 0.0594 - val_accuracy: 0.9583\n",
            "Epoch 9782/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0529 - accuracy: 0.9891\n",
            "Epoch 9782: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0452 - accuracy: 0.9907 - val_loss: 0.0092 - val_accuracy: 1.0000\n",
            "Epoch 9783/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 9783: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.0351 - val_accuracy: 0.9861\n",
            "Epoch 9784/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.9954\n",
            "Epoch 9784: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0090 - accuracy: 0.9954 - val_loss: 0.0137 - val_accuracy: 1.0000\n",
            "Epoch 9785/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 1.0000\n",
            "Epoch 9785: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.0199 - val_accuracy: 0.9861\n",
            "Epoch 9786/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 9786: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0193 - val_accuracy: 0.9861\n",
            "Epoch 9787/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0076 - accuracy: 1.0000\n",
            "Epoch 9787: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.0097 - val_accuracy: 1.0000\n",
            "Epoch 9788/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0083 - accuracy: 0.9948\n",
            "Epoch 9788: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0075 - accuracy: 0.9954 - val_loss: 0.0330 - val_accuracy: 0.9861\n",
            "Epoch 9789/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 9789: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0068 - val_accuracy: 1.0000\n",
            "Epoch 9790/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9954\n",
            "Epoch 9790: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0147 - accuracy: 0.9954 - val_loss: 0.0245 - val_accuracy: 0.9861\n",
            "Epoch 9791/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 9791: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0527 - val_accuracy: 0.9861\n",
            "Epoch 9792/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9954\n",
            "Epoch 9792: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0111 - accuracy: 0.9954 - val_loss: 0.0421 - val_accuracy: 0.9861\n",
            "Epoch 9793/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 9793: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9861\n",
            "Epoch 9794/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0071 - accuracy: 0.9946\n",
            "Epoch 9794: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0063 - accuracy: 0.9954 - val_loss: 0.0214 - val_accuracy: 1.0000\n",
            "Epoch 9795/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0121 - accuracy: 0.9946\n",
            "Epoch 9795: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0107 - accuracy: 0.9954 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
            "Epoch 9796/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0096 - accuracy: 1.0000\n",
            "Epoch 9796: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 76ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0290 - val_accuracy: 0.9861\n",
            "Epoch 9797/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0102 - accuracy: 0.9946\n",
            "Epoch 9797: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0089 - accuracy: 0.9954 - val_loss: 0.0207 - val_accuracy: 0.9861\n",
            "Epoch 9798/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1635 - accuracy: 0.9722\n",
            "Epoch 9798: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.1635 - accuracy: 0.9722 - val_loss: 1.4939 - val_accuracy: 0.8333\n",
            "Epoch 9799/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.5134 - accuracy: 0.9022\n",
            "Epoch 9799: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.5088 - accuracy: 0.9028 - val_loss: 0.5557 - val_accuracy: 0.8889\n",
            "Epoch 9800/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 1.5669 - accuracy: 0.9076\n",
            "Epoch 9800: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 1.3642 - accuracy: 0.9120 - val_loss: 0.1268 - val_accuracy: 0.9444\n",
            "Epoch 9801/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1505 - accuracy: 0.9398\n",
            "Epoch 9801: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.1505 - accuracy: 0.9398 - val_loss: 0.0392 - val_accuracy: 0.9861\n",
            "Epoch 9802/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1076 - accuracy: 0.9630\n",
            "Epoch 9802: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.1076 - accuracy: 0.9630 - val_loss: 0.1033 - val_accuracy: 0.9722\n",
            "Epoch 9803/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.2930 - accuracy: 0.9062\n",
            "Epoch 9803: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.2665 - accuracy: 0.9120 - val_loss: 0.0411 - val_accuracy: 0.9861\n",
            "Epoch 9804/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0544 - accuracy: 0.9844\n",
            "Epoch 9804: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0498 - accuracy: 0.9861 - val_loss: 0.0101 - val_accuracy: 1.0000\n",
            "Epoch 9805/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0461 - accuracy: 0.9740\n",
            "Epoch 9805: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0428 - accuracy: 0.9769 - val_loss: 0.0947 - val_accuracy: 0.9583\n",
            "Epoch 9806/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0151 - accuracy: 0.9946\n",
            "Epoch 9806: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0133 - accuracy: 0.9954 - val_loss: 0.0256 - val_accuracy: 0.9861\n",
            "Epoch 9807/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0775 - accuracy: 0.9946\n",
            "Epoch 9807: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0805 - accuracy: 0.9907 - val_loss: 0.1874 - val_accuracy: 0.9583\n",
            "Epoch 9808/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.5874 - accuracy: 0.9074\n",
            "Epoch 9808: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 71ms/step - loss: 0.5874 - accuracy: 0.9074 - val_loss: 0.4642 - val_accuracy: 0.8611\n",
            "Epoch 9809/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.1983 - accuracy: 0.9457\n",
            "Epoch 9809: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 70ms/step - loss: 0.1801 - accuracy: 0.9491 - val_loss: 0.1219 - val_accuracy: 0.9583\n",
            "Epoch 9810/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.4676 - accuracy: 0.9239\n",
            "Epoch 9810: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.4746 - accuracy: 0.9213 - val_loss: 0.2940 - val_accuracy: 0.9028\n",
            "Epoch 9811/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.1318 - accuracy: 0.9565\n",
            "Epoch 9811: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.1135 - accuracy: 0.9630 - val_loss: 0.1370 - val_accuracy: 0.9583\n",
            "Epoch 9812/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.1153 - accuracy: 0.9620\n",
            "Epoch 9812: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.1086 - accuracy: 0.9630 - val_loss: 0.1069 - val_accuracy: 0.9444\n",
            "Epoch 9813/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0286 - accuracy: 0.9946\n",
            "Epoch 9813: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0372 - accuracy: 0.9861 - val_loss: 0.0731 - val_accuracy: 0.9722\n",
            "Epoch 9814/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9907\n",
            "Epoch 9814: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0228 - accuracy: 0.9907 - val_loss: 0.0223 - val_accuracy: 0.9861\n",
            "Epoch 9815/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0251 - accuracy: 0.9946\n",
            "Epoch 9815: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0240 - accuracy: 0.9954 - val_loss: 0.0210 - val_accuracy: 1.0000\n",
            "Epoch 9816/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0636 - accuracy: 0.9722\n",
            "Epoch 9816: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0636 - accuracy: 0.9722 - val_loss: 0.0146 - val_accuracy: 0.9861\n",
            "Epoch 9817/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0432 - accuracy: 0.9907\n",
            "Epoch 9817: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0432 - accuracy: 0.9907 - val_loss: 0.0854 - val_accuracy: 0.9722\n",
            "Epoch 9818/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0201 - accuracy: 0.9946\n",
            "Epoch 9818: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0179 - accuracy: 0.9954 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
            "Epoch 9819/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0504 - accuracy: 0.9837\n",
            "Epoch 9819: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0540 - accuracy: 0.9815 - val_loss: 0.0214 - val_accuracy: 0.9861\n",
            "Epoch 9820/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 1.0000\n",
            "Epoch 9820: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0084 - val_accuracy: 1.0000\n",
            "Epoch 9821/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0294 - accuracy: 0.9946\n",
            "Epoch 9821: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0307 - accuracy: 0.9907 - val_loss: 0.0236 - val_accuracy: 1.0000\n",
            "Epoch 9822/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9954\n",
            "Epoch 9822: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 78ms/step - loss: 0.0315 - accuracy: 0.9954 - val_loss: 0.1736 - val_accuracy: 0.9722\n",
            "Epoch 9823/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.2894 - accuracy: 0.9891\n",
            "Epoch 9823: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.2484 - accuracy: 0.9907 - val_loss: 0.0193 - val_accuracy: 1.0000\n",
            "Epoch 9824/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0294 - accuracy: 0.9891\n",
            "Epoch 9824: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0257 - accuracy: 0.9907 - val_loss: 0.0692 - val_accuracy: 0.9583\n",
            "Epoch 9825/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9954\n",
            "Epoch 9825: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 78ms/step - loss: 0.0275 - accuracy: 0.9954 - val_loss: 0.0604 - val_accuracy: 0.9583\n",
            "Epoch 9826/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.1534 - accuracy: 0.9783\n",
            "Epoch 9826: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.1347 - accuracy: 0.9769 - val_loss: 0.0621 - val_accuracy: 0.9722\n",
            "Epoch 9827/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0231 - accuracy: 0.9891\n",
            "Epoch 9827: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0198 - accuracy: 0.9907 - val_loss: 0.0321 - val_accuracy: 1.0000\n",
            "Epoch 9828/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9861\n",
            "Epoch 9828: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0403 - accuracy: 0.9861 - val_loss: 0.0870 - val_accuracy: 0.9722\n",
            "Epoch 9829/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9954\n",
            "Epoch 9829: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 71ms/step - loss: 0.0206 - accuracy: 0.9954 - val_loss: 0.0122 - val_accuracy: 1.0000\n",
            "Epoch 9830/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0113 - accuracy: 0.9946\n",
            "Epoch 9830: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0103 - accuracy: 0.9954 - val_loss: 0.0087 - val_accuracy: 1.0000\n",
            "Epoch 9831/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 9831: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0113 - accuracy: 0.9954 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
            "Epoch 9832/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0152 - accuracy: 0.9946\n",
            "Epoch 9832: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0142 - accuracy: 0.9954 - val_loss: 0.0303 - val_accuracy: 0.9722\n",
            "Epoch 9833/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0569 - accuracy: 0.9837\n",
            "Epoch 9833: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0487 - accuracy: 0.9861 - val_loss: 0.0273 - val_accuracy: 0.9861\n",
            "Epoch 9834/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.9954\n",
            "Epoch 9834: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0079 - accuracy: 0.9954 - val_loss: 0.0287 - val_accuracy: 0.9722\n",
            "Epoch 9835/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0069 - accuracy: 1.0000\n",
            "Epoch 9835: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 9836/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 9836: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 80ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
            "Epoch 9837/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0289 - accuracy: 0.9891\n",
            "Epoch 9837: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0254 - accuracy: 0.9907 - val_loss: 0.0226 - val_accuracy: 0.9861\n",
            "Epoch 9838/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 1.0000\n",
            "Epoch 9838: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.0412 - val_accuracy: 0.9861\n",
            "Epoch 9839/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 9839: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
            "Epoch 9840/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0076 - accuracy: 1.0000\n",
            "Epoch 9840: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.0137 - val_accuracy: 0.9861\n",
            "Epoch 9841/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.9954\n",
            "Epoch 9841: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0090 - accuracy: 0.9954 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
            "Epoch 9842/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 9842: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 1.0000\n",
            "Epoch 9843/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0116 - accuracy: 0.9946\n",
            "Epoch 9843: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0102 - accuracy: 0.9954 - val_loss: 0.0398 - val_accuracy: 0.9861\n",
            "Epoch 9844/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 9844: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9861\n",
            "Epoch 9845/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0065 - accuracy: 1.0000\n",
            "Epoch 9845: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0361 - val_accuracy: 0.9861\n",
            "Epoch 9846/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 9846: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0172 - val_accuracy: 0.9861\n",
            "Epoch 9847/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 9847: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0086 - val_accuracy: 1.0000\n",
            "Epoch 9848/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 9848: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
            "Epoch 9849/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0066 - accuracy: 1.0000\n",
            "Epoch 9849: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0121 - val_accuracy: 1.0000\n",
            "Epoch 9850/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0150 - accuracy: 0.9946\n",
            "Epoch 9850: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0138 - accuracy: 0.9954 - val_loss: 0.0104 - val_accuracy: 1.0000\n",
            "Epoch 9851/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 9851: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 9852/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0081 - accuracy: 1.0000\n",
            "Epoch 9852: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
            "Epoch 9853/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0454 - accuracy: 0.9946\n",
            "Epoch 9853: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0404 - accuracy: 0.9954 - val_loss: 0.0256 - val_accuracy: 0.9861\n",
            "Epoch 9854/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 1.0000\n",
            "Epoch 9854: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.0146 - val_accuracy: 1.0000\n",
            "Epoch 9855/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0056 - accuracy: 0.9954\n",
            "Epoch 9855: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0056 - accuracy: 0.9954 - val_loss: 0.0276 - val_accuracy: 0.9861\n",
            "Epoch 9856/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0070 - accuracy: 1.0000\n",
            "Epoch 9856: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0397 - val_accuracy: 0.9861\n",
            "Epoch 9857/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0065 - accuracy: 1.0000\n",
            "Epoch 9857: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0273 - val_accuracy: 0.9722\n",
            "Epoch 9858/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0055 - accuracy: 1.0000\n",
            "Epoch 9858: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.2688 - val_accuracy: 0.9583\n",
            "Epoch 9859/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 9859: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.2553 - val_accuracy: 0.9722\n",
            "Epoch 9860/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 9860: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.2791 - val_accuracy: 0.9583\n",
            "Epoch 9861/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 9861: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 70ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 9862/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9861\n",
            "Epoch 9862: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0691 - accuracy: 0.9861 - val_loss: 0.0364 - val_accuracy: 0.9861\n",
            "Epoch 9863/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 9863: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.1872 - val_accuracy: 0.9861\n",
            "Epoch 9864/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0158 - accuracy: 0.9946\n",
            "Epoch 9864: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0150 - accuracy: 0.9954 - val_loss: 0.0374 - val_accuracy: 0.9722\n",
            "Epoch 9865/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 9865: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.2985 - val_accuracy: 0.9722\n",
            "Epoch 9866/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 9866: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.1913 - val_accuracy: 0.9722\n",
            "Epoch 9867/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 1.0000\n",
            "Epoch 9867: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0871 - val_accuracy: 0.9722\n",
            "Epoch 9868/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0053 - accuracy: 1.0000\n",
            "Epoch 9868: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.1550 - val_accuracy: 0.9722\n",
            "Epoch 9869/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9907\n",
            "Epoch 9869: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0245 - accuracy: 0.9907 - val_loss: 0.3989 - val_accuracy: 0.9722\n",
            "Epoch 9870/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0649 - accuracy: 0.9896\n",
            "Epoch 9870: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0584 - accuracy: 0.9907 - val_loss: 0.0337 - val_accuracy: 0.9861\n",
            "Epoch 9871/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9907\n",
            "Epoch 9871: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0167 - accuracy: 0.9907 - val_loss: 0.0439 - val_accuracy: 0.9861\n",
            "Epoch 9872/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0174 - accuracy: 0.9946\n",
            "Epoch 9872: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0151 - accuracy: 0.9954 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
            "Epoch 9873/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 9873: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 81ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0674 - val_accuracy: 0.9722\n",
            "Epoch 9874/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0067 - accuracy: 1.0000\n",
            "Epoch 9874: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0190 - val_accuracy: 0.9861\n",
            "Epoch 9875/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 9875: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0345 - val_accuracy: 0.9722\n",
            "Epoch 9876/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9907\n",
            "Epoch 9876: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0146 - accuracy: 0.9907 - val_loss: 0.0141 - val_accuracy: 1.0000\n",
            "Epoch 9877/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000    \n",
            "Epoch 9877: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9861\n",
            "Epoch 9878/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0189 - accuracy: 0.9946\n",
            "Epoch 9878: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0190 - accuracy: 0.9954 - val_loss: 0.0260 - val_accuracy: 0.9861\n",
            "Epoch 9879/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0106 - accuracy: 1.0000\n",
            "Epoch 9879: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
            "Epoch 9880/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 9880: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.0093 - val_accuracy: 1.0000\n",
            "Epoch 9881/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0201 - accuracy: 0.9946\n",
            "Epoch 9881: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0186 - accuracy: 0.9954 - val_loss: 0.0059 - val_accuracy: 1.0000\n",
            "Epoch 9882/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0362 - accuracy: 0.9891\n",
            "Epoch 9882: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0325 - accuracy: 0.9907 - val_loss: 0.0828 - val_accuracy: 0.9861\n",
            "Epoch 9883/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 9883: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.0511 - val_accuracy: 0.9722\n",
            "Epoch 9884/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.9907\n",
            "Epoch 9884: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0269 - accuracy: 0.9907 - val_loss: 0.0849 - val_accuracy: 0.9583\n",
            "Epoch 9885/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0117 - accuracy: 1.0000\n",
            "Epoch 9885: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.0168 - val_accuracy: 0.9861\n",
            "Epoch 9886/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9954\n",
            "Epoch 9886: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0121 - accuracy: 0.9954 - val_loss: 0.0520 - val_accuracy: 0.9861\n",
            "Epoch 9887/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 9887: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0076 - val_accuracy: 1.0000\n",
            "Epoch 9888/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0065 - accuracy: 0.9946\n",
            "Epoch 9888: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0068 - accuracy: 0.9954 - val_loss: 0.0060 - val_accuracy: 1.0000\n",
            "Epoch 9889/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.9954\n",
            "Epoch 9889: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 74ms/step - loss: 0.0375 - accuracy: 0.9954 - val_loss: 0.0244 - val_accuracy: 0.9861\n",
            "Epoch 9890/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 9890: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 76ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0154 - val_accuracy: 0.9861\n",
            "Epoch 9891/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 9891: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0162 - val_accuracy: 0.9861\n",
            "Epoch 9892/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 9892: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 76ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0087 - val_accuracy: 1.0000\n",
            "Epoch 9893/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0081 - accuracy: 1.0000\n",
            "Epoch 9893: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0113 - accuracy: 0.9954 - val_loss: 0.0255 - val_accuracy: 0.9861\n",
            "Epoch 9894/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 9894: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0074 - val_accuracy: 1.0000\n",
            "Epoch 9895/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 0.9861\n",
            "Epoch 9895: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0369 - accuracy: 0.9861 - val_loss: 0.0613 - val_accuracy: 0.9722\n",
            "Epoch 9896/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0141 - accuracy: 0.9948\n",
            "Epoch 9896: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0219 - accuracy: 0.9907 - val_loss: 0.0253 - val_accuracy: 0.9861\n",
            "Epoch 9897/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0299 - accuracy: 0.9891\n",
            "Epoch 9897: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0255 - accuracy: 0.9907 - val_loss: 0.1223 - val_accuracy: 0.9583\n",
            "Epoch 9898/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0091 - accuracy: 0.9946\n",
            "Epoch 9898: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0132 - accuracy: 0.9907 - val_loss: 0.0162 - val_accuracy: 1.0000\n",
            "Epoch 9899/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 9899: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 71ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0212 - val_accuracy: 1.0000\n",
            "Epoch 9900/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0095 - accuracy: 1.0000\n",
            "Epoch 9900: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 9901/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0128 - accuracy: 0.9896\n",
            "Epoch 9901: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0115 - accuracy: 0.9907 - val_loss: 0.1195 - val_accuracy: 0.9861\n",
            "Epoch 9902/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 9902: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0075 - val_accuracy: 1.0000\n",
            "Epoch 9903/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 9903: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0466 - val_accuracy: 0.9861\n",
            "Epoch 9904/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0061 - accuracy: 1.0000\n",
            "Epoch 9904: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.0087 - val_accuracy: 1.0000\n",
            "Epoch 9905/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0097 - accuracy: 0.9946\n",
            "Epoch 9905: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 80ms/step - loss: 0.0086 - accuracy: 0.9954 - val_loss: 0.0884 - val_accuracy: 0.9722\n",
            "Epoch 9906/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 9906: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.0258 - val_accuracy: 0.9861\n",
            "Epoch 9907/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 9907: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
            "Epoch 9908/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0111 - accuracy: 0.9946\n",
            "Epoch 9908: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0099 - accuracy: 0.9954 - val_loss: 0.0136 - val_accuracy: 1.0000\n",
            "Epoch 9909/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 9909: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
            "Epoch 9910/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0098 - accuracy: 1.0000\n",
            "Epoch 9910: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.0194 - val_accuracy: 0.9861\n",
            "Epoch 9911/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 9911: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.0255 - val_accuracy: 0.9861\n",
            "Epoch 9912/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0083 - accuracy: 1.0000\n",
            "Epoch 9912: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
            "Epoch 9913/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0063 - accuracy: 1.0000\n",
            "Epoch 9913: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0714 - val_accuracy: 0.9861\n",
            "Epoch 9914/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9954\n",
            "Epoch 9914: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 0.0121 - accuracy: 0.9954 - val_loss: 0.0104 - val_accuracy: 1.0000\n",
            "Epoch 9915/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 9915: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0457 - val_accuracy: 0.9722\n",
            "Epoch 9916/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0119 - accuracy: 0.9946\n",
            "Epoch 9916: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0124 - accuracy: 0.9954 - val_loss: 0.0437 - val_accuracy: 0.9861\n",
            "Epoch 9917/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 9917: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0139 - val_accuracy: 1.0000\n",
            "Epoch 9918/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.9954\n",
            "Epoch 9918: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0085 - accuracy: 0.9954 - val_loss: 0.0114 - val_accuracy: 1.0000\n",
            "Epoch 9919/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0058 - accuracy: 0.9954\n",
            "Epoch 9919: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.0058 - accuracy: 0.9954 - val_loss: 0.0062 - val_accuracy: 1.0000\n",
            "Epoch 9920/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0224 - accuracy: 0.9946\n",
            "Epoch 9920: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0197 - accuracy: 0.9954 - val_loss: 0.0058 - val_accuracy: 1.0000\n",
            "Epoch 9921/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 9921: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 1.0000\n",
            "Epoch 9922/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0159 - accuracy: 0.9946\n",
            "Epoch 9922: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0137 - accuracy: 0.9954 - val_loss: 0.0089 - val_accuracy: 1.0000\n",
            "Epoch 9923/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0063 - accuracy: 0.9946\n",
            "Epoch 9923: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0059 - accuracy: 0.9954 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
            "Epoch 9924/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0243 - accuracy: 0.9896\n",
            "Epoch 9924: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0216 - accuracy: 0.9907 - val_loss: 0.0399 - val_accuracy: 0.9861\n",
            "Epoch 9925/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 9925: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
            "Epoch 9926/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0055 - accuracy: 1.0000\n",
            "Epoch 9926: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
            "Epoch 9927/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 9927: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0262 - val_accuracy: 1.0000\n",
            "Epoch 9928/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 9928: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0058 - val_accuracy: 1.0000\n",
            "Epoch 9929/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9954\n",
            "Epoch 9929: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0128 - accuracy: 0.9954 - val_loss: 0.0210 - val_accuracy: 0.9861\n",
            "Epoch 9930/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0089 - accuracy: 0.9946\n",
            "Epoch 9930: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0085 - accuracy: 0.9954 - val_loss: 0.0421 - val_accuracy: 0.9861\n",
            "Epoch 9931/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0091 - accuracy: 0.9948\n",
            "Epoch 9931: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0083 - accuracy: 0.9954 - val_loss: 0.0307 - val_accuracy: 0.9861\n",
            "Epoch 9932/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 9932: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0161 - val_accuracy: 0.9861\n",
            "Epoch 9933/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0093 - accuracy: 0.9948\n",
            "Epoch 9933: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0173 - accuracy: 0.9907 - val_loss: 0.0068 - val_accuracy: 1.0000\n",
            "Epoch 9934/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0251 - accuracy: 0.9946\n",
            "Epoch 9934: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0218 - accuracy: 0.9954 - val_loss: 0.0728 - val_accuracy: 0.9861\n",
            "Epoch 9935/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9861\n",
            "Epoch 9935: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0181 - accuracy: 0.9861 - val_loss: 0.0171 - val_accuracy: 1.0000\n",
            "Epoch 9936/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0078 - accuracy: 1.0000\n",
            "Epoch 9936: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0404 - val_accuracy: 0.9861\n",
            "Epoch 9937/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0230 - accuracy: 0.9891\n",
            "Epoch 9937: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0209 - accuracy: 0.9907 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
            "Epoch 9938/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0125 - accuracy: 0.9946\n",
            "Epoch 9938: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0122 - accuracy: 0.9954 - val_loss: 0.0183 - val_accuracy: 0.9861\n",
            "Epoch 9939/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0093 - accuracy: 1.0000\n",
            "Epoch 9939: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.0158 - val_accuracy: 1.0000\n",
            "Epoch 9940/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0119 - accuracy: 0.9946\n",
            "Epoch 9940: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0105 - accuracy: 0.9954 - val_loss: 0.0387 - val_accuracy: 0.9861\n",
            "Epoch 9941/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9954\n",
            "Epoch 9941: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0175 - accuracy: 0.9954 - val_loss: 0.0621 - val_accuracy: 0.9861\n",
            "Epoch 9942/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0070 - accuracy: 0.9948\n",
            "Epoch 9942: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0063 - accuracy: 0.9954 - val_loss: 0.0356 - val_accuracy: 0.9861\n",
            "Epoch 9943/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0162 - accuracy: 0.9891\n",
            "Epoch 9943: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0142 - accuracy: 0.9907 - val_loss: 0.0524 - val_accuracy: 0.9861\n",
            "Epoch 9944/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0078 - accuracy: 1.0000\n",
            "Epoch 9944: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
            "Epoch 9945/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0158 - accuracy: 0.9946\n",
            "Epoch 9945: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 72ms/step - loss: 0.0136 - accuracy: 0.9954 - val_loss: 0.0503 - val_accuracy: 0.9861\n",
            "Epoch 9946/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0075 - accuracy: 0.9946\n",
            "Epoch 9946: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0065 - accuracy: 0.9954 - val_loss: 0.0306 - val_accuracy: 0.9861\n",
            "Epoch 9947/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 9947: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0261 - val_accuracy: 0.9861\n",
            "Epoch 9948/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0058 - accuracy: 1.0000\n",
            "Epoch 9948: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0260 - val_accuracy: 0.9861\n",
            "Epoch 9949/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0080 - accuracy: 1.0000\n",
            "Epoch 9949: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0092 - val_accuracy: 1.0000\n",
            "Epoch 9950/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0638 - accuracy: 0.9896\n",
            "Epoch 9950: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0571 - accuracy: 0.9907 - val_loss: 0.0144 - val_accuracy: 1.0000\n",
            "Epoch 9951/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0051 - accuracy: 1.0000\n",
            "Epoch 9951: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0104 - val_accuracy: 1.0000\n",
            "Epoch 9952/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0088 - accuracy: 0.9946\n",
            "Epoch 9952: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0132 - accuracy: 0.9907 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
            "Epoch 9953/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0067 - accuracy: 1.0000\n",
            "Epoch 9953: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
            "Epoch 9954/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0140 - accuracy: 0.9948\n",
            "Epoch 9954: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0141 - accuracy: 0.9954 - val_loss: 0.0360 - val_accuracy: 0.9861\n",
            "Epoch 9955/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0068 - accuracy: 1.0000\n",
            "Epoch 9955: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
            "Epoch 9956/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0148 - accuracy: 0.9946\n",
            "Epoch 9956: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0224 - accuracy: 0.9907 - val_loss: 0.0236 - val_accuracy: 0.9861\n",
            "Epoch 9957/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9954\n",
            "Epoch 9957: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0135 - accuracy: 0.9954 - val_loss: 0.0147 - val_accuracy: 0.9861\n",
            "Epoch 9958/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0243 - accuracy: 0.9896\n",
            "Epoch 9958: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0252 - accuracy: 0.9907 - val_loss: 0.0484 - val_accuracy: 0.9722\n",
            "Epoch 9959/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0181 - accuracy: 0.9891\n",
            "Epoch 9959: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0155 - accuracy: 0.9907 - val_loss: 0.0724 - val_accuracy: 0.9583\n",
            "Epoch 9960/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0191 - accuracy: 0.9946\n",
            "Epoch 9960: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0166 - accuracy: 0.9954 - val_loss: 0.0142 - val_accuracy: 1.0000\n",
            "Epoch 9961/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0249 - accuracy: 0.9891\n",
            "Epoch 9961: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0231 - accuracy: 0.9907 - val_loss: 0.0285 - val_accuracy: 0.9861\n",
            "Epoch 9962/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0103 - accuracy: 0.9946\n",
            "Epoch 9962: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 62ms/step - loss: 0.0225 - accuracy: 0.9907 - val_loss: 0.0211 - val_accuracy: 0.9861\n",
            "Epoch 9963/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9907\n",
            "Epoch 9963: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0127 - accuracy: 0.9907 - val_loss: 0.0271 - val_accuracy: 0.9861\n",
            "Epoch 9964/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9815\n",
            "Epoch 9964: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.0271 - accuracy: 0.9815 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
            "Epoch 9965/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0077 - accuracy: 0.9954\n",
            "Epoch 9965: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0077 - accuracy: 0.9954 - val_loss: 0.0310 - val_accuracy: 0.9861\n",
            "Epoch 9966/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0109 - accuracy: 0.9948\n",
            "Epoch 9966: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0104 - accuracy: 0.9954 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
            "Epoch 9967/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 9967: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0134 - val_accuracy: 0.9861\n",
            "Epoch 9968/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 9968: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0095 - val_accuracy: 1.0000\n",
            "Epoch 9969/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0053 - accuracy: 1.0000\n",
            "Epoch 9969: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0395 - val_accuracy: 0.9861\n",
            "Epoch 9970/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0089 - accuracy: 0.9946\n",
            "Epoch 9970: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0163 - accuracy: 0.9907 - val_loss: 0.0358 - val_accuracy: 0.9722\n",
            "Epoch 9971/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 9971: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0579 - val_accuracy: 0.9861\n",
            "Epoch 9972/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0205 - accuracy: 0.9946\n",
            "Epoch 9972: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0186 - accuracy: 0.9954 - val_loss: 0.0557 - val_accuracy: 0.9722\n",
            "Epoch 9973/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 1.0000\n",
            "Epoch 9973: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0707 - val_accuracy: 0.9722\n",
            "Epoch 9974/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 9974: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.0544 - val_accuracy: 0.9722\n",
            "Epoch 9975/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0090 - accuracy: 0.9946\n",
            "Epoch 9975: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0077 - accuracy: 0.9954 - val_loss: 0.0418 - val_accuracy: 0.9861\n",
            "Epoch 9976/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0079 - accuracy: 1.0000\n",
            "Epoch 9976: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "Epoch 9977/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0451 - accuracy: 0.9946\n",
            "Epoch 9977: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0476 - accuracy: 0.9907 - val_loss: 0.0122 - val_accuracy: 1.0000\n",
            "Epoch 9978/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0068 - accuracy: 1.0000\n",
            "Epoch 9978: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
            "Epoch 9979/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0078 - accuracy: 1.0000\n",
            "Epoch 9979: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0196 - val_accuracy: 1.0000\n",
            "Epoch 9980/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0799 - accuracy: 0.9861\n",
            "Epoch 9980: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 78ms/step - loss: 0.0799 - accuracy: 0.9861 - val_loss: 0.0529 - val_accuracy: 0.9722\n",
            "Epoch 9981/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9907\n",
            "Epoch 9981: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0295 - accuracy: 0.9907 - val_loss: 0.1577 - val_accuracy: 0.9444\n",
            "Epoch 9982/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0429 - accuracy: 0.9837\n",
            "Epoch 9982: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0392 - accuracy: 0.9861 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
            "Epoch 9983/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0209 - accuracy: 0.9946\n",
            "Epoch 9983: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 62ms/step - loss: 0.0187 - accuracy: 0.9954 - val_loss: 0.0058 - val_accuracy: 1.0000\n",
            "Epoch 9984/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9954\n",
            "Epoch 9984: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0149 - accuracy: 0.9954 - val_loss: 0.0076 - val_accuracy: 1.0000\n",
            "Epoch 9985/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9861\n",
            "Epoch 9985: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0181 - accuracy: 0.9861 - val_loss: 0.0306 - val_accuracy: 0.9861\n",
            "Epoch 9986/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0068 - accuracy: 1.0000\n",
            "Epoch 9986: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0136 - val_accuracy: 1.0000\n",
            "Epoch 9987/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0068 - accuracy: 1.0000\n",
            "Epoch 9987: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.1376 - val_accuracy: 0.9583\n",
            "Epoch 9988/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 9988: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "Epoch 9989/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 9989: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0436 - val_accuracy: 0.9861\n",
            "Epoch 9990/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0065 - accuracy: 1.0000\n",
            "Epoch 9990: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0079 - val_accuracy: 1.0000\n",
            "Epoch 9991/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 9991: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "Epoch 9992/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0115 - accuracy: 0.9954\n",
            "Epoch 9992: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0115 - accuracy: 0.9954 - val_loss: 0.0359 - val_accuracy: 0.9722\n",
            "Epoch 9993/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 1.0000\n",
            "Epoch 9993: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0835 - val_accuracy: 0.9722\n",
            "Epoch 9994/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9907\n",
            "Epoch 9994: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 1s 80ms/step - loss: 0.0132 - accuracy: 0.9907 - val_loss: 0.0334 - val_accuracy: 0.9722\n",
            "Epoch 9995/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 9995: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0504 - val_accuracy: 0.9722\n",
            "Epoch 9996/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 1.0000\n",
            "Epoch 9996: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.0058 - val_accuracy: 1.0000\n",
            "Epoch 9997/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 9997: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
            "Epoch 9998/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0053 - accuracy: 0.9954\n",
            "Epoch 9998: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 62ms/step - loss: 0.0053 - accuracy: 0.9954 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
            "Epoch 9999/10000\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9861\n",
            "Epoch 9999: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0392 - accuracy: 0.9861 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
            "Epoch 10000/10000\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0207 - accuracy: 0.9896\n",
            "Epoch 10000: val_loss did not improve from 0.00143\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.0186 - accuracy: 0.9907 - val_loss: 0.0292 - val_accuracy: 0.9861\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Making Predictions\n",
        "predictions = cnn_calc.predict(test_data)\n",
        "predicted = [np.argmax(w) for w in predictions]\n",
        "expected = test_data.labels\n",
        "\n",
        "\n",
        "# Evaluation Results\n",
        "loss_value , accuracy = cnn_calc.evaluate(train_data)\n",
        "\n",
        "print(f'Test loss_value: {loss_value}')\n",
        "print(f'Test accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tJS6Qytj1iY",
        "outputId": "5a0b5a59-edfc-4f95-dff9-d3f0f843c031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0097 - accuracy: 1.0000\n",
            "Test loss_value: 0.009662914089858532\n",
            "Test accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Confusion Matrix\n",
        "confusion_matrix(expected, predicted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sLOcSnS82P9",
        "outputId": "19783504-467d-4a03-f883-2a3d67be8898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[34,  0],\n",
              "       [ 1, 37]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Classification Report\n",
        "# B, M or A\n",
        "print(f'Classes: {test_data.class_indices}\\n')\n",
        "print(classification_report(expected, predicted))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqCCB8pu87gF",
        "outputId": "26894e92-860e-4689-9d27-803a80d6dbe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: {'B': 0, 'M': 1}\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.99        34\n",
            "           1       1.00      0.97      0.99        38\n",
            "\n",
            "    accuracy                           0.99        72\n",
            "   macro avg       0.99      0.99      0.99        72\n",
            "weighted avg       0.99      0.99      0.99        72\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting the results\n",
        "results_plot = plot_results(hist.history['accuracy'],\n",
        "                            hist.history['val_accuracy'], \n",
        "                            hist.history['loss'], \n",
        "                            hist.history['val_loss'])\n",
        "\n",
        "# enter data to the final results dataframe\n",
        "results_calc = classification_report(expected, predicted, output_dict=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "cf8qoOKq8_74",
        "outputId": "a4cbb70b-8447-4454-c4d3-6aa2e1a9fad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x432 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calc_malignant = results_calc['1']\n",
        "calc_malignant\n",
        "final_results.loc['calcifications'] = (accuracy,\n",
        "                                       calc_malignant['precision'], \n",
        "                                       calc_malignant['recall'], \n",
        "                                       calc_malignant['f1-score'])\n",
        "final_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "RzLMybHY9JkD",
        "outputId": "b6be3a5c-e232-4017-9cc5-6522e4c6989c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                test_accuracy  precision    recall  f1-score\n",
              "calcifications            1.0        1.0  0.973684  0.986667"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f64177d5-f30f-4626-b713-702e44101c81\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>test_accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1-score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>calcifications</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.973684</td>\n",
              "      <td>0.986667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f64177d5-f30f-4626-b713-702e44101c81')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f64177d5-f30f-4626-b713-702e44101c81 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f64177d5-f30f-4626-b713-702e44101c81');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking wrong predictions\n",
        "wrong_predictions = []\n",
        "images = calcifications.p_matrix\n",
        "predictions = cnn_calc.predict(test_data)\n",
        "\n",
        "for i, (p,e) in enumerate(zip(predictions, test_data.labels)):\n",
        "    predicted, expected = np.argmax(p), np.argmax(e)\n",
        "\n",
        "    if predicted != expected:\n",
        "        wrong_predictions.append(\n",
        "            (i, images[i], predicted, expected))"
      ],
      "metadata": {
        "id": "YAYUSQJz9PHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(wrong_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75pM3V6O9SEQ",
        "outputId": "64779235-c7c2-4acf-d805-0d818f090c27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a,b,c,d = wrong_predictions[4]"
      ],
      "metadata": {
        "id": "CihgpHDs9TRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_probabilities(predictions[4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7N_z5Dm9U34",
        "outputId": "eac20613-1b41-4fa5-f9e5-d413a90890d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: 100.0000000000%\n",
            "1: 0.0000000000%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_calc.get_weights()"
      ],
      "metadata": {
        "id": "JH419ugv9WvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_calc.summary()"
      ],
      "metadata": {
        "id": "EYYYQMOt_rgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saves the architecture, weights, training config [loss, optimizer] and the optimizer's state \n",
        "if os.path.isfile('/content/drive/MyDrive/vectorgram/Data/models/vectorgramCNNReg.h5') is False:\n",
        "  cnn_calc.save('/content/drive/MyDrive/vectorgram/Data/models/vectorgramCNNReg.h5')"
      ],
      "metadata": {
        "id": "9pCB292nBSWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "vectorgramCNNReg = load_model('/content/drive/MyDrive/vectorgram/Data/models/vectorgramCNNReg.h5')\n",
        "\n",
        "#serialize the model into a pickle file\n",
        "import pickle\n",
        "#this is the regularized model, with 98-ish percent scores instead of an overfit 100\n",
        "model = vectorgramCNNReg\n",
        "with open('/content/drive/MyDrive/vectorgram/Data/models/vectorgramCNNReg_pkl', 'wb') as files:\n",
        "    pickle.dump(model, files)"
      ],
      "metadata": {
        "id": "N4e7HTcqJih4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32a9b404-30e7-439b-ca80-2f7516b1e270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ram://3280f5a2-bf01-4846-98b6-de56d0156fe7/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "vectorgramCNNReg = load_model('/content/drive/MyDrive/vectorgram/Data/models/vectorgramCNNReg.h5')"
      ],
      "metadata": {
        "id": "KOm8eBIqb7H7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}